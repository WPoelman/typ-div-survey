id,title,year,web_url,venue,track,abstract,has_claim,has_claim_e,has_claim_w,introduces_dataset,introduces_dataset_e,introduces_dataset_w,claim_related_to_e,claim_related_to_w,has_typ_justification_e,has_typ_justification_w,justification_e,justification_w,iso_codes_e,iso_codes_w,iso_codes,isos_wals,n_langs,mpd,mpd_missing,mpd_low_cov,gb_complete_cov,gb_incomplete_cov,gb_coverage,gb_missing
,Language Modelling with Pixels,2023-01-01,https://openreview.net/pdf?id=FkSp8VW8RjH,iclr,ICLR2023 ,"Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.",1,1,1,0,0,0,,,0,0,,,"amh,ara,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swa,tam,tel,tha,tur,urd,vie,wol,yor,zho","amh,ara,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swa,tam,tel,tha,tur,urd,vie,wol,yor,zho","amh,ara,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swa,tam,tel,tha,tur,urd,vie,wol,yor,zho","amh,arb,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swh,tam,tel,tha,tur,urd,vie,wol,yor,cmn",32,0.659891182795699,0,1,175,20,0.897435897435898,0
,Language models are multilingual chain-of-thought reasoners,2023-01-01,https://openreview.net/pdf?id=fR3wGCk-IXp,iclr,ICLR2023 ,"We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at AnonymousLink and the supplementary material.",1,1,1,1,1,1,,,0,0,,,"ben,deu,fra,jpn,rus,spa,swa,tel,tha,zho","ben,deu,eng,fra,jpn,rus,spa,swa,tel,tha,zho","ben,deu,fra,jpn,rus,spa,swa,tel,tha,zho","ben,deu,fra,jpn,rus,spa,swh,tel,tha,cmn",10,0.648277777777778,0,0,142,53,0.728205128205128,0
,Weakly Supervised POS Taggers Perform Poorly on ,2020-01-01,https://aaai.org/papers/08066-weakly-supervised-pos-taggers-perform-poorly-on-em-truly-em-low-resource-languages/,aaai,AAAI Technical Track: Natural Language Processing,"Part-of-speech (POS) taggers for low-resource languages which are exclusively based on various forms of weak supervision – e.g., cross-lingual transfer, type-level supervision, or a combination thereof – have been reported to perform almost as well as supervised ones. However, weakly supervised POS taggers are commonly only evaluated on languages that are very different from truly low-resource languages, and the taggers use sources of information, like high-coverage and almost error-free dictionaries, which are likely not available for resource-poor languages. We train and evaluate state-of-the-art weakly supervised POS taggers for a typologically diverse set of 15 truly low-resource languages. On these languages, given a realistic amount of resources, even our best model gets only less than half of the words right. Our results highlight the need for new and different approaches to POS tagging for truly low-resource languages.",1,1,1,0,0,0,,,0,0,,,"amh,bel,bre,bxr,fao,fil,hsb,hye,kaz,kmr,lit,mar,mlt,tam,tel","amh,bel,bre,bxr,deu,fao,fil,hsb,hye,ita,kaz,kmr,lit,mar,mlt,por,spa,swe,tam,tel","amh,bel,bre,bxr,fao,fil,hsb,hye,kaz,kmr,lit,mar,mlt,tam,tel","amh,bel,bre,bxm,fao,tgl,hsb,hye,kaz,kmr,lit,mar,mlt,tam,tel",15,0.714276923076923,0,1,146,49,0.748717948717949,0
,Improving the Cross-Lingual Generalisation in Visual Question Answering,2023-01-01,https://ojs.aaai.org/index.php/AAAI/article/view/26574,aaai,AAAI Technical Track on Speech & Natural Language Processing,"While several benefits were realized for multilingual vision-language pretrained models, recent benchmarks across various tasks and languages showed poor cross-lingual generalisation when multilingually pre-trained vision-language models are applied to non-English data, with a large gap between (supervised) English performance and (zero-shot) cross-lingual transfer. In this work, we explore the poor performance of these models on a zero-shot cross-lingual visual question answering (VQA) task, where models are fine-tuned on English visual-question data and evaluated on 7 typologically diverse languages. We improve cross-lingual transfer with three strategies: (1) we introduce a linguistic prior objective to augment the cross-entropy loss with a similarity-based loss to guide the model during training, (2) we learn a task-specific subnetwork that improves cross-lingual generalisation and reduces variance without model modification, (3) we augment training examples using synthetic code-mixing to promote alignment of embeddings between source and target languages. Our experiments on xGQA using the pretrained multilingual multimodal transformers UC2 and M3P demonstrates the consistent effectiveness of the proposed fine-tuning strategy for 7 languages, outperforming existing transfer methods with sparse models.",1,1,1,0,0,0,,,0,0,,,"ben,deu,ind,kor,por,rus,zho","ben,deu,eng,ind,kor,por,rus,zho","ben,deu,ind,kor,por,rus,zho","ben,deu,ind,kor,por,rus,cmn",7,0.5901,0,0,124,71,0.635897435897436,0
,"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",2022-01-01,https://proceedings.mlr.press/v162/bugliarello22a/bugliarello22a.pdf,icml,Proceedings of the 39th International Conference on Machine Learning,"Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together{—}by both aggregating pre-existing datasets and creating new ones{—}visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target{–}source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.",1,0,1,1,1,1,,,0,1,,,,"arb,ben,bul,cmn,dan,deu,ell,eng,est,fra,ind,jpn,kor,por,rus,spa,swa,tam,tur,vie","arb,ben,bul,cmn,dan,deu,ell,eng,est,fra,ind,jpn,kor,por,rus,spa,swa,tam,tur,vie","arb,ben,bul,cmn,dan,deu,ell,eng,ekk,fra,ind,jpn,kor,por,rus,spa,swh,tam,tur,vie",20,0.630598421052632,0,0,166,29,0.851282051282051,0
2023.sigmorphon-1.13,SIGMORPHON–UniMorph 2023 Shared Task 0: Typologically Diverse Morphological Inflection,2023-01-01,https://aclanthology.org/2023.sigmorphon-1.13/,sigmorphon,,"The 2023 SIGMORPHON–UniMorph shared task on typologically diverse morphological inflection included a wide range of languages: 26 languages from 9 primary language families. The data this year was all lemma-split, to allow testing models’ generalization ability, and structured along the new hierarchical schema presented in (Batsuren et al., 2022). The systems submitted this year, 9 in number, showed ingenuity and innovativeness, including hard attention for explainability and bidirectional decoding. Special treatment was also given by many participants to the newly-introduced data in Japanese, due to the high abundance of unseen Kanji characters in its test set.",1,1,1,1,1,0,b,d,1,0,"f,l",n,"afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur","afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur","afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur","afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swh,tur",26,0.644370153846154,0,0,173,22,0.887179487179487,0
2023.sigmorphon-1.20,Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing,2023-01-01,https://aclanthology.org/2023.sigmorphon-1.20/,sigmorphon,,"This paper presents the findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing. This first iteration of the shared task explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Natügu, Tsez and Uspanteko. The shared task encompasses two tracks: a resource-scarce closed track and an open track, where participants are allowed to utilize external data resources. Five teams participated in the shared task. The winning team Tü-CL achieved a 23.99%-point improvement over a baseline RoBERTa system in the closed track and a 17.42%-point improvement in the open track.",1,1,0,1,1,1,b,p,1,0,i,n,"arp,ddo,git,lez,ntu,nyb,usp","arp,ddo,git,lez,ntu,nyb,usp","arp,ddo,git,lez,ntu,nyb,usp","arp,ddo,git,lez,ntu,nyb,usp",7,0.79936,0,2,98,97,0.502564102564103,0
L16-1405,Rapid Development of Morphological Analyzers for Typologically Diverse Languages,2016-01-01,https://aclanthology.org/L16-1405/,lrec,,"The Low Resource Language research conducted under DARPA’s Broad Operational Language Translation (BOLT) program required the rapid creation of text corpora of typologically diverse languages (Turkish, Hausa, and Uzbek) which were annotated with morphological information, along with other types of annotation. Since the output of morphological analyzers is a significant aid to morphological annotation, we developed a morphological analyzer for each language in order to support the annotation task, and also as a deliverable by itself. Our framework for analyzer creation results in tables similar to those used in the successful SAMA analyzer for Arabic, but with a more abstract linguistic level, from which the tables are derived. A lexicon was developed from available resources for integration with the analyzer, and given the speed of development and uncertain coverage of the lexicon, we assumed that the analyzer would necessarily be lacking in some coverage for the project annotation. Our analyzer framework was therefore focused on rapid implementation of the key structures of the language, together with accepting “wildcard” solutions as possible analyses for a word with an unknown stem, building upon our similar experiences with morphological annotation with Modern Standard Arabic and Egyptian Arabic.",1,1,1,0,0,0,p,b,1,0,i,n,"hau,tur,uzb","hau,tur,uzb","hau,tur,uzb","hau,tur,uzb",3,0.634466666666667,0,0,34,161,0.174358974358974,0
N18-1173,Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem,2018-01-01,https://aclanthology.org/N18-1173/,naacl,,"Predicting the emotional value of lexical items is a well-known problem in sentiment analysis. While research has focused on polarity for quite a long time, meanwhile this early focus has been shifted to more expressive emotion representation models (such as Basic Emotions or Valence-Arousal-Dominance). This change resulted in a proliferation of heterogeneous formats and, in parallel, often small-sized, non-interoperable resources (lexicons and corpus annotations). In particular, the limitations in size hampered the application of deep learning methods in this area because they typically require large amounts of input data. We here present a solution to get around this language data bottleneck by rephrasing word emotion induction as a multi-task learning problem. In this approach, the prediction of each independent emotion dimension is considered as an individual task and hidden layers are shared between these dimensions. We investigate whether multi-task learning is more advantageous than single-task learning for emotion prediction by comparing our model against a wide range of alternative emotion and polarity induction methods featuring 9 typologically diverse languages and a total of 15 conditions. Our model turns out to outperform each one of them. Against all odds, the proposed deep learning approach yields the largest gain on the smallest data sets, merely composed of one thousand samples.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eng,ind,ita,nld,pol,por,spa,zho","deu,eng,ind,ita,nld,pol,por,spa,zho","deu,eng,ind,ita,nld,pol,por,spa,zho","deu,eng,ind,ita,nld,pol,por,spa,cmn",9,0.510569444444444,0,0,128,67,0.656410256410256,0
2023.acl-long.42,Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation,2023-01-01,https://aclanthology.org/2023.acl-long.42/,acl,,"The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training data from text-only sources. We find that using a self-training approach consistently yields improved performance (a relative WER reduction up to 20.5% compared to using an ASR system trained on 24 minutes of manually transcribed speech). The performance gain from TTS augmentation for Gronings was even stronger (up to 25.5% relative reduction in WER compared to a system based on 24 minutes of manually transcribed speech). In sum, our results show the benefit of using self-training or (if possible) TTS-generated data as an efficient solution to overcome the limitations of data availability for resource-scarce languages in order to improve ASR performance.",1,1,1,1,1,0,b,b,0,0,n,n,"fry,gos,nsy,pse","bes,fry,gos,nas","fry,gos,nsy,pse","fry,gos,nsy,pse",4,,0,3,0,195,0,0
2023.acl-long.210,MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset,2023-01-01,https://aclanthology.org/2023.acl-long.210/,acl,,"Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",1,1,1,1,1,1,b,b,1,1,l,l,"ara,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,zho","ara,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,zho","ara,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,zho","arb,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,cmn",12,0.579813636363636,0,0,164,31,0.841025641025641,0
2023.acl-long.235,MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages,2023-01-01,https://aclanthology.org/2023.acl-long.235/,acl,,"We present the MASSIVE dataset–Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly.",1,1,1,1,1,1,b,b,0,1,i,i,"afr,amh,ara,aze,ben,cmn,cym,dan,deu,ell,eng,fas,fil,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,msa,mya,nld,nob,pol,por,ron,rus,slv,spa,sqi,swa,swe,tam,tel,tha,tur,urd,vie","afr,amh,ara,aze,ben,cat,cat,cym,dan,deu,ell,eng,fas,fil,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,msa,mya,nld,nor,pol,por,ron,rus,slv,spa,sqi,swa,swe,tam,tel,tha,tur,urd,vie,zho,zho","afr,amh,ara,aze,ben,cat,cmn,cym,dan,deu,ell,eng,fas,fil,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,msa,mya,nld,nob,pol,por,ron,rus,slv,spa,sqi,swa,swe,tam,tel,tha,tur,urd,vie","afr,amh,arb,azj,ben,cat,cmn,cym,dan,deu,ell,eng,pes,tgl,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,zsm,mya,nld,nor,pol,por,ron,rus,slv,spa,sqi,swh,swe,tam,tel,tha,tur,urd,vie",51,0.642887918367347,0,1,179,16,0.917948717948718,0
2023.acl-long.337,Using Neural Machine Translation for Generating Diverse Challenging Exercises for Language Learner,2023-01-01,https://aclanthology.org/2023.acl-long.337/,acl,,"We propose a novel approach to automatically generate distractors for cloze exercises for English language learners, using round-trip neural machine translation. A carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence with its round-trip translation. We make use of 16 linguistically-diverse pivots and generate hundreds of translation hypotheses in each direction. We show that using hundreds of translations allows us to generate a rich set of challenging distractors. Moreover, we find that typologically unrelated language pivots contribute more diverse candidate distractors, compared to language pivots that are closely related. We further evaluate the use of machine translation systems of varying quality and find that better quality MT systems produce more challenging distractors. Finally, we conduct a study with language learners, demonstrating that the automatically generated distractors are of the same difficulty as the gold distractors produced by human experts.",1,1,1,1,1,0,b,b,0,1,n,f,"ara,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,zho","ara,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,zho","ara,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,zho","arb,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,cmn",16,0.620637362637363,0,2,164,31,0.841025641025641,0
2023.acl-long.396,Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features,2023-01-01,https://aclanthology.org/2023.acl-long.396/,acl,,"A challenge towards developing NLP systems for the world’s languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models’ behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots.",1,1,1,0,0,0,p,b,1,1,i,i,"ara,cmn,deu,eng,fin,fra,ita,rus,slk,spa,swa,swe","ara,deu,eng,fin,fra,ita,rus,slk,spa,swa,swe,zho","ara,cmn,deu,eng,fin,fra,ita,rus,slk,spa,swa,swe","arb,cmn,deu,eng,fin,fra,ita,rus,slk,spa,swh,swe",12,0.566767272727273,0,1,154,41,0.78974358974359,0
2023.acl-long.609,MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African languages,2023-01-01,https://aclanthology.org/2023.acl-long.609/,acl,,"In this paper, we present AfricaPOS, the largest part-of-speech (POS) dataset for 20 typologically diverse African languages. We discuss the challenges in annotating POS for these languages using the universal dependencies (UD) guidelines. We conducted extensive POS baseline experiments using both conditional random field and several multilingual pre-trained language models. We applied various cross-lingual transfer models trained with data available in the UD. Evaluating on the AfricaPOS dataset, we show that choosing the best transfer language(s) in both single-source and multi-source setups greatly improves the POS tagging performance of the target languages, in particular when combined with parameter-fine-tuning methods. Crucially, transferring knowledge from a language that matches the language family and morphosyntactic properties seems to be more effective for POS tagging in unseen languages.",1,1,1,1,1,1,b,b,1,1,i,l,"bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swa,tsn,twi,wol,xho,yor,zul","bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swa,tsn,twi,wol,xho,yor,zul","bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swa,tsn,twi,wol,xho,yor,zul","bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swh,tsn,aka,wol,xho,yor,zul",20,0.637055555555556,0,2,131,64,0.671794871794872,0
2023.acl-long.657,SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT,2023-01-01,https://aclanthology.org/2023.acl-long.657/,acl,,"Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker’s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between L1 and L2, we model sequential second language acquisition in LMs. Further, we build a Mutlilingual Age Ordered CHILDES (MAO-CHILDES)—a dataset consisting of 5 typologically diverse languages, i.e., German, French, Polish, Indonesian, and Japanese—to understand the degree to which native Child-Directed Speech (CDS) [L1] can help or conflict with English language acquisition [L2]. To examine the impact of native CDS, we use the TILT-based cross lingual transfer learning approach established by Papadimitriou and Jurafsky (2020) and find that, as in human SLA, language family distance predicts more negative transfer. Additionally, we find that conversational speech data shows greater facilitation for language acquisition than scripted speech data. Our findings call for further research using our novel Transformer-based SLA models and we would like to encourage it by releasing our code, data, and models.",1,1,1,1,1,0,b,b,0,1,f,l,"deu,fra,ind,jpn,pol","deu,fra,ind,jpn,pol","deu,fra,ind,jpn,pol","deu,fra,ind,jpn,pol",5,0.60819,0,0,128,67,0.656410256410256,0
K19-1021,On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages,2019-01-01,https://aclanthology.org/K19-1021/,conll,,"Recent work has validated the importance of subword information for word representation learning. Since subwords increase parameter sharing ability in neural models, their value should be even more pronounced in low-data regimes. In this work, we therefore provide a comprehensive analysis focused on the usefulness of subwords for word representation learning in truly low-resource scenarios and for three representative morphological tasks: fine-grained entity typing, morphological tagging, and named entity recognition. We conduct a systematic study that spans several dimensions of comparison: 1) type of data scarcity which can stem from the lack of task-specific training data, or even from the lack of unannotated data required to train word embeddings, or both; 2) language type by working with a sample of 16 typologically diverse languages including some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the choice of the subword-informed word representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and task-based models, where having sufficient in-task data is a more critical requirement.",1,1,1,0,0,0,p,b,1,1,i,i,"amh,bam,bxr,eng,fao,gle,got,heb,mlt,myv,rue,tel,tur,yor,zho,zul","amh,bam,eng,fao,gle,got,heb,mlt,myv,tel,tur,yor,zho,zul","amh,bam,bxr,eng,fao,gle,got,heb,mlt,myv,rue,tel,tur,yor,zho,zul","amh,bam,bxm,eng,fao,gle,got,heb,mlt,myv,rue,tel,tur,yor,cmn,zul",16,0.719714285714286,0,1,147,48,0.753846153846154,0
K19-1086,A Simple and Effective Method for Injecting Word-Level Information into Character-Aware Neural Language Models,2019-01-01,https://aclanthology.org/K19-1086/,conll,,"We propose a simple and effective method to inject word-level information into character-aware neural language models. Unlike previous approaches which usually inject word-level information at the input of a long short-term memory (LSTM) network, we inject it into the softmax function. The resultant model can be seen as a combination of character-aware language model and simple word-level language model. Our injection method can also be used together with previous methods. Through the experiments on 14 typologically diverse languages, we empirically show that our injection method, when used together with the previous methods, works better than the previous methods, including a gating mechanism, averaging, and concatenation of word vectors. We also provide a comprehensive comparison of these injection methods.",1,1,1,0,0,0,p,p,0,0,l,n,"ara,ces,deu,eng,est,fin,heb,jpn,mal,por,rus,spa,vie,zho","ara,ces,deu,eng,est,fin,heb,jpn,msa,por,rus,spa,vie,zho","ara,ces,deu,eng,est,fin,heb,jpn,mal,por,rus,spa,vie,zho","arb,ces,deu,eng,ekk,fin,heb,jpn,mal,por,rus,spa,vie,cmn",14,0.621946153846154,0,0,157,38,0.805128205128205,0
2023.dstc-1.21,Exploring Back Translation with Typo Noise for Enhanced Inquiry Understanding in Task-Oriented Dialogue,2023-01-01,https://aclanthology.org/2023.dstc-1.21/,"dstc, ws",,"This paper presents our approach to the DSTC11 Track 5 selection task, which focuses on retrieving appropriate natural language knowledge sources for task-oriented dialogue. We propose typologically diverse back-translation method with typo noise, which could generate various structured user inquries. Through our noised back translation, we augmented inquiries by combining three different typologies of language sources with five different typo noise injections. Our experiments demonstrate that typological variety and typo noise aids the model in generalizing to diverse user inquiries in dialogue. In the competition, where 14 teams participated, our approach achieved the 5th rank for exact matching metric.",1,1,1,0,0,0,p,b,1,1,l,i,"dan,deu,fil,fin,fun,hin,ind,ine,kor,spa,tur,zho","dan,deu,fil,fin,hun,ind,spa,tur,zho","dan,deu,fil,fin,hun,ind,spa,tur,zho","dan,deu,tgl,fin,hun,ind,spa,tur,cmn",9,0.591252777777778,0,0,131,64,0.671794871794872,0
L14-1123,Untrained Forced Alignment of Transcriptions and Audio for Language Documentation Corpora using WebMAUS,2014-01-01,https://aclanthology.org/L14-1123/,lrec,,"Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units, such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could, however, be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages, there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler, 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training, especially if a manual prealignment of larger annotation units is already avaible.",1,0,1,0,1,0,,b,0,,,l,,"boa,brg,ctn,eve,msa,nuu,poq,sah,win","boa,brg,ctn,eve,msa,nuu,poq,sah,win","boa,brg,ctn,eve,zsm,nuu,poq,sah,win",9,0.738233333333333,0,3,128,67,0.656410256410256,0
L14-1397,A Database for Measuring Linguistic Information Content,2014-01-01,https://aclanthology.org/L14-1397/,lrec,,"Which languages convey the most information in a given amount of space? This is a question often asked of linguists, especially by engineers who often have some information theoretic measure of information in mind, but rarely define exactly how they would measure that information. The question is, in fact remarkably hard to answer, and many linguists consider it unanswerable. But it is a question that seems as if it ought to have an answer. If one had a database of close translations between a set of typologically diverse languages, with detailed marking of morphosyntactic and morphosemantic features, one could hope to quantify the differences between how these different languages convey information. Since no appropriate database exists we decided to construct one. The purpose of this paper is to present our work on the database, along with some preliminary results. We plan to release the dataset once complete.",1,1,1,1,1,1,b,d,1,0,l,n,"ara,cmn,deu,eng,fra,ita,kor,rus","ara,deu,eng,fra,ita,kor,rus,zho","ara,deu,eng,fra,ita,kor,rus,zho","arb,deu,eng,fra,ita,kor,rus,cmn",8,0.581564285714286,0,0,146,49,0.748717948717949,0
2022.findings-acl.24,Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation,2022-01-01,https://aclanthology.org/2022.findings-acl.24/,findings,,"Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-XNLG) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ben,ces,deu,ell,fra,guj,hin,ind,jpn,kor,mar,nep,nld,por,ron,rus,spa,swa,tam,tel,tha,tur,urd,vie,zho","ara,ben,ces,deu,ell,fin,fra,guj,hin,ind,ita,jpn,kor,mar,nep,nld,por,ron,rus,spa,swa,tam,tel,tha,tur,urd,vie,zho","ara,ben,ces,deu,ell,fin,fra,guj,hin,ind,ita,jpn,kor,mar,nep,nld,por,ron,rus,spa,swa,tam,tel,tha,tur,urd,vie,zho","arb,ben,ces,deu,ell,fin,fra,guj,hin,ind,ita,jpn,kor,mar,npi,nld,por,ron,rus,spa,swh,tam,tel,tha,tur,urd,vie,cmn",28,0.64727380952381,0,0,174,21,0.892307692307692,0
2022.findings-acl.196,xGQA: Cross-Lingual Visual Question Answering,2022-01-01,https://aclanthology.org/2022.findings-acl.196/,findings,,"Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and—vice versa—multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.",1,1,1,1,1,1,b,b,0,1,n,f,"ben,deu,eng,ind,kor,por,rus,zho","ind,swa,tam,tur,zho","ben,deu,eng,ind,kor,por,rus,zho","ben,deu,eng,ind,kor,por,rus,cmn",8,0.577032142857143,0,0,129,66,0.661538461538462,0
2022.findings-emnlp.420,TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages,2022-01-01,https://aclanthology.org/2022.findings-emnlp.420/,findings,,"We study politeness phenomena in nine typologically diverse languages. Politeness is an important facet of communication and is sometimes argued to be cultural-specific, yet existing computational linguistic study is limited to English. We create TyDiP, a dataset containing three-way politeness annotations for 500 examples in each language, totaling 4.5K examples. We evaluate how well multilingual models can identify politeness levels – they show a fairly robust zero-shot transfer ability, yet fall short of estimated human accuracy significantly. We further study mapping the English politeness strategy lexicon into nine languages via automatic translation and lexicon induction, analyzing whether each strategy’s impact stays consistent across languages. Lastly, we empirically study the complicated relationship between formality and politeness through transfer experiments. We hope our dataset will support various research questions and applications, from evaluating multilingual models to constructing polite multilingual agents.",1,1,1,1,1,1,b,b,0,0,i,n,"afr,fra,hin,hun,kor,rus,spa,tam,vie","afr,fra,hin,hun,kor,rus,spa,tam,vie","afr,fra,hin,hun,kor,rus,spa,tam,vie","afr,fra,hin,hun,kor,rus,spa,tam,vie",9,0.622741666666667,0,0,147,48,0.753846153846154,0
2020.sigmorphon-1.1,SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.1/,sigmorphon,,"A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systems’ ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.",1,1,1,1,1,0,b,d,1,1,i,f,"aka,ang,ast,aze,azg,bak,ben,bod,cat,cev,cly,cpa,cre,crh,ctp,czn,dak,dan,deu,dje,eng,est,evn,fas,fil,fin,frm,frr,fur,gaa,glg,gmh,gml,gsw,hil,hin,isl,izh,kan,kaz,kir,kjh,kon,kpv,krl,lin,liv,lld,lud,lug,mdf,mhr,mlg,mlt,mri,mwf,myv,nld,nno,nob,nya,olo,ood,orm,ote,otm,pei,pus,san,sme,sna,sot,swa,swe,syc,tel,tgk,tuk,udm,uig,urd,uzb,vec,vep,vot,vro,xno,xty,zpv,zul","aka,ang,azg,ceb,chm,cpa,cta,cta,ctp,dan,deu,eng,est,fil,fin,frr,gaa,gmh,hil,isl,izh,kon,krl,lin,liv,lug,mdf,mlg,mri,myv,nld,nob,nya,ote,otm,pei,sme,sot,swa,swe,vep,vot,xty,zpv,zul","aka,ang,ast,aze,azg,bak,ben,bod,cat,ceb,cly,cpa,cre,crh,ctp,czn,dak,dan,deu,dje,eng,est,evn,fas,fil,fin,frm,frr,fur,gaa,glg,gmh,gml,gsw,hil,hin,isl,izh,kan,kaz,kir,kjh,kon,kpv,krl,lin,liv,lld,lud,lug,mdf,mhr,mlg,mlt,mri,mwf,myv,nld,nno,nob,nya,olo,ood,orm,ote,otm,pei,pus,san,sme,sna,sot,swa,swe,syc,tel,tgk,tuk,udm,uig,urd,uzb,vec,vep,vot,vro,xno,xty,zpv,zul","aka,ang,ast,azj,azg,bak,ben,bod,cat,ceb,cly,cpa,cre,crh,ctp,czn,dak,dan,deu,dje,eng,ekk,evn,pes,tgl,fin,frm,frr,fur,gaa,glg,gmh,gml,gsw,hil,hin,isl,izh,kan,kaz,kir,kjh,kng,kpv,krl,lin,liv,lld,lud,lug,mdf,mhr,zsm,mlt,mri,mwf,myv,nld,nno,nor,nya,olo,ood,orm,ote,otm,pei,pst,san,sme,sna,sot,swh,swe,syc,tel,tgk,tuk,udm,uig,urd,uzb,vec,vep,vot,vro,xno,xty,zpv,zul",90,0.761628461538461,0,25,180,15,0.923076923076923,1
P17-1137,Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling,2017-01-01,https://aclanthology.org/P17-1137/,acl,,"Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the “bursty” distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",1,1,1,0,0,0,p,p,0,0,l,n,"ces,deu,eng,fin,fra,rus,spa","ces,deu,eng,fin,fra,rus,spa","ces,deu,eng,fin,fra,rus,spa","ces,deu,eng,fin,fra,rus,spa",7,0.502542857142857,0,0,97,98,0.497435897435897,0
2022.mia-1.11,MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,2022-01-01,https://aclanthology.org/2022.mia-1.11/,mia,,"We present the results of the Workshop on Multilingual Information Access (MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question answering (QA) systems in 16 typologically diverse languages. In this task, we adapted two large-scale cross-lingual open-retrieval QA datasets in 14 typologically diverse languages, and newly annotated open-retrieval QA data in 2 underrepresented languages: Tagalog and Tamil. Four teams submitted their systems. The best constrained system uses entity-aware contextualized representations for document retrieval, thereby achieving an average F1 score of 31.6, which is 4.1 F1 absolute higher than the challenging baseline. The best system obtains particularly significant improvements in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores. The best unconstrained system achieves 32.2 F1, outperforming our baseline by 4.5 points.",1,1,1,1,1,0,b,b,0,0,i,n,"ara,ben,eng,fil,fin,jpn,khm,kor,mal,rus,spa,swe,tam,tel,zho","ara,ben,eng,fil,fin,jpn,khm,kor,msa,rus,spa,swe,tam,tel,zho","ara,ben,eng,fil,fin,jpn,khm,kor,mal,rus,spa,swe,tam,tel,zho","arb,ben,eng,tgl,fin,jpn,khm,kor,mal,rus,spa,swe,tam,tel,cmn",15,0.659938095238095,0,0,163,32,0.835897435897436,0
2021.naacl-main.283,X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering,2021-01-01,https://aclanthology.org/2021.naacl-main.283/,naacl,,"Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.",1,1,1,0,0,0,p,b,0,1,i,l,"ara,ben,fin,ind,rus,spa,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,spa,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,spa,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,spa,swh,tel,tha",12,0.6573,0,0,149,46,0.764102564102564,0
2021.naacl-main.350,How (Non-)Optimal is the Lexicon?,2021-01-01,https://aclanthology.org/2021.naacl-main.350/,naacl,,"The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf’s law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world’s languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon’s optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes—as measured by code length.",1,1,1,0,0,0,p,p,1,1,l,f,"eng,fin,heb,ind,tam,tur,yor","eng,fin,heb,ind,tam,tur,yor","eng,fin,heb,ind,tam,tur,yor","eng,fin,heb,ind,tam,tur,yor",7,0.639333333333333,0,0,114,81,0.584615384615385,0
2020.repl4nlp-1.7,Improving Bilingual Lexicon Induction with Unsupervised Post-Processing of Monolingual Word Vector Spaces,2020-01-01,https://aclanthology.org/2020.repl4nlp-1.7/,repl4nlp,,"Work on projection-based induction of cross-lingual word embedding spaces (CLWEs) predominantly focuses on the improvement of the projection (i.e., mapping) mechanisms. In this work, in contrast, we show that a simple method for post-processing monolingual embedding spaces facilitates learning of the cross-lingual alignment and, in turn, substantially improves bilingual lexicon induction (BLI). The post-processing method we examine is grounded in the generalisation of first- and second-order monolingual similarities to the nth-order similarity. By post-processing monolingual spaces before the cross-lingual alignment, the method can be coupled with any projection-based method for inducing CLWE spaces. We demonstrate the effectiveness of this simple monolingual post-processing across a set of 15 typologically diverse languages (i.e., 15*14 BLI setups), and in combination with two different projection methods.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,cat,epo,est,eus,fin,heb,hun,ind,kat,kor,lit,nob,tha,tur","bul,cat,epo,est,eus,fin,heb,hun,ind,kat,kor,lit,nor,tha,tur","bul,cat,epo,est,eus,fin,heb,hun,ind,kat,kor,lit,nor,tha,tur","bul,cat,epo,ekk,eus,fin,heb,hun,ind,kat,kor,lit,nor,tha,tur",15,0.620277142857143,0,0,150,45,0.769230769230769,0
C18-1245,Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level,2018-01-01,https://aclanthology.org/C18-1245/,coling,,"Emotion Representation Mapping (ERM) has the goal to convert existing emotion ratings from one representation format into another one, e.g., mapping Valence-Arousal-Dominance annotations for words or sentences into Ekman’s Basic Emotions and vice versa. ERM can thus not only be considered as an alternative to Word Emotion Induction (WEI) techniques for automatic emotion lexicon construction but may also help mitigate problems that come from the proliferation of emotion representation formats in recent years. We propose a new neural network approach to ERM that not only outperforms the previous state-of-the-art. Equally important, we present a refined evaluation methodology and gather strong evidence that our model yields results which are (almost) as reliable as human annotations, even in cross-lingual settings. Based on these results we generate new emotion ratings for 13 typologically diverse languages and claim that they have near-gold quality, at least.",1,1,1,0,1,0,d,b,0,0,n,n,"deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,zho","deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,zho","deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,zho","deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,cmn",13,0.513364102564103,0,0,147,48,0.753846153846154,0
C18-1249,Clausal Modifiers in the Grammar Matrix,2018-01-01,https://aclanthology.org/C18-1249/,coling,,"We extend the coverage of an existing grammar customization system to clausal modifiers, also referred to as adverbial clauses. We present an analysis, taking a typologically-driven approach to account for this phenomenon across the world’s languages, which we implement in the Grammar Matrix customization system (Bender et al., 2002, 2010). Testing our analysis on testsuites from five genetically and geographically diverse languages that were not considered in development, we achieve 88.4% coverage and 1.5% overgeneration.",1,1,1,0,0,0,b,p,1,0,i,n,"cas,eus,lvk,mhi,ura","deu,dru,wmb,zho","cas,eus,lvk,mhi,ura","cas,eus,lvk,mhi,ura",5,0.72834,0,0,129,66,0.661538461538462,0
2022.emnlp-main.492,"Eeny, meeny, miny, moe. How to choose data for morphological inflection.",2022-01-01,https://aclanthology.org/2022.emnlp-main.492/,emnlp,,"Data scarcity is a widespread problem for numerous natural language processing (NLP) tasks within low-resource languages. Within morphology, the labour-intensive task of tagging/glossing data is a serious bottleneck for both NLP and fieldwork. Active learning (AL) aims to reduce the cost of data annotation by selecting data that is most informative for the model. In this paper, we explore four sampling strategies for the task of morphological inflection using a Transformer model: a pair of oracle experiments where data is chosen based on correct/incorrect predictions by the model, model confidence, entropy, and random selection. We investigate the robustness of each sampling strategy across 30 typologically diverse languages, as well as a 10-cycle iteration using Natügu as a case study. Our results show a clear benefit to selecting data based on model confidence. Unsurprisingly, the oracle experiment, which is presented as a proxy for linguist/language informer feedback, shows the most improvement. This is followed closely by low-confidence and high-entropy forms. We also show that despite the conventional wisdom of larger data sets yielding better accuracy, introducing more instances of high-confidence, low-entropy, or forms that the model can already inflect correctly, can reduce model performance.",1,1,1,0,0,0,p,p,1,0,l,i,"ady,amh,ara,arp,aym,cni,cpa,cre,ddo,deu,eus,evn,fas,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swa,tur,zul","ady,amh,ara,arp,aym,cni,cpa,cre,ddo,deu,eus,evn,fas,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swa,tur,zul","ady,amh,ara,arp,aym,cni,cpa,cre,ddo,deu,eus,evn,fas,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swa,tur,zul","ady,amh,arb,arp,ayr,cpx,cpa,cre,ddo,deu,eus,evn,pes,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swh,tur,zul",30,0.69196455026455,0,2,175,20,0.897435897435898,1
2022.emnlp-main.513,Discovering Language-neutral Sub-networks in Multilingual Language Models,2022-01-01,https://aclanthology.org/2022.emnlp-main.513/,emnlp,,"Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on cross-lingual transfer performance, remain open questions. In this work, we conceptualize language neutrality of multilingual models as a function of the overlap between language-encoding sub-networks of these models. We employ the lottery ticket hypothesis to discover sub-networks that are individually optimized for various languages and tasks. Our evaluation across three distinct tasks and eleven typologically-diverse languages demonstrates that sub-networks for different languages are topologically similar (i.e., language-neutral), making them effective initializations for cross-lingual transfer with limited performance degradation.",1,1,1,0,0,0,p,p,0,1,i,f,"ara,deu,eng,fas,fra,hin,rus,spa,swa,urd,zho","ara,deu,eng,fas,fra,hin,rus,spa,swa,urd,zho","ara,deu,eng,fas,fra,hin,rus,spa,swa,urd,zho","arb,deu,eng,pes,fra,hin,rus,spa,swh,urd,cmn",11,0.612607272727273,0,0,154,41,0.78974358974359,0
2022.emnlp-main.532,DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages,2022-01-01,https://aclanthology.org/2022.emnlp-main.532/,emnlp,,"We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset including all collected behavioral data, to foster new research on the translation capabilities of NMT systems for typologically diverse languages.",1,1,1,1,1,1,b,b,1,1,i,l,"ara,ita,nld,tur,ukr,vie","ara,ita,nld,tur,ukr,vie","ara,ita,nld,tur,ukr,vie","arb,ita,nld,tur,ukr,vie",6,0.637233333333333,0,0,135,60,0.692307692307692,0
2022.emnlp-main.552,Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?,2022-01-01,https://aclanthology.org/2022.emnlp-main.552/,emnlp,,"Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and whether it fairly reflects difference between languages. In this work, we investigate the distributions of grammatical relations induced from mBERT in the context of 24 typologically different languages. We demonstrate that the distance between the distributions of different languages is highly consistent with the syntactic difference in terms of linguistic formalisms. Such difference learnt via self-supervision plays a crucial role in the zero-shot transfer performance and can be predicted by variation in morphosyntactic properties between languages. These results suggest that mBERT properly encodes languages in a way consistent with linguistic diversity and provide insights into the mechanism of cross-lingual transfer.",1,1,1,0,0,0,p,p,0,1,f,f,"ara,bul,deu,ell,eng,est,fas,fin,fra,heb,hin,ita,jpn,kor,lat,nld,pol,por,ron,rus,spa,tur,vie,zho","ara,bul,deu,ell,eng,est,fas,fin,fra,heb,hin,ita,jpn,kor,lav,nld,pol,por,ron,rus,spa,tur,vie,zho","ara,bul,deu,ell,eng,est,fas,fin,fra,heb,hin,ita,jpn,kor,lav,nld,pol,por,ron,rus,spa,tur,vie,zho","arb,bul,deu,ell,eng,ekk,pes,fin,fra,heb,hin,ita,jpn,kor,lav,nld,pol,por,ron,rus,spa,tur,vie,cmn",24,0.58746231884058,0,0,170,25,0.871794871794872,0
2022.emnlp-main.607,On Parsing as Tagging,2022-01-01,https://aclanthology.org/2022.emnlp-main.607/,emnlp,,"There are many proposals to reduce constituency parsing to tagging. To figure out what these approaches have in common, we offer a unifying pipeline, which consists of three steps: linearization, learning, and decoding. We prove that classic shift–reduce parsing can be reduced to tetratagging—the state-of-the-art constituency tagger—under two assumptions: right-corner transformation in the linearization step and factored scoring in the learning step. We ask what is the most critical factor that makes parsing-as-tagging methods accurate while being efficient. To answer this question, we empirically evaluate a taxonomy of tagging pipelines with different choices of linearizers, learners, and decoders. Based on the results in English as well as a set of 8 typologically diverse languages, we conclude that the linearization of the derivation tree and its alignment with the input sequence is the most critical factor in achieving accurate parsers as taggers.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eus,fra,heb,hun,kor,pol,swe","deu,eng,eus,fra,heb,hun,kor,pol,swe","deu,eus,fra,heb,hun,kor,pol,swe","deu,eus,fra,heb,hun,kor,pol,swe",8,0.56175,0,0,148,47,0.758974358974359,0
2022.emnlp-main.722,"An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy",2022-01-01,https://aclanthology.org/2022.emnlp-main.722/,emnlp,,"Polysemy is the phenomenon where a single word form possesses two or more related senses. It is an extremely ubiquitous part of natural language and analyzing it has sparked rich discussions in the linguistics, psychology and philosophy communities alike. With scarce attention paid to polysemy in computational linguistics, and even scarcer attention toward quantifying polysemy, in this paper, we propose a novel, unsupervised framework to compute and estimate polysemy scores for words in multiple languages. We infuse our proposed quantification with syntactic knowledge in the form of dependency structures. This informs the final polysemy scores of the lexicon motivated by recent linguistic findings that suggest there is an implicit relation between syntax and ambiguity/polysemy. We adopt a graph based approach by computing the discrete Ollivier Ricci curvature on a graph of the contextual nearest neighbors. We test our framework on curated datasets controlling for different sense distributions of words in 3 typologically diverse languages - English, French and Spanish. The effectiveness of our framework is demonstrated by significant correlations of our quantification with expert human annotated language resources like WordNet. We observe a 0.3 point increase in the correlation coefficient as compared to previous quantification studies in English. Our research leverages contextual language models and syntactic structures to empirically support the widely held theoretical linguistic notion that syntax is intricately linked to ambiguity/polysemy.",1,1,1,0,0,0,p,p,0,0,n,n,"eng,fra,spa","eng,fra,spa","eng,fra,spa","eng,fra,spa",3,0.423466666666667,0,0,45,150,0.230769230769231,0
2022.emnlp-main.730,Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples,2022-01-01,https://aclanthology.org/2022.emnlp-main.730/,emnlp,,"Negative samples have not been efficiently explored in multilingual dense passage retrieval. In this paper, we propose a novel multilingual dense passage retrieval framework, mHFN, to recover and utilize hard and false negative samples. mHFN consists of three key components: 1) a multilingual hard negative sample augmentation module that allows knowledge of indistinguishable passages to be shared across multiple languages and synthesizes new hard negative samples by interpolating representations of queries and existing hard negative samples, 2) a multilingual negative sample cache queue that stores negative samples from previous batches in each language to increase the number of multilingual negative samples used in training beyond the batch size limit, and 3) a lightweight adaptive false negative sample filter that uses generated pseudo labels to separate unlabeled false negative samples and converts them into positive passages in training. We evaluate mHFN on Mr. TyDi, a high-quality multilingual dense passage retrieval dataset covering eleven typologically diverse languages, and experimental results show that mHFN outperforms strong sparse, dense and hybrid baselines and achieves new state-of-the-art performance on all languages. Our source code is available at https://github.com/Magnetic2014/mHFN.",1,1,1,0,0,0,p,p,0,0,i,n,"ara,ben,eng,fin,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,swh,tel,tha",11,0.666190909090909,0,0,149,46,0.764102564102564,0
L12-1174,SMALLWorlds – Multilingual Content-Controlled Monologues,2012-01-01,https://aclanthology.org/L12-1174/,lrec,,"We present the speech corpus SMALLWorlds (Spoken Multi-lingual Accounts of Logically Limited Worlds), newly established and still growing. SMALLWorlds contains monologic descriptions of scenes or worlds which are simple enough to be formally describable. The descriptions are instances of content-controlled monologue: semantically """"""""pre-specified"""""""" but still bearing most hallmarks of spontaneous speech (hesitations and filled pauses, relaxed syntax, repetitions, self-corrections, incomplete constituents, irrelevant or redundant information, etc.) as well as idiosyncratic speaker traits. In the paper, we discuss the pros and cons of data so elicited. Following that, we present a typical SMALLWorlds task: the description of a simple drawing with differently coloured circles, squares, and triangles, with no hints given as to which description strategy or language style to use. We conclude with an example on how SMALLWorlds may be used: unsupervised lexical learning from phonetic transcription. At the time of writing, SMALLWorlds consists of more than 250 recordings in a wide range of typologically diverse languages from many parts of the world, some unwritten and endangered.",1,1,1,1,1,1,d,d,0,0,i,n,"ara,aze,ben,bos,bul,cat,ces,dan,deu,ell,eng,fas,fin,fra,fuc,gle,gsw,heb,hin,hrv,hun,ibo,isl,ita,jpn,khm,kjg,kor,lav,lit,ltz,new,nld,nob,npi,pan,pol,por,ron,rus,sci,spa,swe,tam,tcx,tha,tur,ukr,urd,vie,wuu,yue,zho","ara,dan,deu,eng,fas,fin,fra,hin,ita,jpn,kor,nld,rus,spa,swe,tam,tur,zho","ara,aze,ben,bos,bul,cat,ces,dan,deu,ell,eng,fas,fin,fra,fuc,gle,gsw,heb,hin,hrv,hun,ibo,isl,ita,jpn,khm,kjg,kor,lav,lit,ltz,new,nld,nob,npi,pan,pol,por,ron,rus,sci,spa,swe,tam,tcx,tha,tur,ukr,urd,vie,wuu,yue,zho","arb,azj,ben,bos,bul,cat,ces,dan,deu,ell,eng,pes,fin,fra,fuc,gle,gsw,heb,hin,hrv,hun,ibo,isl,ita,jpn,khm,kjg,kor,lav,lit,ltz,new,nld,nor,npi,pan,pol,por,ron,rus,sci,spa,swe,tam,tcx,tha,tur,ukr,urd,vie,wuu,yue,cmn",53,0.65054906122449,0,3,184,11,0.943589743589744,0
2021.findings-emnlp.382,A multilabel approach to morphosyntactic probing,2021-01-01,https://aclanthology.org/2021.findings-emnlp.382/,findings,,"We propose using a multilabel probing task to assess the morphosyntactic representations of multilingual word embeddings. This tweak on canonical probing makes it easy to explore morphosyntactic representations, both holistically and at the level of individual features (e.g., gender, number, case), and leads more naturally to the study of how language models handle co-occurring features (e.g., agreement phenomena). We demonstrate this task with multilingual BERT (Devlin et al., 2018), training probes for seven typologically diverse languages: Afrikaans, Croatian, Finnish, Hebrew, Korean, Spanish, and Turkish. Through this simple but robust paradigm, we verify that multilingual BERT renders many morphosyntactic features simultaneously extractable. We further evaluate the probes on six held-out languages: Arabic, Chinese, Marathi, Slovenian, Tagalog, and Yoruba. This zero-shot style of probing has the added benefit of revealing which cross-linguistic properties a language model recognizes as being shared by multiple languages.",1,1,1,0,0,0,p,p,0,0,l,n,"afr,fin,heb,hrv,kor,spa,tur","afr,ara,fil,fin,heb,hrv,kor,mar,slv,spa,tur,yor,zho","afr,ara,fil,fin,heb,hrv,kor,mar,slv,spa,tur,yor,zho","afr,arb,tgl,fin,heb,hrv,kor,mar,slv,spa,tur,yor,cmn",13,0.675957692307692,0,0,148,47,0.758974358974359,0
2020.tacl-1.30,TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,2020-01-01,https://aclanthology.org/2020.tacl-1.30/,tacl,,"Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA—a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology—the set of linguistic features each language expresses—such that we expect models performing well on this set to generalize across a large number of the world’s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don’t know the answer yet, and the data is collected directly in each language without the use of translation.",1,1,1,1,1,1,b,d,1,0,l,n,"ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,swh,tel,tha",11,0.666190909090909,0,0,149,46,0.764102564102564,0
2023.findings-acl.885,Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages,2023-01-01,https://aclanthology.org/2023.findings-acl.885/,findings,,"While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets. In this work, we propose Cross-Lingual Knowledge Distillation (CLKD) from a strong English AS2 teacher as a method to train AS2 models for low-resource languages in the tasks without the need of labeled data for the target language. To evaluate our method, we introduce 1) Xtr-WikiQA, a translation-based WikiQA dataset for 9 additional languages, and 2) TyDi-AS2, a multilingual AS2 dataset with over 70K questions spanning 8 typologically diverse languages. We conduct extensive experiments on Xtr-WikiQA and TyDi-AS2 with multiple teachers, diverse monolingual and multilingual pretrained language models (PLMs) as students, and both monolingual and multilingual training. The results demonstrate that CLKD either outperforms or rivals even supervised fine-tuning with the same amount of labeled data and a combination of machine translation and the teacher model. Our method can potentially enable stronger AS2 models for low-resource languages, while TyDi-AS2 can serve as the largest multilingual AS2 dataset for further studies in the research community.",1,1,1,1,1,1,b,b,0,0,n,n,"ben,eng,fin,ind,jpn,kor,rus,swa","ben,eng,fin,ind,jpn,kor,rus,swa","ben,eng,fin,ind,jpn,kor,rus,swa","ben,eng,fin,ind,jpn,kor,rus,swh",8,0.636642857142857,0,0,135,60,0.692307692307692,0
2023.findings-emnlp.52,Improving Cross-lingual Transfer through Subtree-aware Word Reordering,2023-01-01,https://aclanthology.org/2023.findings-emnlp.52/,findings,,"Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via source- or target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consistently outperforms strong baselines over different language pairs and model architectures. This performance advantage holds true in both zero-shot and few-shot scenarios.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,deu,fas,fra,gle,hin,kor,spa,tha","ara,deu,fas,fra,gle,hin,kor,spa,tha","ara,deu,fas,fra,gle,hin,kor,spa,tha","arb,deu,pes,fra,gle,hin,kor,spa,tha",9,0.641741666666667,0,0,138,57,0.707692307692308,0
2022.lrec-1.527,Evaluating Pre-training Objectives for Low-Resource Translation into Morphologically Rich Languages,2022-01-01,https://aclanthology.org/2022.lrec-1.527/,lrec,,"The scarcity of parallel data is a major limitation for Neural Machine Translation (NMT) systems, in particular for translation into morphologically rich languages (MRLs). An important way to overcome the lack of parallel data is to leverage target monolingual data, which is typically more abundant and easier to collect. We evaluate a number of techniques to achieve this, ranging from back-translation to random token masking, on the challenging task of translating English into four typologically diverse MRLs, under low-resource settings. Additionally, we introduce Inflection Pre-Training (or PT-Inflect), a novel pre-training objective whereby the NMT system is pre-trained on the task of re-inflecting lemmatized target sentences before being trained on standard source-to-target language translation. We conduct our evaluation on four typologically diverse target MRLs, and find that PT-Inflect surpasses NMT systems trained only on parallel data. While PT-Inflect is outperformed by back-translation overall, combining the two techniques leads to gains in some of the evaluated language pairs.",1,1,1,0,0,0,p,p,1,0,l,n,"deu,est,lit,tam,tur","deu,est,lit,tam,tur","deu,est,lit,tam,tur","deu,ekk,lit,tam,tur",5,0.60791,0,0,92,103,0.471794871794872,0
2020.emnlp-main.185,XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,2020-01-01,https://aclanthology.org/2020.emnlp-main.185/,emnlp,,"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",1,1,1,1,1,1,b,d,1,1,i,i,"eng,est,hat,ind,swa,tam,tha,tur,vie,zho","est,hat,ind,ita,que,swa,tam,tha,tur,vie,zho","eng,est,hat,ind,ita,que,swa,tam,tha,tur,vie,zho","eng,ekk,hat,ind,ita,que,swh,tam,tha,tur,vie,cmn",12,0.658428787878788,0,0,147,48,0.753846153846154,1
2020.emnlp-main.257,Are All Good Word Vector Spaces Isomorphic?,2020-01-01,https://aclanthology.org/2020.emnlp-main.257/,emnlp,,"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. “under-training”).",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ben,eng,eus,glg,jpn,que,spa,tam,urd","ara,eng,jpn,spa","ara,ben,eng,eus,glg,jpn,que,spa,tam,urd","arb,ben,eng,eus,glg,jpn,que,spa,tam,urd",10,0.673784444444445,0,0,134,61,0.687179487179487,1
2020.emnlp-main.328,Speakers Fill Lexical Semantic Gaps with Context,2020-01-01,https://aclanthology.org/2020.emnlp-main.328/,emnlp,,"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear—resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this—one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. 𝜌 = 0.40 in English). We then test our main hypothesis—that a word’s lexical ambiguity should negatively correlate with its contextual uncertainty—and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.",1,1,1,0,0,0,p,p,0,0,n,n,"afr,ara,ben,eng,est,fas,fil,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor","afr,ara,ben,eng,est,fas,fil,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor","afr,ara,ben,eng,est,fas,fil,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor","afr,arb,ben,eng,ekk,pes,tgl,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor",18,0.672283006535948,0,0,149,46,0.764102564102564,0
2020.emnlp-main.391,Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios,2020-01-01,https://aclanthology.org/2020.emnlp-main.391/,emnlp,,"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.",1,0,1,0,,0,,,,1,,,,"afr,bul,eus,fas,fin,hin,ind,lit,por,tel,tur","afr,bul,eus,fas,fin,hin,ind,lit,por,tel,tur","afr,bul,eus,pes,fin,hin,ind,lit,por,tel,tur",11,0.616114545454546,0,0,135,60,0.692307692307692,0
2020.emnlp-main.479,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,2020-01-01,https://aclanthology.org/2020.emnlp-main.479/,emnlp,,"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as “Punta Cana is located in _.” However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.",1,1,1,1,1,1,b,b,0,0,i,n,"ben,ceb,ell,eng,fil,fra,heb,hun,ilo,jpn,kor,mal,mar,nld,pan,rus,spa,swa,tur,vie,war,yor,zho","ben,ceb,ell,eng,fil,fra,heb,hun,ilo,jpn,kor,mar,mlg,nld,pan,rus,spa,swa,tur,vie,war,yor,zho","ben,ceb,ell,eng,fil,fra,heb,hun,ilo,jpn,kor,mal,mar,nld,pan,rus,spa,swa,tur,vie,war,yor,zho","ben,ceb,ell,eng,tgl,fra,heb,hun,ilo,jpn,kor,mal,mar,nld,pan,rus,spa,swh,tur,vie,war,yor,cmn",23,0.664731601731602,0,1,171,24,0.876923076923077,0
2020.emnlp-main.586,Probing Pretrained Language Models for Lexical Semantics,2020-01-01,https://aclanthology.org/2020.emnlp-main.586/,emnlp,,"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",1,1,1,0,0,0,p,p,0,0,i,n,"deu,eng,fin,rus,tur,zho","deu,eng,fin,ita,rus,tur,zho","deu,eng,fin,ita,rus,tur,zho","deu,eng,fin,ita,rus,tur,cmn",7,0.548133333333333,0,0,131,64,0.671794871794872,0
2020.emnlp-main.617,MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer,2020-01-01,https://aclanthology.org/2020.emnlp-main.617/,emnlp,,"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.",1,1,1,0,0,0,p,p,0,0,i,n,"ara,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swa,tuk,xmf,zho","ara,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swa,tuk,xmf,zho","ara,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swa,tuk,xmf,zho","arb,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swh,tuk,xmf,cmn",16,0.709371428571429,0,2,164,31,0.841025641025641,2
2021.ranlp-1.48,Improving Character-Aware Neural Language Model by Warming up Character Encoder under Skip-gram Architecture,2021-01-01,https://aclanthology.org/2021.ranlp-1.48/,ranlp,,"Character-aware neural language models can capture the relationship between words by exploiting character-level information and are particularly effective for languages with rich morphology. However, these models are usually biased towards information from surface forms. To alleviate this problem, we propose a simple and effective method to improve a character-aware neural language model by forcing a character encoder to produce word-based embeddings under Skip-gram architecture in a warm-up step without extra training data. We empirically show that the resulting character-aware neural language model achieves obvious improvements of perplexity scores on typologically diverse languages, that contain many low-frequency or unseen words.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ces,deu,eng,est,fin,heb,jpn,kan,kor,por,rus,spa,zho","ara,ces,deu,eng,est,fin,heb,jpn,kan,kor,por,rus,spa,zho","ara,ces,deu,eng,est,fin,heb,jpn,kan,kor,por,rus,spa,zho","arb,ces,deu,eng,ekk,fin,heb,jpn,kan,kor,por,rus,spa,cmn",14,0.597897802197802,0,0,157,38,0.805128205128205,0
2020.cl-2.4,LINSPECTOR: Multilingual Probing Tasks for Word Representations,2020-01-01,https://aclanthology.org/2020.cl-2.4/,cl,,"Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.",1,1,1,0,0,0,p,p,0,1,i,f,"deu,fin,rus,spa,tur",,"deu,fin,rus,spa,tur","deu,fin,rus,spa,tur",5,0.5256,0,0,83,112,0.425641025641026,0
2020.cl-4.5,Multi-SimLex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity,2020-01-01,https://aclanthology.org/2020.cl-4.5/,cl,,"We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex–style resources for additional languages. We make these contributions—the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.",1,1,1,1,1,1,b,d,1,0,i,n,"cmn,cym,eng,est,fin,fra,heb,pol,rus,spa,swa,yue","ara,cym,eng,est,fin,fra,heb,pol,rus,spa,swa,yue,zho","cym,eng,est,fin,fra,heb,pol,rus,spa,swa,yue,zho","cym,eng,ekk,fin,fra,heb,pol,rus,spa,swh,yue,cmn",12,0.581422727272727,0,0,161,34,0.825641025641026,0
2021.eacl-main.189,"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT",2021-01-01,https://aclanthology.org/2021.eacl-main.189/,eacl,,"Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model’s internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,zho","ara,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,zho","ara,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,zho","arb,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,cmn",17,0.576729411764706,0,0,166,29,0.851282051282051,0
2021.eacl-main.194,Subword Pooling Makes a Difference,2021-01-01,https://aclanthology.org/2021.eacl-main.194/,eacl,,"Contextual word-representations became a standard in modern natural language processing systems. These models use subword tokenization to handle large vocabularies and unknown words. Word-level usage of such systems requires a way of pooling multiple subwords that correspond to a single word. In this paper we investigate how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages. We compare these in two massively multilingual models, mBERT and XLM-RoBERTa. For morphological tasks, the widely used ‘choose the first subword’ is the worst strategy and the best results are obtained by using attention over the subwords. For POS tagging both of these strategies perform poorly and the best choice is to use a small LSTM over the subwords. The same strategy works best for NER and we show that mBERT is better than XLM-RoBERTa in all 9 languages. We publicly release all code, data and the full result tables at https://github.com/juditacs/subword-choice .",1,1,1,0,0,0,p,p,0,0,f,n,"ara,ces,deu,eng,fin,fra,jpn,kor,zho","ara,ces,deu,eng,fin,fra,jpn,kor,zho","ara,ces,deu,eng,fin,fra,jpn,kor,zho","arb,ces,deu,eng,fin,fra,jpn,kor,cmn",9,0.613144444444445,0,0,147,48,0.753846153846154,0
2021.eacl-main.302,From characters to words: the turning point of BPE merges,2021-01-01,https://aclanthology.org/2021.eacl-main.302/,eacl,,"The distributions of orthographic word types are very different across languages due to typological characteristics, different writing traditions and potentially other factors. The wide range of cross-linguistic diversity is still a major challenge for NLP and the study of language. We use BPE and information-theoretic measures to investigate if distributions become similar under specific levels of subword tokenization. We perform a cross-linguistic comparison, following incremental merges of BPE (we go from characters to words) for 47 diverse languages. We show that text entropy values (a feature of probability distributions) tend to converge at specific subword levels: relatively few BPE merges (around 350) lead to the most similar distributions across languages. Additionally, we analyze the interaction between subword and word-level distributions and show that our findings can be interpreted in light of the ongoing discussion regarding different types of morphological complexity.",1,1,1,0,0,0,p,p,,1,i,l,"aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,fil,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swa,tha,tur,vie,xsu,yad,yaq,yor","aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,fil,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,kew,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swh,tha,tur,vie,xsu,yad,yaq,yor","aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,fil,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,kew,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swh,tha,tur,vie,xsu,yad,yaq,yor","aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,tgl,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,kew,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swh,tha,tur,vie,xsu,yad,yaq,yor",47,0.684365587419056,0,0,185,10,0.948717948717949,0
2023.eacl-main.85,K-hop neighbourhood regularization for few-shot learning on graphs: A case study of text classification,2023-01-01,https://aclanthology.org/2023.eacl-main.85/,eacl,,"We present FewShotTextGCN, a novel method designed to effectively utilize the properties of word-document graphs for improved learning in low-resource settings. We introduce K-hop Neighbourhood Regularization, a regularizer for heterogeneous graphs, and show that it stabilizes and improves learning when only a few training samples are available. We furthermore propose a simplification in the graph-construction method, which results in a graph that is ∼7 times less dense and yields better performance in little-resource settings while performing on par with the state of the art in high-resource settings. Finally, we introduce a new variant of Adaptive Pseudo-Labeling tailored for word-document graphs. When using as little as 20 samples for training, we outperform a strong TextGCN baseline with 17% in absolute accuracy on average over eight languages. We demonstrate that our method can be applied to document classification without any language model pretraining on a wide range of typologically diverse languages while performing on par with large pretrained language models.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eng,fra,ita,jpn,rus,spa,zho","deu,eng,fra,ita,jpn,rus,spa,zho","deu,eng,fra,ita,jpn,rus,spa,zho","deu,eng,fra,ita,jpn,rus,spa,cmn",8,0.546375,0,0,131,64,0.671794871794872,0
2021.acl-long.243,How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models,2021-01-01,https://aclanthology.org/2021.acl-long.243/,"acl, ijcnlp",,"In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.",1,1,1,0,0,0,p,p,1,1,i,i,"ara,eng,fin,ind,jpn,kor,rus,tur,zho","ara,eng,fin,ind,jpn,kor,rus,tur,zho","ara,eng,fin,ind,jpn,kor,rus,tur,zho","arb,eng,fin,ind,jpn,kor,rus,tur,cmn",9,0.608086111111111,0,0,147,48,0.753846153846154,0
2021.acl-short.69,When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation,2021-01-01,https://aclanthology.org/2021.acl-short.69/,"acl, ijcnlp",,"Subword segmentation algorithms have been a de facto choice when building neural machine translation systems. However, most of them need to learn a segmentation model based on some heuristics, which may produce sub-optimal segmentation. This can be problematic in some scenarios when the target language has rich morphological changes or there is not enough data for learning compact composition rules. Translating at fully character level has the potential to alleviate the issue, but empirical performances of character-based models has not been fully explored. In this paper, we present an in-depth comparison between character-based and subword-based NMT systems under three settings: translating to typologically diverse languages, training with low resource, and adapting to unseen domains. Experiment results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains.",1,1,1,0,0,0,p,p,1,1,l,l,"ara,fin,fra,heb,ron,tur,vie,zsm","ara,fin,fra,heb,mri,ron,tur,vie","ara,fin,fra,heb,ron,tur,vie,zsm","arb,fin,fra,heb,ron,tur,vie,zsm",8,0.654182142857143,0,0,131,64,0.671794871794872,0
2021.acl-short.72,Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking,2021-01-01,https://aclanthology.org/2021.acl-short.72/,"acl, ijcnlp",,"Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data.",1,1,1,0,0,0,b,p,,0,i,n,"ces,deu,ell,eng,est,fin,fra,hrv,ita,jpn,kor,lav,nld,nob,pol,por,rus,spa,swe,tha,tur,zho","deu,eng,fin,jpn,kor,rus,spa,tha,tur,zho","ces,deu,ell,eng,est,fin,fra,hrv,ita,jpn,kor,lav,nld,nob,pol,por,rus,spa,swe,tha,tur,zho","ces,deu,ell,eng,ekk,fin,fra,hrv,ita,jpn,kor,lav,nld,nor,pol,por,rus,spa,swe,tha,tur,cmn",22,0.600961471861472,0,0,160,35,0.820512820512821,0
2021.starsem-1.22,Inducing Language-Agnostic Multilingual Representations,2021-01-01,https://aclanthology.org/2021.starsem-1.22/,starsem,,"Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this: (i) re-aligning the vector spaces of target languages (all together) to a pivot source language; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approaches—unlike vector normalization, vector space re-alignment and text normalization do not achieve consistent gains across encoders and languages. Due to the approaches’ additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however.",1,1,1,0,0,0,p,p,,1,n,i,"afr,ben,deu,eng,est,fil,fin,fra,hin,hun,ind,ita,jav,mar,nld,por,spa,urd,zsm","afr,ben,deu,eng,est,fil,fin,fra,hin,hun,ind,ita,jav,mar,msa,nld,por,spa,urd","afr,ben,deu,eng,est,fil,fin,fra,hin,hun,ind,ita,jav,mar,nld,por,spa,urd,zsm","afr,ben,deu,eng,ekk,tgl,fin,fra,hin,hun,ind,ita,jav,mar,nld,por,spa,urd,zsm",19,0.639350292397661,0,0,147,48,0.753846153846154,0
P19-1341,A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction,2019-01-01,https://aclanthology.org/P19-1341/,acl,,"We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world’s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show – across typologically diverse languages in PBC+ – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.",1,1,1,0,1,0,p,p,0,0,i,n,"bul,deu,hrv,hun,pol,por,rus,slk,slv,spa,sqi,srp,swe",,"bul,deu,hrv,hun,pol,por,rus,slk,slv,spa,sqi,srp,swe","bul,deu,hrv,hun,pol,por,rus,slk,slv,spa,sqi,srp,swe",13,0.572031818181818,0,1,128,67,0.656410256410256,0
P19-1383,Is Word Segmentation Child’s Play in All Languages?,2019-01-01,https://aclanthology.org/P19-1383/,acl,,"When learning language, infants need to break down the flow of input speech into minimal word-like units, a process best described as unsupervised bottom-up segmentation. Proposed strategies include several segmentation algorithms, but only cross-linguistically robust algorithms could be plausible candidates for human word learning, since infants have no initial knowledge of the ambient language. We report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages. The results consist evidence that some segmentation algorithms are cross-linguistically valid, thus could be considered as potential strategies employed by all infants.",1,1,1,0,0,0,p,p,1,1,i,i,"ind,inu,jpn,rus,ses,tur,yuc,zho","iku,ind,jpn,rus,sot,tur,yua,zho","iku,ind,jpn,rus,sot,tur,yua,zho","iku,ind,jpn,rus,sot,tur,yua,cmn",8,0.677071428571429,0,1,144,51,0.738461538461539,1
2021.tacl-1.1,Reducing Confusion in Active Learning for Part-Of-Speech Tagging,2021-01-01,https://aclanthology.org/2021.tacl-1.1/,tacl,,"Active learning (AL) uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech (POS) taggers. Existing AL heuristics are generally designed on the principle of selecting uncertain yet representative training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages (German, Swedish, Galician, North Sami, Persian, and Ukrainian), we found the surprising result that even in an oracle scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances that maximally reduce the confusion between particular pairs of output tags. Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin. We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The code is publicly released here.1",1,1,1,0,0,0,p,p,1,0,i,n,"deu,fas,glg,sme,swe,ukr","deu,fas,glg,sme,swe,ukr","deu,fas,glg,sme,swe,ukr","deu,pes,glg,sme,swe,ukr",6,0.6437,0,0,98,97,0.502564102564103,0
2021.tacl-1.25,Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages,2021-01-01,https://aclanthology.org/2021.tacl-1.25/,tacl,,"Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task–language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task–language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods. Our code is available at github.com/cambridgeltl/parameter-factorization.",1,1,1,0,0,0,p,p,0,1,i,i,"aii,amh,ara,bam,cym,est,eus,fao,fil,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,sme,tam,tel,tha,tur,uig,vie,wol,yor,yue","amh,ara,bam,cym,est,eus,fao,fil,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,tam,tel,tha,tur,uig,vie,wol,yor,yue","aii,amh,ara,bam,cym,est,eus,fao,fil,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,sme,tam,tel,tha,tur,uig,vie,wol,yor,yue","aii,amh,arb,bam,cym,ekk,eus,fao,tgl,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,sme,tam,tel,tha,tur,uig,vie,wol,yor,yue",33,0.713741290322581,0,2,175,20,0.897435897435898,0
2021.tacl-1.82,MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering,2021-01-01,https://aclanthology.org/2021.tacl-1.82/,tacl,,"Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1",1,1,1,1,1,1,b,d,0,1,i,f,"ara,cmn,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,nld,nob,pol,por,rus,spa,swe,tha,tur,vie,yue,zsm","ara,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,mal,nld,nor,pol,por,rus,spa,swe,tha,tur,vie,zho","ara,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,nld,nor,pol,por,rus,spa,swe,tha,tur,vie,yue,zho,zsm","arb,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,nld,nor,pol,por,rus,spa,swe,tha,tur,vie,yue,cmn,zsm",25,0.612549,0,0,174,21,0.892307692307692,0
2020.lrec-1.20,The ACQDIV Corpus Database and Aggregation Pipeline,2020-01-01,https://aclanthology.org/2020.lrec-1.20/,lrec,,"We present the ACQDIV corpus database and aggregation pipeline, a tool developed as part of the European Research Council (ERC) funded project ACQDIV, which aims to identify the universal cognitive processes that allow children to acquire any language. The corpus database represents 15 corpora from 14 typologically maximally diverse languages. Here we give an overview of the project, database, and our extensible software package for adding more corpora to the current language sample. Lastly, we discuss how we use the corpus database to mine for universal patterns in child language acquisition corpora and we describe avenues for future research.",1,1,1,1,1,1,d,d,1,1,i,i,"byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw","byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw","byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw","byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw",14,0.727749090909091,0,3,164,31,0.841025641025641,0
2020.lrec-1.493,Morphological Segmentation for Low Resource Languages,2020-01-01,https://aclanthology.org/2020.lrec-1.493/,lrec,,"This paper describes a new morphology resource created by Linguistic Data Consortium and the University of Pennsylvania for the DARPA LORELEI Program. The data consists of approximately 2000 tokens annotated for morphological segmentation in each of 9 low resource languages, along with root information for 7 of the languages. The languages annotated show a broad diversity of typological features. A minimal annotation scheme for segmentation was developed such that it could capture the patterns of a wide range of languages and also be performed reliably by non-linguist annotators. The basic annotation guidelines were designed to be language-independent, but included language-specific morphological paradigms and other specifications. The resulting annotated corpus is designed to support and stimulate the development of unsupervised morphological segmenters and analyzers by providing a gold standard for their evaluation on a more typologically diverse set of languages than has previously been available. By providing root annotation, this corpus is also a step toward supporting research in identifying richer morphological structures than simple morpheme boundaries.",1,1,1,1,1,1,b,d,1,1,i,i,"aka,fil,hin,hun,ind,rus,spa,swa,tam","aka,fil,hin,hun,ind,rus,spa,swa,tam","aka,fil,hin,hun,ind,rus,spa,swa,tam","aka,tgl,hin,hun,ind,rus,spa,swh,tam",9,0.651458333333333,0,0,142,53,0.728205128205128,0
2020.lrec-1.879,"MorphAGram, Evaluation and Framework for Unsupervised Morphological Segmentation",2020-01-01,https://aclanthology.org/2020.lrec-1.879/,lrec,,"Computational morphological segmentation has been an active research topic for decades as it is beneficial for many natural language processing tasks. With the high cost of manually labeling data for morphology and the increasing interest in low-resource languages, unsupervised morphological segmentation has become essential for processing a typologically diverse set of languages, whether high-resource or low-resource. In this paper, we present and release MorphAGram, a publicly available framework for unsupervised morphological segmentation that uses Adaptor Grammars (AG) and is based on the work presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource).",1,1,1,1,1,1,d,b,1,1,i,l,"ara,azd,deu,eng,est,fin,hch,kat,mfy,nah,tur,zul","ara,azd,deu,eng,est,fin,hch,kat,mfy,tur,zul","ara,azd,deu,eng,est,fin,hch,kat,mfy,ngu,tur,zul","arb,azd,deu,eng,ekk,fin,hch,kat,mfy,ngu,tur,zul",12,0.717205454545455,0,1,146,49,0.748717948717949,0
2023.emnlp-main.19,Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection,2023-01-01,https://aclanthology.org/2023.emnlp-main.19/,emnlp,,"Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation strategy StemCorrupt, a method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples. Our analysis uncovers that StemCorrupt brings about fundamental changes in the underlying data distribution, revealing inherent compositional concatenative structure. To complement our theoretical analysis, we investigate the data-efficiency of StemCorrupt. Through evaluation across a diverse set of seven typologically distinct languages, we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of compared to competitive baselines. Furthermore, we explore the impact of typological features on the choice of augmentation strategy and find that languages incorporating non-concatenativity, such as morphonological alternations, derive less benefit from synthetic examples with high predictive uncertainty. We attribute this effect to phonotactic violations induced by StemCorrupt, emphasizing the need for further research to ensure optimal performance across the entire spectrum of natural language morphology.",1,1,1,1,1,0,p,b,0,1,l,l,"ara,ben,fin,kat,nav,spa,tur","ara,ben,fin,kat,nav,spa,tur","ara,ben,fin,kat,nav,spa,tur","arb,ben,fin,kat,nav,spa,tur",7,0.660085714285714,0,0,130,65,0.666666666666667,0
2023.emnlp-main.350,Controllable Contrastive Generation for Multilingual Biomedical Entity Linking,2023-01-01,https://aclanthology.org/2023.emnlp-main.350/,emnlp,,"Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive generation framework for MBEL, which summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template. Instead of tackling the MBEL problem with a discriminative classifier, we formulate it as a sequence-to-sequence generation task, which better exploits the shared dependencies between source mentions and target entities. Moreover, Con2GEN matches against UMLS concepts in as many languages and types as possible, hence facilitating cross-information disambiguation. Extensive experiments show that our model achieves promising performance improvements compared with several state-of-the-art techniques on the XL-BEL and the Mantra GSC datasets spanning 12 typologically diverse languages.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,zho","deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,zho","deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,zho","deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,cmn",12,0.577078787878788,0,0,145,50,0.743589743589744,0
2023.emnlp-main.595,On Bilingual Lexicon Induction with Large Language Models,2023-01-01,https://aclanthology.org/2023.emnlp-main.595/,emnlp,,"Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,cat,deu,eng,fra,hun,ita,rus","bul,cat,deu,eng,fin,fra,hrv,hun,ita,rus,tur","bul,cat,deu,eng,fin,fra,hrv,hun,ita,rus,tur","bul,cat,deu,eng,fin,fra,hrv,hun,ita,rus,tur",11,0.5813,0,0,127,68,0.651282051282051,0
2023.emnlp-main.614,Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models,2023-01-01,https://aclanthology.org/2023.emnlp-main.614/,emnlp,,"Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of “tokens” processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API’s pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI’s language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable, to begin with. Through these analyses, we aim to increase transparency around language model APIs’ pricing policies and encourage the vendors to make them more equitable.",1,1,1,0,0,0,p,p,0,0,n,n,,"afr,asm,bel,ben,bos,bul,cat,ces,cym,dan,deu,ell,eng,epo,est,eus,fil,fin,fra,gla,gle,glg,guj,heb,hin,hrv,hun,hye,ind,isl,ita,jav,jpn,kan,kat,kaz,khm,kir,kor,lit,mal,mar,mkd,nld,pan,pol,por,ron,rus,san,sin,slk,slv,som,spa,srp,sun,swe,tam,tel,tha,tur,uig,ukr,urd,vie,zho","afr,asm,bel,ben,bos,bul,cat,ces,cym,dan,deu,ell,eng,epo,est,eus,fil,fin,fra,gla,gle,glg,guj,heb,hin,hrv,hun,hye,ind,isl,ita,jav,jpn,kan,kat,kaz,khm,kir,kor,lit,mal,mar,mkd,nld,pan,pol,por,ron,rus,san,sin,slk,slv,som,spa,srp,sun,swe,tam,tel,tha,tur,uig,ukr,urd,vie,zho","afr,asm,bel,ben,bos,bul,cat,ces,cym,dan,deu,ell,eng,epo,ekk,eus,tgl,fin,fra,gla,gle,glg,guj,heb,hin,hrv,hun,hye,ind,isl,ita,jav,jpn,kan,kat,kaz,khm,kir,kor,lit,mal,mar,mkd,nld,pan,pol,por,ron,rus,san,sin,slk,slv,som,spa,srp,sun,swe,tam,tel,tha,tur,uig,ukr,urd,vie,cmn",67,0.657782843822844,0,1,181,14,0.928205128205128,0
D19-1102,A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages,2019-01-01,https://aclanthology.org/D19-1102/,"emnlp, ijcnlp",,"Parsers are available for only a handful of the world’s languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languages—North Sámi, Galician, and Kazah—We find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.",1,1,1,0,0,0,p,p,0,1,n,l,"glg,kaz,sme","glg,kaz,sme","glg,kaz,sme","glg,kaz,sme",3,0.857833333333333,0,0,52,143,0.266666666666667,0
D19-1288,Towards Zero-shot Language Modeling,2019-01-01,https://aclanthology.org/D19-1288/,"emnlp, ijcnlp",,"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace’s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world’s languages, we hope that these insights will broaden the scope of applications for language technology.",1,1,1,0,0,0,p,p,0,0,i,n,"acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cjp,cni,dan,deu,dik,dje,djk,dop,eng,epo,est,eus,ewe,fil,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,que,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul","acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cip,cni,dan,deu,dik,dje,djk,dop,eng,epo,est,eus,ewe,fil,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,quw,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul","acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cjp,cni,dan,deu,dik,dje,djk,dop,eng,epo,est,eus,ewe,fil,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,quw,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul","acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cjp,cpx,dan,deu,dik,dje,djk,dop,eng,epo,ekk,eus,ewe,tgl,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,quw,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul",77,0.714621115013169,0,9,183,12,0.938461538461539,0
2022.naacl-main.176,Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations,2022-01-01,https://aclanthology.org/2022.naacl-main.176/,naacl,,"In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,cat,dan,deu,est,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,zho","bul,cat,dan,deu,est,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,zho","bul,cat,dan,deu,est,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,zho","bul,cat,dan,deu,ekk,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,cmn",17,0.613960294117647,0,0,154,41,0.78974358974359,0
2022.naacl-main.270,Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog,2022-01-01,https://aclanthology.org/2022.naacl-main.270/,naacl,,"Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MultiWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts, Multi2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot cross-lingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks.",1,1,1,1,1,1,b,b,0,0,i,n,"ara,deu,rus,zho","ara,deu,eng,rus,zho","ara,deu,rus,zho","arb,deu,rus,cmn",4,0.608833333333333,0,0,113,82,0.57948717948718,0
2022.naacl-main.298,Unsupervised Stem-based Cross-lingual Part-of-Speech Tagging for Morphologically Rich Low-Resource Languages,2022-01-01,https://aclanthology.org/2022.naacl-main.298/,naacl,,"Unsupervised cross-lingual projection for part-of-speech (POS) tagging relies on the use of parallel data to project POS tags from a source language for which a POS tagger is available onto a target language across word-level alignments. The projected tags then form the basis for learning a POS model for the target language. However, languages with rich morphology often yield sparse word alignments because words corresponding to the same citation form do not align well. We hypothesize that for morphologically complex languages, it is more efficient to use the stem rather than the word as the core unit of abstraction. Our contributions are: 1) we propose an unsupervised stem-based cross-lingual approach for POS tagging for low-resource languages of rich morphology; 2) we further investigate morpheme-level alignment and projection; and 3) we examine whether the use of linguistic priors for morphological segmentation improves POS tagging. We conduct experiments using six source languages and eight morphologically complex target languages of diverse typologies. Our results show that the stem-based approach improves the POS models for all the target languages, with an average relative error reduction of 10.3% in accuracy per target language, and outperforms the word-based approach that operates on three-times more data for about two thirds of the language pairs we consider. Moreover, we show that morpheme-level alignment and projection and the use of linguistic priors for morphological segmentation further improve POS tagging.",1,1,1,0,0,0,,,1,1,,,,"amh,ara,deu,eng,eus,fin,fra,ind,kat,kaz,rus,spa,tel,tur","amh,ara,deu,eng,eus,fin,fra,ind,kat,kaz,rus,spa,tel,tur","amh,arb,deu,eng,eus,fin,fra,ind,kat,kaz,rus,spa,tel,tur",14,0.638272527472527,0,0,148,47,0.758974358974359,0
2022.naacl-main.386,Syn2Vec: Synset Colexification Graphs for Lexical Semantic Similarity,2022-01-01,https://aclanthology.org/2022.naacl-main.386/,naacl,,"In this paper we focus on patterns of colexification (co-expressions of form-meaning mapping in the lexicon) as an aspect of lexical-semantic organization, and use them to build large scale synset graphs across BabelNet’s typologically diverse set of 499 world languages. We introduce and compare several approaches: monolingual and cross-lingual colexification graphs, popular distributional models, and fusion approaches. The models are evaluated against human judgments on a semantic similarity task for nine languages. Our strong empirical findings also point to the importance of universality of our graph synset embedding representations with no need for any language-specific adaptation when evaluated on the lexical similarity task. The insights of our exploratory investigation of large-scale colexification graphs could inspire significant advances in NLP across languages, especially for tasks involving languages which lack dedicated lexical resources, and can benefit from language transfer from large shared cross-lingual semantic spaces.",1,1,1,0,0,1,p,b,0,0,n,n,"ara,eng,fin,heb,pol,rus,spa,zho",,"ara,eng,fin,heb,pol,rus,spa,zho","arb,eng,fin,heb,pol,rus,spa,cmn",8,0.536960714285714,0,0,149,46,0.764102564102564,0
N19-1097,A Systematic Study of Leveraging Subword Information for Learning Word Representations,2019-01-01,https://aclanthology.org/N19-1097/,naacl,,"The use of subword-level information (e.g., characters, character n-grams, morphemes) has become ubiquitous in modern word representation learning. Its importance is attested especially for morphologically rich languages which generate a large number of rare words. Despite a steadily increasing interest in such subword-informed word representations, their systematic comparative analysis across typologically diverse languages and different tasks is still missing. In this work, we deliver such a study focusing on the variation of two crucial components required for subword-level integration into word representation models: 1) segmentation of words into subword units, and 2) subword composition functions to obtain final word representations. We propose a general framework for learning subword-informed word representations that allows for easy experimentation with different segmentation and composition components, also including more advanced techniques based on position embeddings and self-attention. Using the unified framework, we run experiments over a large number of subword-informed word representation configurations (60 in total) on 3 tasks (general and rare word similarity, dependency parsing, fine-grained entity typing) for 5 languages representing 3 language types. Our main results clearly indicate that there is no “one-size-fits-all” configuration, as performance is both language- and task-dependent. We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation.",1,1,1,0,0,0,p,p,1,1,n,l,"deu,eng,fin,heb,tur","deu,eng,fin,heb,tur","deu,eng,fin,heb,tur","deu,eng,fin,heb,tur",5,0.55548,0,0,96,99,0.492307692307692,0
N19-1153,Improving Lemmatization of Non-Standard Languages with Joint Learning,2019-01-01,https://aclanthology.org/N19-1153/,naacl,,"Lemmatization of standard languages is concerned with (i) abstracting over morphological differences and (ii) resolving token-lemma ambiguities of inflected words in order to map them to a dictionary headword. In the present paper we aim to improve lemmatization performance on a set of non-standard historical languages in which the difficulty is increased by an additional aspect (iii): spelling variation due to lacking orthographic standards. We approach lemmatization as a string-transduction task with an Encoder-Decoder architecture which we enrich with sentence information using a hierarchical sentence encoder. We show significant improvements over the state-of-the-art by fine-tuning the sentence encodings to jointly optimize a bidirectional language model loss. Crucially, our architecture does not require POS or morphological annotations, which are not always available for historical corpora. Additionally, we also test the proposed model on a set of typologically diverse standard languages showing results on par or better than a model without fine-tuned sentence representations and previous state-of-the-art systems. Finally, to encourage future work on processing of non-standard varieties, we release the dataset of non-standard languages underlying the present study, which is based on openly accessible sources.",1,1,1,0,0,0,p,p,0,0,i,n,"ara,bul,ces,cga,cgl,cgr,crm,deu,eng,est,eus,fas,fin,fra,fro,gml,goo,heb,hun,ita,lat,lav,nob,rus,slv,spa,tur,urd","ara,bul,ces,deu,eng,est,eus,fas,fin,fra,fra,heb,hun,ita,lat,lav,nds,nld,nor,rus,slv,slv,spa,tur,urd","ara,bul,ces,cga,crm,deu,dum,eng,est,eus,fas,fin,fra,fro,gml,goo,heb,hun,ita,lat,lav,nob,rus,slv,spa,tur,urd","arb,bul,ces,cga,crm,deu,dum,eng,ekk,eus,pes,fin,fra,fro,gml,slv,heb,hun,ita,lat,lav,nor,rus,slv,spa,tur,urd",26,0.598450197628459,0,3,150,45,0.769230769230769,0
N19-1203,Contextualization of Morphological Inflection,2019-01-01,https://aclanthology.org/N19-1203/,naacl,,"Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide “gold” tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,eng,eus,fin,gla,hin,ita,lat,pol,swe","bul,eng,eus,fin,gle,hin,ita,lat,pol,swe","bul,eng,eus,fin,gla,hin,ita,lat,pol,swe","bul,eng,eus,fin,gla,hin,ita,lat,pol,swe",10,0.571551111111111,0,0,141,54,0.723076923076923,0
2022.acl-long.582,Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing,2022-01-01,https://aclanthology.org/2022.acl-long.582/,acl,,"Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks. We apply model-agnostic meta-learning (MAML) to the task of cross-lingual dependency parsing. We train our model on a diverse set of languages to learn a parameter initialization that can adapt quickly to new languages. We find that meta-learning with pre-training can significantly improve upon the performance of language transfer and standard supervised learning baselines for a variety of unseen, typologically diverse, and low-resource languages, in a few-shot learning setup.",1,1,1,0,0,0,p,p,1,0,i,n,"bre,bua,deu,fao,fas,fin,fra,hsb,hun,hye,jpn,kaz,swe,tam,urd,vie","ara,bre,bua,bul,ces,deu,eng,fao,fas,fin,fra,hin,hsb,hun,hye,ita,jpn,kaz,kor,nor,rus,swe,tam,tel,urd,vie","ara,bre,bua,bul,ces,deu,eng,fao,fas,fin,fra,hin,hsb,hun,hye,ita,jpn,kaz,kor,nor,rus,swe,tam,tel,urd,vie","arb,bre,bxm,bul,ces,deu,eng,fao,pes,fin,fra,hin,hsb,hun,hye,ita,jpn,kaz,kor,nor,rus,swe,tam,tel,urd,vie",26,0.641877,0,1,168,27,0.861538461538462,0
2022.sigmorphon-1.19,SIGMORPHON–UniMorph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection,2022-01-01,https://aclanthology.org/2022.sigmorphon-1.19/,sigmorphon,,"The 2022 SIGMORPHON–UniMorph shared task on large scale morphological inflection generation included a wide range of typologically diverse languages: 33 languages from 11 top-level language families: Arabic (Modern Standard), Assamese, Braj, Chukchi, Eastern Armenian, Evenki, Georgian, Gothic, Gujarati, Hebrew, Hungarian, Itelmen, Karelian, Kazakh, Ket, Khalkha Mongolian, Kholosi, Korean, Lamahalot, Low German, Ludic, Magahi, Middle Low German, Old English, Old High German, Old Norse, Polish, Pomak, Slovak, Turkish, Upper Sorbian, Veps, and Xibe. We emphasize generalization along different dimensions this year by evaluating test items with unseen lemmas and unseen features separately under small and large training conditions. Across the five submitted systems and two baselines, the prediction of inflections with unseen features proved challenging, with average performance decreased substantially from last year. This was true even for languages for which the forms were in principle predictable, which suggests that further work is needed in designing systems that capture the various types of generalization required for the world’s languages.",1,1,1,1,1,1,b,d,1,0,i,n,"ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,arb,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep",32,0.676222631578948,2,10,172,23,0.882051282051282,2
2022.sigmorphon-1.25,Morphology is not just a naive Bayes – UniMelb Submission to SIGMORPHON 2022 ST on Morphological Inflection,2022-01-01,https://aclanthology.org/2022.sigmorphon-1.25/,sigmorphon,,"The paper describes the Flexica team’s submission to the SIGMORPHON 2022 Shared Task 1 Part 1: Typologically Diverse Morphological Inflection. Our team submitted a nonneural system that extracted transformation patterns from alignments between a lemma and inflected forms. For each inflection category, we chose a pattern based on its abstractness score. The system outperformed the non-neural baseline, the extracted patterns covered a substantial part of possible inflections. However, we discovered that such score that does not account for all possible combinations of string segments as well as morphosyntactic features is not sufficient for a certain proportion of inflection cases.",1,1,0,0,0,0,p,p,0,0,n,n,"ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,arb,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep",32,0.676222631578948,2,10,172,23,0.882051282051282,2
D18-1029,On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling,2018-01-01,https://aclanthology.org/D18-1029/,emnlp,,"A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world’s languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.",1,1,1,0,0,0,p,p,1,1,i,l,,"amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,msa,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho","amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,msa,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho","amh,arb,bul,cat,ces,dan,deu,ell,eng,ekk,eus,pes,tgl,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,zsm,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,cmn",50,0.631831826241135,1,1,178,17,0.912820512820513,1
Q18-1032,Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction,2018-01-01,https://aclanthology.org/Q18-1032/,tacl,,"Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available.",1,1,1,0,0,0,p,p,1,1,i,l,"amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mng,mya,nan,nld,nob,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho,zsm","amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,msa,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho","amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mng,mya,nan,nld,nob,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho,zsm","amh,arb,bul,cat,ces,dan,deu,ell,eng,ekk,eus,pes,tgl,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mng,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,cmn,zsm",50,0.632058094357077,1,2,178,17,0.912820512820513,1
2020.acl-main.112,Learning and Evaluating Emotion Lexicons for 91 Languages,2020-01-01,https://aclanthology.org/2020.acl-main.112/,acl,,"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.",1,1,1,1,1,0,p,b,0,0,n,n,"deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,zho","deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,zho","deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,zho","deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,cmn",12,0.581354545454546,0,0,141,54,0.723076923076923,0
2020.acl-main.420,Information-Theoretic Probing for Linguistic Structure,2020-01-01,https://aclanthology.org/2020.acl-main.420/,acl,,"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research—plus English—totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.",1,1,1,0,0,0,p,p,0,0,n,n,"ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd","ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd","ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd","ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd",11,0.626516363636364,0,0,138,57,0.707692307692308,0
2020.acl-main.596,Modeling Morphological Typology for Unsupervised Learning of Language Morphology,2020-01-01,https://aclanthology.org/2020.acl-main.596/,acl,,"This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology. By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology. The system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems. We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type.",1,1,1,0,0,0,p,p,1,1,f,l,"aka,hin,hun,ind,rus,spa,swa,tag,tam","aka,hin,hun,ind,rus,spa,swa,tag,tam","aka,hin,hun,ind,rus,spa,swa,tag,tam","aka,hin,hun,ind,rus,spa,swh,tag,tam",9,0.654364285714286,0,1,136,59,0.697435897435897,0
2020.acl-main.598,Unsupervised Morphological Paradigm Completion,2020-01-01,https://aclanthology.org/2020.acl-main.598/,acl,,"We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.",1,1,1,0,0,0,p,p,0,1,f,f,"bul,deu,eng,eus,fas,fin,kan,mlt,nav,por,rus,spa,swe,tur","bul,deu,eng,eus,fas,fin,kan,mlt,nav,por,rus,spa,swe,tur","bul,deu,eng,eus,fas,fin,kan,mlt,nav,por,rus,spa,swe,tur","bul,deu,eng,eus,pes,fin,kan,mlt,nav,por,rus,spa,swe,tur",14,0.626538461538462,0,0,155,40,0.794871794871795,0
2022.fieldmatters-1.8,"How to encode arbitrarily complex morphology in word embeddings, no corpus needed",2022-01-01,https://aclanthology.org/2022.fieldmatters-1.8/,fieldmatters,,"In this paper, we present a straightforward technique for constructing interpretable word embeddings from morphologically analyzed examples (such as interlinear glosses) for all of the world’s languages. Currently, fewer than 300-400 languages out of approximately 7000 have have more than a trivial amount of digitized texts; of those, between 100-200 languages (most in the Indo-European language family) have enough text data for BERT embeddings of reasonable quality to be trained. The word embeddings in this paper are explicitly designed to be both linguistically interpretable and fully capable of handling the broad variety found in the world’s diverse set of 7000 languages, regardless of corpus size or morphological characteristics. We demonstrate the applicability of our representation through examples drawn from a typologically diverse set of languages whose morphology includes prefixes, suffixes, infixes, circumfixes, templatic morphemes, derivational morphemes, inflectional morphemes, and reduplication.",1,1,1,0,0,0,p,p,1,1,l,l,"cat,ckt,eng,gug,mlt,ypk,zsm","cat,ckt,eng,grn,mlt,ypk,zsm","cat,ckt,eng,gug,mlt,ypk,zsm","cat,ckt,eng,gug,mlt,ypk,zsm",7,0.800793333333333,1,0,130,65,0.666666666666667,1
2021.sigmorphon-1.25,SIGMORPHON 2021 Shared Task on Morphological Reinflection: Generalization Across Languages,2021-01-01,https://aclanthology.org/2021.sigmorphon-1.25/,sigmorphon,,"This year’s iteration of the SIGMORPHON Shared Task on morphological reinflection focuses on typological diversity and cross-lingual variation of morphosyntactic features. In terms of the task, we enrich UniMorph with new data for 32 languages from 13 language families, with most of them being under-resourced: Kunwinjku, Classical Syriac, Arabic (Modern Standard, Egyptian, Gulf), Hebrew, Amharic, Aymara, Magahi, Braj, Kurdish (Central, Northern, Southern), Polish, Karelian, Livvi, Ludic, Veps, Võro, Evenki, Xibe, Tuvan, Sakha, Turkish, Indonesian, Kodi, Seneca, Asháninka, Yanesha, Chukchi, Itelmen, Eibela. We evaluate six systems on the new data and conduct an extensive error analysis of the systems’ predictions. Transformer-based models generally demonstrate superior performance on the majority of languages, achieving >90% accuracy on 65% of them. The languages on which systems yielded low accuracy are mainly under-resourced, with a limited amount of data. Most errors made by the systems are due to allomorphy, honorificity, and form variation. In addition, we observe that systems especially struggle to inflect multiword lemmas. The systems also produce misspelled forms or end up in repetitive loops (e.g., RNN-based models). Finally, we report a large drop in systems’ performance on previously unseen lemmas.",1,1,1,0,0,0,p,p,1,1,l,l,,"ail,ame,amh,ara,arz,atb,aym,bra,bul,ces,ckb,ckt,cni,deu,evn,gup,heb,ind,itl,kmr,kod,krl,lud,mag,nld,olo,pol,por,rus,sah,sdh,see,sjo,spa,syc,tur,tyv,vep,vro","ail,ame,amh,ara,arz,atb,aym,bra,bul,ces,ckb,ckt,cni,deu,evn,gup,heb,ind,itl,kmr,kod,krl,lud,mag,nld,olo,pol,por,rus,sah,sdh,see,sjo,spa,syc,tur,tyv,vep,vro","ail,ame,amh,arb,arz,atb,ayr,bra,bul,ces,ckb,ckt,cpx,deu,evn,gup,heb,ind,itl,kmr,kod,krl,lud,mag,nld,olo,pol,por,rus,sah,sdh,see,sjo,spa,syc,tur,tyv,vep,vro",39,0.688281481481481,0,12,179,16,0.917948717948718,0
2020.coling-main.423,Manual Clustering and Spatial Arrangement of Verbs for Multilingual Evaluation and Typology Analysis,2020-01-01,https://aclanthology.org/2020.coling-main.423/,coling,,"We present the first evaluation of the applicability of a spatial arrangement method (SpAM) to a typologically diverse language sample, and its potential to produce semantic evaluation resources to support multilingual NLP, with a focus on verb semantics. We demonstrate SpAM’s utility in allowing for quick bottom-up creation of large-scale evaluation datasets that balance cross-lingual alignment with language specificity. Starting from a shared sample of 825 English verbs, translated into Chinese, Japanese, Finnish, Polish, and Italian, we apply a two-phase annotation process which produces (i) semantic verb classes and (ii) fine-grained similarity scores for nearly 130 thousand verb pairs. We use the two types of verb data to (a) examine cross-lingual similarities and variation, and (b) evaluate the capacity of static and contextualised representation models to accurately reflect verb semantics, contrasting the performance of large language specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics.",1,1,1,1,1,0,b,p,0,1,f,f,"cmn,fin,ita,jpn,pol","eng,fin,ita,jpn,pol,zho","cmn,fin,ita,jpn,pol","cmn,fin,ita,jpn,pol",5,0.5834,0,0,137,58,0.702564102564103,0
2020.coling-main.559,XHate-999: Analyzing and Detecting Abusive Language Across Domains and Languages,2020-01-01,https://aclanthology.org/2020.coling-main.559/,coling,,"We present XHate-999, a multi-domain and multilingual evaluation data set for abusive language detection. By aligning test instances across six typologically diverse languages, XHate-999 for the first time allows for disentanglement of the domain transfer and language transfer effects in abusive language detection. We conduct a series of domain- and language-transfer experiments with state-of-the-art monolingual and multilingual transformer models, setting strong baseline results and profiling XHate-999 as a comprehensive evaluation resource for abusive language detection. Finally, we show that domain- and language-adaption, via intermediate masked language modeling on abusive corpora in the target language, can lead to substantially improved abusive language detection in the target language in the zero-shot transfer setups.",1,1,1,1,1,1,b,b,,0,i,n,"deu,hrv,rus,sqi,tur","deu,eng,hrv,rus,sqi,tur","deu,eng,hrv,rus,sqi,tur","deu,eng,hrv,rus,sqi,tur",6,0.64014,0,0,95,100,0.487179487179487,0
S17-1003,Decoding Sentiment from Distributed Representations of Sentences,2017-01-01,https://aclanthology.org/S17-1003/,starsem,,"Distributed representations of sentences have been developed recently to represent their meaning as real-valued vectors. However, it is not clear how much information such representations retain about the polarity of sentences. To study this question, we decode sentiment from unsupervised sentence representations learned with different architectures (sensitive to the order of words, the order of sentences, or none) in 9 typologically diverse languages. Sentiment results from the (recursive) composition of lexical items and grammatical strategies such as negation and concession. The results are manifold: we show that there is no ‘one-size-fits-all’ representation architecture outperforming the others across the board. Rather, the top-ranking architectures depend on the language at hand. Moreover, we find that in several cases the additive composition model based on skip-gram word vectors may surpass supervised state-of-art architectures such as bi-directional LSTMs. Finally, we provide a possible explanation of the observed variation based on the type of negative constructions in each language.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,eng,fra,ita,nld,rus,spa,tur,zho","ara,eng,fra,ita,nld,rus,spa,zho","ara,eng,fra,ita,nld,rus,spa,tur,zho","arb,eng,fra,ita,nld,rus,spa,tur,cmn",9,0.586452777777778,0,0,145,50,0.743589743589744,0
2021.emnlp-main.802,XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation,2021-01-01,https://aclanthology.org/2021.emnlp-main.802/,emnlp,,"Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.",1,1,1,1,1,1,b,d,0,1,i,f,"afr,ara,aze,ben,bul,deu,ell,eng,est,eus,fas,fil,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,msa,mya,nld,pan,pol,por,que,rus,spa,swa,tam,tel,tha,tur,ukr,urd,vie,wol,yor,zho","afr,ara,aze,ben,bul,deu,ell,eng,est,eus,fas,fil,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,msa,mya,nld,pan,pol,por,que,ron,rus,spa,swa,tam,tel,tha,tur,ukr,urd,vie,wol,yor,zho","afr,ara,aze,ben,bul,deu,ell,eng,est,eus,fas,fil,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,msa,mya,nld,pan,pol,por,que,ron,rus,spa,swa,tam,tel,tha,tur,ukr,urd,vie,wol,yor,zho","afr,arb,azj,ben,bul,deu,ell,eng,ekk,eus,pes,tgl,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,zsm,mya,nld,pan,pol,por,que,ron,rus,spa,swh,tam,tel,tha,tur,ukr,urd,vie,wol,yor,cmn",50,0.655107482993197,0,1,180,15,0.923076923076923,1
2021.emnlp-main.818,Visually Grounded Reasoning across Languages and Cultures,2021-01-01,https://aclanthology.org/2021.emnlp-main.818/,emnlp,,"The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.",1,1,1,1,1,1,b,d,0,0,i,n,"cmn,ind,swa,tam,tur","ind,swa,tam,tur,zho","ind,swa,tam,tur,zho","ind,swh,tam,tur,cmn",5,0.69058,0,0,110,85,0.564102564102564,0
2021.mrl-1.12,Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval,2021-01-01,https://aclanthology.org/2021.mrl-1.12/,mrl,,"We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual retrieval in eleven typologically diverse languages, designed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for representation learning perform poorly when applied to out-of-distribution data. As a starting point, we provide zero-shot baselines for this new dataset based on a multi-lingual adaptation of DPR that we call “mDPR”. Experiments show that although the effectiveness of mDPR is much lower than BM25, dense representations nevertheless appear to provide valuable relevance signals, improving BM25 results in sparse–dense hybrids. In addition to analyses of our results, we also discuss future challenges and present a research agenda in multi-lingual dense retrieval. Mr. TyDi can be downloaded at https://github.com/castorini/mr.tydi.",1,1,1,1,1,1,b,d,0,0,n,n,"ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,swh,tel,tha",11,0.666190909090909,0,0,149,46,0.764102564102564,0
2021.mrl-1.18,Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling,2021-01-01,https://aclanthology.org/2021.mrl-1.18/,mrl,,"Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding (NLU). Since annotated datasets are only available for a handful of languages, our work focuses particularly on a zero-shot scenario where the target language is unseen during training. In the context of zero-shot learning, this task is typically approached using representations from pre-trained multilingual language models such as mBERT or by fine-tuning on data automatically translated into the target language. We propose a novel method which augments monolingual source data using multilingual code-switching via random translations, to enhance generalizability of large multilingual language models when fine-tuning them for downstream tasks. Experiments on the MultiATIS++ benchmark show that our method leads to an average improvement of +4.2% in accuracy for the intent task and +1.8% in F1 for the slot-filling task over the state-of-the-art across 8 typologically diverse languages. We also study the impact of code-switching into different families of languages on downstream performance. Furthermore, we present an application of our method for crisis informatics using a new human-annotated tweet dataset of slot filling in English and Haitian Creole, collected during the Haiti earthquake.",1,1,1,0,0,0,p,p,0,1,n,f,"deu,fra,hin,jpn,por,spa,tur,zho","deu,eng,fra,hat,hin,jpn,por,spa,tur,zho","deu,fra,hin,jpn,por,spa,tur,zho","deu,fra,hin,jpn,por,spa,tur,cmn",8,0.5823,0,0,133,62,0.682051282051282,0
2023.tacl-1.59,A Cross-Linguistic Pressure for Uniform Information Density in Word Order,2023-01-01,https://aclanthology.org/2023.tacl-1.59/,tacl,,"While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1",1,1,1,0,0,0,p,p,1,1,i,l,"deu,eng,fas,fra,hin,hun,ind,rus,tur,vie","deu,eng,fas,fra,hin,hun,ind,rus,tur,vie","deu,eng,fas,fra,hin,hun,ind,rus,tur,vie","deu,eng,pes,fra,hin,hun,ind,rus,tur,vie",10,0.59866,0,0,146,49,0.748717948717949,0
2023.tacl-1.63,MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,2023-01-01,https://aclanthology.org/2023.tacl-1.63/,tacl,,"MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http://miracl.ai/.",1,1,1,1,1,1,b,d,0,1,i,f,"ara,ben,deu,eng,fas,fin,fra,hin,ind,jpn,kor,rus,spa,swa,tel,tha,yor,zho","ara,ben,deu,eng,fas,fin,fra,ind,jpn,kor,rus,spa,swa,tel,tha,yor,zho","ara,ben,deu,eng,fas,fin,fra,hin,ind,jpn,kor,rus,spa,swa,tel,tha,yor,zho","arb,ben,deu,eng,pes,fin,fra,hin,ind,jpn,kor,rus,spa,swh,tel,tha,yor,cmn",18,0.640119607843137,0,0,166,29,0.851282051282051,0
2023.sigtyp-1.2,Multilingual End-to-end Dependency Parsing with Linguistic Typology knowledge,2023-01-01,https://aclanthology.org/2023.sigtyp-1.2/,sigtyp,,"We evaluate a Multilingual End-to-end BERT based Dependency Parser which parses an input sentence by directly predicting the relative head-position for each word within it. Our model is a Cross-lingual dependency parser which is trained on a diverse polyglot corpus of high-resource source languages, and is applied on a low-resource target language. To make model more robust to typological variations between source and target languages, and to facilitate the cross-lingual transferring, we utilized the Linguistic typology knowledge, available in typological databases WALS and URIEL. We induce such typology knowledge within our model through an auxiliary task within Multi-task Learning framework.",1,0,1,0,0,0,,p,0,1,,f,,"deu,est,hin,hrv,ita,vie,zho","deu,est,hin,hrv,ita,vie,zho","deu,ekk,hin,hrv,ita,vie,cmn",7,0.673247619047619,0,0,117,78,0.6,0
L02-1035,Co-reference annotation and resources: A multilingual corpus of typologically diverse languages,2002-01-01,https://aclanthology.org/L02-1035/,lrec,,,1,1,1,1,1,1,d,d,1,0,l,n,"jpn,kij","jpn,kij","jpn,kij","jpn,kij",2,0.8413,0,0,76,119,0.38974358974359,0
