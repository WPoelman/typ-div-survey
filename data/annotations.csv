id,title,year,web_url,venue,track,abstract,has_claim,has_claim_e,has_claim_w,introduces_dataset,introduces_dataset_e,introduces_dataset_w,claim_related_to_e,claim_related_to_w,has_typ_justification_e,has_typ_justification_w,justification_e,justification_w,iso_codes_e,iso_codes_w,iso_codes,isos_wals
,Language Modelling with Pixels,2023-01-01,https://openreview.net/pdf?id=FkSp8VW8RjH,iclr,ICLR2023 ,"Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.",1,1,1,0,0,0,,,0,0,,,"amh,ara,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swa,tam,tel,tha,tur,urd,vie,wol,yor,zho","amh,ara,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swa,tam,tel,tha,tur,urd,vie,wol,yor,zho","amh,ara,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swa,tam,tel,tha,tur,urd,vie,wol,yor,zho","amh,arb,ben,bul,cop,deu,ell,eng,fin,fra,hau,hin,ibo,ind,jpn,kin,kor,lug,luo,pcm,rus,spa,swh,tam,tel,tha,tur,urd,vie,wol,yor,cmn"
,Language models are multilingual chain-of-thought reasoners,2023-01-01,https://openreview.net/pdf?id=fR3wGCk-IXp,iclr,ICLR2023 ,"We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at AnonymousLink and the supplementary material.",1,1,1,1,1,1,,,0,0,,,"ben,deu,fra,jpn,rus,spa,swa,tel,tha,zho","ben,deu,eng,fra,jpn,rus,spa,swa,tel,tha,zho","ben,deu,fra,jpn,rus,spa,swa,tel,tha,zho","ben,deu,fra,jpn,rus,spa,swh,tel,tha,cmn"
,ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning,2023-01-01,https://openreview.net/pdf?id=xYlJRpzZtsY,iclr,ICLR2023 ,"Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality — among other traits — by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.",0,0,0,,,,,,,,,,,,,
,Weakly Supervised POS Taggers Perform Poorly on ,2020-01-01,https://aaai.org/papers/08066-weakly-supervised-pos-taggers-perform-poorly-on-em-truly-em-low-resource-languages/,aaai,AAAI Technical Track: Natural Language Processing,"Part-of-speech (POS) taggers for low-resource languages which are exclusively based on various forms of weak supervision – e.g., cross-lingual transfer, type-level supervision, or a combination thereof – have been reported to perform almost as well as supervised ones. However, weakly supervised POS taggers are commonly only evaluated on languages that are very different from truly low-resource languages, and the taggers use sources of information, like high-coverage and almost error-free dictionaries, which are likely not available for resource-poor languages. We train and evaluate state-of-the-art weakly supervised POS taggers for a typologically diverse set of 15 truly low-resource languages. On these languages, given a realistic amount of resources, even our best model gets only less than half of the words right. Our results highlight the need for new and different approaches to POS tagging for truly low-resource languages.",1,1,1,0,0,0,,,0,0,,,"amh,bel,bre,bxr,fao,fil,hsb,hye,kaz,kmr,lit,mar,mlt,tam,tel","amh,bel,bre,bxr,deu,fao,fil,hsb,hye,ita,kaz,kmr,lit,mar,mlt,por,spa,swe,tam,tel","amh,bel,bre,bxr,fao,fil,hsb,hye,kaz,kmr,lit,mar,mlt,tam,tel","amh,bel,bre,bxm,fao,tgl,hsb,hye,kaz,kmr,lit,mar,mlt,tam,tel"
,Improving the Cross-Lingual Generalisation in Visual Question Answering,2023-01-01,https://ojs.aaai.org/index.php/AAAI/article/view/26574,aaai,AAAI Technical Track on Speech & Natural Language Processing,"While several benefits were realized for multilingual vision-language pretrained models, recent benchmarks across various tasks and languages showed poor cross-lingual generalisation when multilingually pre-trained vision-language models are applied to non-English data, with a large gap between (supervised) English performance and (zero-shot) cross-lingual transfer. In this work, we explore the poor performance of these models on a zero-shot cross-lingual visual question answering (VQA) task, where models are fine-tuned on English visual-question data and evaluated on 7 typologically diverse languages. We improve cross-lingual transfer with three strategies: (1) we introduce a linguistic prior objective to augment the cross-entropy loss with a similarity-based loss to guide the model during training, (2) we learn a task-specific subnetwork that improves cross-lingual generalisation and reduces variance without model modification, (3) we augment training examples using synthetic code-mixing to promote alignment of embeddings between source and target languages. Our experiments on xGQA using the pretrained multilingual multimodal transformers UC2 and M3P demonstrates the consistent effectiveness of the proposed fine-tuning strategy for 7 languages, outperforming existing transfer methods with sparse models.",1,1,1,0,0,0,,,0,0,,,"ben,deu,ind,kor,por,rus,zho","ben,deu,eng,ind,kor,por,rus,zho","ben,deu,ind,kor,por,rus,zho","ben,deu,ind,kor,por,rus,cmn"
,"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",2022-01-01,https://proceedings.mlr.press/v162/bugliarello22a/bugliarello22a.pdf,icml,Proceedings of the 39th International Conference on Machine Learning,"Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together{—}by both aggregating pre-existing datasets and creating new ones{—}visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target{–}source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.",1,0,1,1,1,1,,,0,1,,,,"arb,ben,bul,cmn,dan,deu,ell,eng,est,fra,ind,jpn,kor,por,rus,spa,swa,tam,tur,vie","arb,ben,bul,cmn,dan,deu,ell,eng,est,fra,ind,jpn,kor,por,rus,spa,swa,tam,tur,vie","arb,ben,bul,cmn,dan,deu,ell,eng,ekk,fra,ind,jpn,kor,por,rus,spa,swh,tam,tur,vie"
2022.wanlp-1.36,Beyond Arabic: Software for Perso-Arabic Script Manipulation,2022-01-01,https://aclanthology.org/2022.wanlp-1.36/,wanlp,,"This paper presents an open-source software library that provides a set of finite-state transducer (FST) components and corresponding utilities for manipulating the writing systems of languages that use the Perso-Arabic script. The operations include various levels of script normalization, including visual invariance-preserving operations that subsume and go beyond the standard Unicode normalization forms, as well as transformations that modify the visual appearance of characters in accordance with the regional orthographies for eleven contemporary languages from diverse language families. The library also provides simple FST-based romanization and transliteration. We additionally attempt to formalize the typology of Perso-Arabic characters by providing one-to-many mappings from Unicode code points to the languages that use them. While our work focuses on the Arabic script diaspora rather than Arabic itself, this approach could be adopted for any language that uses the Arabic script, thus providing a unified framework for treating a script family used by close to a billion people.",0,0,0,,,,,,,,,,,,,
2023.sigmorphon-1.13,SIGMORPHON–UniMorph 2023 Shared Task 0: Typologically Diverse Morphological Inflection,2023-01-01,https://aclanthology.org/2023.sigmorphon-1.13/,sigmorphon,,"The 2023 SIGMORPHON–UniMorph shared task on typologically diverse morphological inflection included a wide range of languages: 26 languages from 9 primary language families. The data this year was all lemma-split, to allow testing models’ generalization ability, and structured along the new hierarchical schema presented in (Batsuren et al., 2022). The systems submitted this year, 9 in number, showed ingenuity and innovativeness, including hard attention for explainability and bidirectional decoding. Special treatment was also given by many participants to the newly-introduced data in Japanese, due to the high abundance of unseen Kanji characters in its test set.",1,1,1,1,1,0,b,d,1,0,"f,l",n,"afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur","afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur","afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur","afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swh,tur"
2023.sigmorphon-1.15,Morphological reinflection with weighted finite-state transducers,2023-01-01,https://aclanthology.org/2023.sigmorphon-1.15/,sigmorphon,,"This paper describes the submission by the University of Arizona to the SIGMORPHON 2023 Shared Task on typologically diverse morphological (re-)infection. In our submission, we investigate the role of frequency, length, and weighted transducers in addressing the challenge of morphological reinflection. We start with the non-neural baseline provided for the task and show how some improvement can be gained by integrating length and frequency in prefix selection. We also investigate using weighted finite-state transducers, jump-started from edit distance and directly augmented with frequency. Our specific technique is promising and quite simple, but we see only modest improvements for some languages here.",0,0,0,,,,,p,,,,n,,"afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur",,
2023.sigmorphon-1.18,The BGU-MeLeL System for the SIGMORPHON 2023 Shared Task on Morphological Inflection,2023-01-01,https://aclanthology.org/2023.sigmorphon-1.18/,sigmorphon,,"This paper presents the submission by the MeLeL team to the SIGMORPHON–UniMorph Shared Task on Typologically Diverse and Acquisition-Inspired Morphological Inflection Generation Part 3: Models of Acquisition of Inflectional Noun Morphology in Polish, Estonian, and Finnish. This task requires us to produce the word form given a lemma and a grammatical case, while trying to produce the same error-rate as in children. We approach this task with a reduced-size character-based transformer model, multilingual training and an upsampling method to introduce bias.",0,0,0,,,,,p,,,,n,,"afb,amh,arz,bel,dan,deu,eng,fin,fra,grc,heb,hun,hye,ita,jpn,kat,klr,mkd,nav,rus,san,sme,spa,sqi,swa,tur",,
2023.sigmorphon-1.20,Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing,2023-01-01,https://aclanthology.org/2023.sigmorphon-1.20/,sigmorphon,,"This paper presents the findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing. This first iteration of the shared task explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Natügu, Tsez and Uspanteko. The shared task encompasses two tracks: a resource-scarce closed track and an open track, where participants are allowed to utilize external data resources. Five teams participated in the shared task. The winning team Tü-CL achieved a 23.99%-point improvement over a baseline RoBERTa system in the closed track and a 17.42%-point improvement in the open track.",1,1,0,1,1,1,b,p,1,0,i,n,"arp,ddo,git,lez,ntu,nyb,usp","arp,ddo,git,lez,ntu,nyb,usp","arp,ddo,git,lez,ntu,nyb,usp","arp,ddo,git,lez,ntu,nyb,usp"
L16-1405,Rapid Development of Morphological Analyzers for Typologically Diverse Languages,2016-01-01,https://aclanthology.org/L16-1405/,lrec,,"The Low Resource Language research conducted under DARPA’s Broad Operational Language Translation (BOLT) program required the rapid creation of text corpora of typologically diverse languages (Turkish, Hausa, and Uzbek) which were annotated with morphological information, along with other types of annotation. Since the output of morphological analyzers is a significant aid to morphological annotation, we developed a morphological analyzer for each language in order to support the annotation task, and also as a deliverable by itself. Our framework for analyzer creation results in tables similar to those used in the successful SAMA analyzer for Arabic, but with a more abstract linguistic level, from which the tables are derived. A lexicon was developed from available resources for integration with the analyzer, and given the speed of development and uncertain coverage of the lexicon, we assumed that the analyzer would necessarily be lacking in some coverage for the project annotation. Our analyzer framework was therefore focused on rapid implementation of the key structures of the language, together with accepting “wildcard” solutions as possible analyses for a word with an unknown stem, building upon our similar experiences with morphological annotation with Modern Standard Arabic and Egyptian Arabic.",1,1,1,0,0,0,p,b,1,0,i,n,"hau,tur,uzb","hau,tur,uzb","hau,tur,uzb","hau,tur,uzb"
L16-1515,Legacy language atlas data mining: mapping Kru languages,2016-01-01,https://aclanthology.org/L16-1515/,lrec,,"An online tool based on dialectometric methods, DistGraph, is applied to a group of Kru languages of Côte d’Ivoire, Liberia and Burkina Faso. The inputs to this resource consist of tables of languages x linguistic features (e.g. phonological, lexical or grammatical), and statistical and graphical outputs are generated which show similarities and differences between the languages in terms of the features as virtual distances. In the present contribution, attention is focussed on the consonant systems of the languages, a traditional starting point for language comparison. The data are harvested from a legacy language data resource based on fieldwork in the 1970s and 1980s, a language atlas of the Kru languages. The method on which the online tool is based extends beyond documentation of individual languages to the documentation of language groups, and supports difference-based prioritisation in education programmes, decisions on language policy and documentation and conservation funding, as well as research on language typology and heritage documentation of history and migration.",0,0,0,,,,,,,,,,,,,
N18-1004,A Deep Generative Model of Vowel Formant Typology,2018-01-01,https://aclanthology.org/N18-1004/,naacl,,"What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information—the first two formant values—rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.",0,0,0,,,,,,,,,,,,,
N18-1173,Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem,2018-01-01,https://aclanthology.org/N18-1173/,naacl,,"Predicting the emotional value of lexical items is a well-known problem in sentiment analysis. While research has focused on polarity for quite a long time, meanwhile this early focus has been shifted to more expressive emotion representation models (such as Basic Emotions or Valence-Arousal-Dominance). This change resulted in a proliferation of heterogeneous formats and, in parallel, often small-sized, non-interoperable resources (lexicons and corpus annotations). In particular, the limitations in size hampered the application of deep learning methods in this area because they typically require large amounts of input data. We here present a solution to get around this language data bottleneck by rephrasing word emotion induction as a multi-task learning problem. In this approach, the prediction of each independent emotion dimension is considered as an individual task and hidden layers are shared between these dimensions. We investigate whether multi-task learning is more advantageous than single-task learning for emotion prediction by comparing our model against a wide range of alternative emotion and polarity induction methods featuring 9 typologically diverse languages and a total of 15 conditions. Our model turns out to outperform each one of them. Against all odds, the proposed deep learning approach yields the largest gain on the smallest data sets, merely composed of one thousand samples.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eng,ind,ita,nld,pol,por,spa,zho","deu,eng,ind,ita,nld,pol,por,spa,zho","deu,eng,ind,ita,nld,pol,por,spa,zho","deu,eng,ind,ita,nld,pol,por,spa,cmn"
N18-2085,Are All Languages Equally Hard to Language-Model?,2018-01-01,https://aclanthology.org/N18-2085/,naacl,,"For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.",0,0,0,,,,,,,,,,,,,
2023.acl-long.42,Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation,2023-01-01,https://aclanthology.org/2023.acl-long.42/,acl,,"The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training data from text-only sources. We find that using a self-training approach consistently yields improved performance (a relative WER reduction up to 20.5% compared to using an ASR system trained on 24 minutes of manually transcribed speech). The performance gain from TTS augmentation for Gronings was even stronger (up to 25.5% relative reduction in WER compared to a system based on 24 minutes of manually transcribed speech). In sum, our results show the benefit of using self-training or (if possible) TTS-generated data as an efficient solution to overcome the limitations of data availability for resource-scarce languages in order to improve ASR performance.",1,1,1,1,1,0,b,b,0,0,n,n,"fry,gos,nsy,pse","bes,fry,gos,nas","fry,gos,nsy,pse","fry,gos,nsy,pse"
2023.acl-long.120,Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization,2023-01-01,https://aclanthology.org/2023.acl-long.120/,acl,,"The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1600 English summaries produced by 16 diverse extractive systems. We find that 30% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that is designed for detecting unfaithful extractive summaries and is shown to have the best performance. We hope our work can increase the awareness of unfaithfulness problems in extractive summarization and help future work to evaluate and resolve these issues.",0,0,0,,,,,,,,,,,,,
2023.acl-long.210,MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset,2023-01-01,https://aclanthology.org/2023.acl-long.210/,acl,,"Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",1,1,1,1,1,1,b,b,1,1,l,l,"ara,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,zho","ara,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,zho","ara,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,zho","arb,deu,fin,fra,hin,hun,jpn,pol,rus,spa,tur,cmn"
2023.acl-long.235,MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages,2023-01-01,https://aclanthology.org/2023.acl-long.235/,acl,,"We present the MASSIVE dataset–Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly.",1,1,1,1,1,1,b,b,0,1,i,i,"afr,amh,ara,aze,ben,cmn,cym,dan,deu,ell,eng,fas,fil,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,msa,mya,nld,nob,pol,por,ron,rus,slv,spa,sqi,swa,swe,tam,tel,tha,tur,urd,vie","afr,amh,ara,aze,ben,cat,cat,cym,dan,deu,ell,eng,fas,fil,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,msa,mya,nld,nor,pol,por,ron,rus,slv,spa,sqi,swa,swe,tam,tel,tha,tur,urd,vie,zho,zho","afr,amh,ara,aze,ben,cat,cmn,cym,dan,deu,ell,eng,fas,fil,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,msa,mya,nld,nob,pol,por,ron,rus,slv,spa,sqi,swa,swe,tam,tel,tha,tur,urd,vie","afr,amh,arb,azj,ben,cat,cmn,cym,dan,deu,ell,eng,pes,tgl,fin,fra,heb,hin,hun,hye,ind,isl,ita,jav,jpn,kan,kat,khm,kor,lav,mal,mon,zsm,mya,nld,nor,pol,por,ron,rus,slv,spa,sqi,swh,swe,tam,tel,tha,tur,urd,vie"
2023.acl-long.337,Using Neural Machine Translation for Generating Diverse Challenging Exercises for Language Learner,2023-01-01,https://aclanthology.org/2023.acl-long.337/,acl,,"We propose a novel approach to automatically generate distractors for cloze exercises for English language learners, using round-trip neural machine translation. A carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence with its round-trip translation. We make use of 16 linguistically-diverse pivots and generate hundreds of translation hypotheses in each direction. We show that using hundreds of translations allows us to generate a rich set of challenging distractors. Moreover, we find that typologically unrelated language pivots contribute more diverse candidate distractors, compared to language pivots that are closely related. We further evaluate the use of machine translation systems of varying quality and find that better quality MT systems produce more challenging distractors. Finally, we conduct a study with language learners, demonstrating that the automatically generated distractors are of the same difficulty as the gold distractors produced by human experts.",1,1,1,1,1,0,b,b,0,1,n,f,"ara,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,zho","ara,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,zho","ara,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,zho","arb,bis,ces,chk,deu,fra,hin,ind,ita,mal,nld,rus,spa,urd,vie,cmn"
2023.acl-long.396,Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features,2023-01-01,https://aclanthology.org/2023.acl-long.396/,acl,,"A challenge towards developing NLP systems for the world’s languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models’ behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots.",1,1,1,0,0,0,p,b,1,1,i,i,"ara,cmn,deu,eng,fin,fra,ita,rus,slk,spa,swa,swe","ara,deu,eng,fin,fra,ita,rus,slk,spa,swa,swe,zho","ara,cmn,deu,eng,fin,fra,ita,rus,slk,spa,swa,swe","arb,cmn,deu,eng,fin,fra,ita,rus,slk,spa,swh,swe"
2023.acl-long.609,MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African languages,2023-01-01,https://aclanthology.org/2023.acl-long.609/,acl,,"In this paper, we present AfricaPOS, the largest part-of-speech (POS) dataset for 20 typologically diverse African languages. We discuss the challenges in annotating POS for these languages using the universal dependencies (UD) guidelines. We conducted extensive POS baseline experiments using both conditional random field and several multilingual pre-trained language models. We applied various cross-lingual transfer models trained with data available in the UD. Evaluating on the AfricaPOS dataset, we show that choosing the best transfer language(s) in both single-source and multi-source setups greatly improves the POS tagging performance of the target languages, in particular when combined with parameter-fine-tuning methods. Crucially, transferring knowledge from a language that matches the language family and morphosyntactic properties seems to be more effective for POS tagging in unseen languages.",1,1,1,1,1,1,b,b,1,1,i,l,"bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swa,tsn,twi,wol,xho,yor,zul","bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swa,tsn,twi,wol,xho,yor,zul","bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swa,tsn,twi,wol,xho,yor,zul","bam,bbj,ewe,fon,hau,ibo,kin,lug,luo,mos,nya,pcm,sna,swh,tsn,aka,wol,xho,yor,zul"
2023.acl-long.657,SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT,2023-01-01,https://aclanthology.org/2023.acl-long.657/,acl,,"Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker’s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between L1 and L2, we model sequential second language acquisition in LMs. Further, we build a Mutlilingual Age Ordered CHILDES (MAO-CHILDES)—a dataset consisting of 5 typologically diverse languages, i.e., German, French, Polish, Indonesian, and Japanese—to understand the degree to which native Child-Directed Speech (CDS) [L1] can help or conflict with English language acquisition [L2]. To examine the impact of native CDS, we use the TILT-based cross lingual transfer learning approach established by Papadimitriou and Jurafsky (2020) and find that, as in human SLA, language family distance predicts more negative transfer. Additionally, we find that conversational speech data shows greater facilitation for language acquisition than scripted speech data. Our findings call for further research using our novel Transformer-based SLA models and we would like to encourage it by releasing our code, data, and models.",1,1,1,1,1,0,b,b,0,1,f,l,"deu,fra,ind,jpn,pol","deu,fra,ind,jpn,pol","deu,fra,ind,jpn,pol","deu,fra,ind,jpn,pol"
2023.acl-long.726,A Crosslingual Investigation of Conceptualization in 1335 Languages,2023-01-01,https://aclanthology.org/2023.acl-long.726/,acl,,"Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for ‘belly’ and ‘womb’. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all languages for one concept (‘bird’) and an evaluation on gold standard data for 32 Swadesh concepts, we show that Conceptualizer has good alignment accuracy. We demonstrate the potential of research on conceptualization in NLP with two experiments. (1) We define crosslingual stability of a concept as the degree to which it has 1-1 correspondences across languages, and show that concreteness predicts stability. (2) We represent each language by its conceptualization pattern for 83 concepts, and define a similarity measure on these representations. The resulting measure for the conceptual similarity between two languages is complementary to standard genealogical, typological, and surface similarity measures. For four out of six language families, we can assign languages to their correct family based on conceptual similarity with accuracies between 54% and 87%",0,0,0,,,,,,,,,,,,,
Q19-1021,On the Complexity and Typology of Inflectional Morphological Systems,2019-01-01,https://aclanthology.org/Q19-1021/,tacl,,"We quantify the linguistic complexity of different languages’ morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language’s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm— how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.",1,1,1,0,0,0,p,p,0,1,n,i,,,,
K19-1021,On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages,2019-01-01,https://aclanthology.org/K19-1021/,conll,,"Recent work has validated the importance of subword information for word representation learning. Since subwords increase parameter sharing ability in neural models, their value should be even more pronounced in low-data regimes. In this work, we therefore provide a comprehensive analysis focused on the usefulness of subwords for word representation learning in truly low-resource scenarios and for three representative morphological tasks: fine-grained entity typing, morphological tagging, and named entity recognition. We conduct a systematic study that spans several dimensions of comparison: 1) type of data scarcity which can stem from the lack of task-specific training data, or even from the lack of unannotated data required to train word embeddings, or both; 2) language type by working with a sample of 16 typologically diverse languages including some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the choice of the subword-informed word representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and task-based models, where having sufficient in-task data is a more critical requirement.",1,1,1,0,0,0,p,b,1,1,i,i,"amh,bam,bxr,eng,fao,gle,got,heb,mlt,myv,rue,tel,tur,yor,zho,zul","amh,bam,eng,fao,gle,got,heb,mlt,myv,tel,tur,yor,zho,zul","amh,bam,bxr,eng,fao,gle,got,heb,mlt,myv,rue,tel,tur,yor,zho,zul","amh,bam,bxm,eng,fao,gle,got,heb,mlt,myv,rue,tel,tur,yor,cmn,zul"
K19-1086,A Simple and Effective Method for Injecting Word-Level Information into Character-Aware Neural Language Models,2019-01-01,https://aclanthology.org/K19-1086/,conll,,"We propose a simple and effective method to inject word-level information into character-aware neural language models. Unlike previous approaches which usually inject word-level information at the input of a long short-term memory (LSTM) network, we inject it into the softmax function. The resultant model can be seen as a combination of character-aware language model and simple word-level language model. Our injection method can also be used together with previous methods. Through the experiments on 14 typologically diverse languages, we empirically show that our injection method, when used together with the previous methods, works better than the previous methods, including a gating mechanism, averaging, and concatenation of word vectors. We also provide a comprehensive comparison of these injection methods.",1,1,1,0,0,0,p,p,0,0,l,n,"ara,ces,deu,eng,est,fin,heb,jpn,mal,por,rus,spa,vie,zho","ara,ces,deu,eng,est,fin,heb,jpn,msa,por,rus,spa,vie,zho","ara,ces,deu,eng,est,fin,heb,jpn,mal,por,rus,spa,vie,zho","arb,ces,deu,eng,ekk,fin,heb,jpn,mal,por,rus,spa,vie,cmn"
2023.dstc-1.21,Exploring Back Translation with Typo Noise for Enhanced Inquiry Understanding in Task-Oriented Dialogue,2023-01-01,https://aclanthology.org/2023.dstc-1.21/,"dstc, ws",,"This paper presents our approach to the DSTC11 Track 5 selection task, which focuses on retrieving appropriate natural language knowledge sources for task-oriented dialogue. We propose typologically diverse back-translation method with typo noise, which could generate various structured user inquries. Through our noised back translation, we augmented inquiries by combining three different typologies of language sources with five different typo noise injections. Our experiments demonstrate that typological variety and typo noise aids the model in generalizing to diverse user inquiries in dialogue. In the competition, where 14 teams participated, our approach achieved the 5th rank for exact matching metric.",1,1,1,0,0,0,p,b,1,1,l,i,"dan,deu,fil,fin,fun,hin,ind,ine,kor,spa,tur,zho","dan,deu,fil,fin,hun,ind,spa,tur,zho","dan,deu,fil,fin,hun,ind,spa,tur,zho","dan,deu,tgl,fin,hun,ind,spa,tur,cmn"
2022.blackboxnlp-1.37,Universal and Independent: Multilingual Probing Framework for Exhaustive Model Interpretation and Evaluation,2022-01-01,https://aclanthology.org/2022.blackboxnlp-1.37/,blackboxnlp,,"Linguistic analysis of language models is one of the ways to explain and describe their reasoning, weaknesses, and limitations. In the probing part of the model interpretability research, studies concern individual languages as well as individual linguistic structures. The question arises: are the detected regularities linguistically coherent, or on the contrary, do they dissonate at the typological scale? Moreover, the majority of studies address the inherent set of languages and linguistic structures, leaving the actual typological diversity knowledge out of scope. In this paper, we present and apply the GUI-assisted framework allowing us to easily probe massive amounts of languages for all the morphosyntactic features present in the Universal Dependencies data. We show that reflecting the anglo-centric trend in NLP over the past years, most of the regularities revealed in the mBERT model are typical for the western-European languages. Our framework can be integrated with the existing probing toolboxes, model cards, and leaderboards, allowing practitioners to use and share their familiar probing methods to interpret multilingual models. Thus we propose a toolkit to systematize the multilingual flaws in multilingual models, providing a reproducible experimental setup for 104 languages and 80 morphosyntactic features.",0,0,1,,,,,b,,,,i,,"afr,aii,akk,amh,apu,aqz,ara,bam,bej,bel,ben,bho,bre,btx,bua,bul,cat,ces,chu,cop,cym,dan,deu,ell,eng,est,esu,eus,fao,fas,fil,fin,fra,fro,gla,gle,glg,glv,got,grc,gub,gun,heb,hin,hrv,hsb,hun,hye,hyw,ind,isl,ita,jav,kaz,kmr,koi,kor,kpv,krl,lat,lav,lij,lit,lzh,mar,mdf,muh,myv,nld,nor,olo,orv,pcm,pol,por,quc,ron,rus,san,sjo,slk,slv,sme,sms,spa,sqi,srp,swe,tam,tat,tha,tpn,tur,tur,uig,ukr,urb,urd,wbp,wol,yor,zho",,
L14-1123,Untrained Forced Alignment of Transcriptions and Audio for Language Documentation Corpora using WebMAUS,2014-01-01,https://aclanthology.org/L14-1123/,lrec,,"Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units, such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could, however, be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages, there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler, 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training, especially if a manual prealignment of larger annotation units is already avaible.",1,0,1,0,1,0,,b,0,,,l,,"boa,brg,ctn,eve,msa,nuu,poq,sah,win","boa,brg,ctn,eve,msa,nuu,poq,sah,win","boa,brg,ctn,eve,zsm,nuu,poq,sah,win"
L14-1397,A Database for Measuring Linguistic Information Content,2014-01-01,https://aclanthology.org/L14-1397/,lrec,,"Which languages convey the most information in a given amount of space? This is a question often asked of linguists, especially by engineers who often have some information theoretic measure of information in mind, but rarely define exactly how they would measure that information. The question is, in fact remarkably hard to answer, and many linguists consider it unanswerable. But it is a question that seems as if it ought to have an answer. If one had a database of close translations between a set of typologically diverse languages, with detailed marking of morphosyntactic and morphosemantic features, one could hope to quantify the differences between how these different languages convey information. Since no appropriate database exists we decided to construct one. The purpose of this paper is to present our work on the database, along with some preliminary results. We plan to release the dataset once complete.",1,1,1,1,1,1,b,d,1,0,l,n,"ara,cmn,deu,eng,fra,ita,kor,rus","ara,deu,eng,fra,ita,kor,rus,zho","ara,deu,eng,fra,ita,kor,rus,zho","arb,deu,eng,fra,ita,kor,rus,cmn"
L14-1508,Language CoLLAGE: Grammatical Description with the LinGO Grammar Matrix,2014-01-01,https://aclanthology.org/L14-1508/,lrec,,"Language CoLLAGE is a collection of grammatical descriptions developed in the context of a grammar engineering graduate course with the LinGO Grammar Matrix. These grammatical descriptions include testsuites in well-formed interlinear glossed text (IGT) format, high-level grammatical characterizations called choices files, HPSG grammar fragments (capable of parsing and generation), and documentation. As of this writing, Language CoLLAGE includes resources for 52 typologically and areally diverse languages and this number is expected to grow over time. The resources for each language cover a similar range of core grammatical phenomena and are implemented in a uniform framework, compatible with the DELPH-IN suite of processing tools.",1,1,1,1,1,1,d,d,0,0,l,f,,,,
2022.cl-3.5,Investigating Language Relationships in Multilingual Sentence Encoders Through the Lens of Linguistic Typology,2022-01-01,https://aclanthology.org/2022.cl-3.5/,cl,,"Multilingual sentence encoders have seen much success in cross-lingual model transfer for downstream NLP tasks. The success of this transfer is, however, dependent on the model’s ability to encode the patterns of cross-lingual similarity and variation. Yet, we know relatively little about the properties of individual languages or the general patterns of linguistic variation that the models encode. In this article, we investigate these questions by leveraging knowledge from the field of linguistic typology, which studies and documents structural and semantic variation across languages. We propose methods for separating language-specific subspaces within state-of-the-art multilingual sentence encoders (LASER, M-BERT, XLM, and XLM-R) with respect to a range of typological properties pertaining to lexical, morphological, and syntactic structure. Moreover, we investigate how typological information about languages is distributed across all layers of the models. Our results show interesting differences in encoding linguistic variation associated with different pretraining strategies. In addition, we propose a simple method to study how shared typological properties of languages are encoded in two state-of-the-art multilingual models—M-BERT and XLM-R. The results provide insight into their information-sharing mechanisms and suggest that these linguistic properties are encoded jointly across typologically similar languages in these models.",0,0,0,,,,,,,,,,,,,
2022.findings-acl.24,Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation,2022-01-01,https://aclanthology.org/2022.findings-acl.24/,findings,,"Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-XNLG) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ben,ces,deu,ell,fra,guj,hin,ind,jpn,kor,mar,nep,nld,por,ron,rus,spa,swa,tam,tel,tha,tur,urd,vie,zho","ara,ben,ces,deu,ell,fin,fra,guj,hin,ind,ita,jpn,kor,mar,nep,nld,por,ron,rus,spa,swa,tam,tel,tha,tur,urd,vie,zho","ara,ben,ces,deu,ell,fin,fra,guj,hin,ind,ita,jpn,kor,mar,nep,nld,por,ron,rus,spa,swa,tam,tel,tha,tur,urd,vie,zho","arb,ben,ces,deu,ell,fin,fra,guj,hin,ind,ita,jpn,kor,mar,npi,nld,por,ron,rus,spa,swh,tam,tel,tha,tur,urd,vie,cmn"
2022.findings-acl.196,xGQA: Cross-Lingual Visual Question Answering,2022-01-01,https://aclanthology.org/2022.findings-acl.196/,findings,,"Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and—vice versa—multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.",1,1,1,1,1,1,b,b,0,1,n,f,"ben,deu,eng,ind,kor,por,rus,zho","ind,swa,tam,tur,zho","ben,deu,eng,ind,kor,por,rus,zho","ben,deu,eng,ind,kor,por,rus,cmn"
2022.findings-emnlp.420,TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages,2022-01-01,https://aclanthology.org/2022.findings-emnlp.420/,findings,,"We study politeness phenomena in nine typologically diverse languages. Politeness is an important facet of communication and is sometimes argued to be cultural-specific, yet existing computational linguistic study is limited to English. We create TyDiP, a dataset containing three-way politeness annotations for 500 examples in each language, totaling 4.5K examples. We evaluate how well multilingual models can identify politeness levels – they show a fairly robust zero-shot transfer ability, yet fall short of estimated human accuracy significantly. We further study mapping the English politeness strategy lexicon into nine languages via automatic translation and lexicon induction, analyzing whether each strategy’s impact stays consistent across languages. Lastly, we empirically study the complicated relationship between formality and politeness through transfer experiments. We hope our dataset will support various research questions and applications, from evaluating multilingual models to constructing polite multilingual agents.",1,1,1,1,1,1,b,b,0,0,i,n,"afr,fra,hin,hun,kor,rus,spa,tam,vie","afr,fra,hin,hun,kor,rus,spa,tam,vie","afr,fra,hin,hun,kor,rus,spa,tam,vie","afr,fra,hin,hun,kor,rus,spa,tam,vie"
2020.conll-1.20,Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment,2020-01-01,https://aclanthology.org/2020.conll-1.20/,conll,,"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier’s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.",0,0,0,,,,,,,,,,,,,
2003.mtsummit-systems.4,SYSTRAN Review Manager,2003-01-01,https://aclanthology.org/2003.mtsummit-systems.4/,mtsummit,,"The SYSTRAN Review Manager (SRM) is one of the components that comprise the SYSTRAN Linguistics Platform (SLP), a comprehensive enterprise solution for managing MT customization and localization projects. The SRM is a productivity tool used for the review, quality assessment and maintenance of linguistic resources combined with a SYSTRAN solution. The SRM is used in-house by SYSTRAN’s development team and is also licensed to corporate customers as it addresses leading linguistic challenges, such as terminology and homographs, which makes it a key component of the QA process. Extremely flexible, the SRM adapts to localization and MT customization projects from small to large-scale. Its Web-based interface and multi-user architecture enable a centralized and efficient work environment for local and geographically disbursed individual users and teams. Users segment a given corpus to fluidly review and evaluate translations, as well as identify the typology of errors. Corpus metrics, terminology extraction and detailed reporting capabilities facilitate prioritizing tasks, resulting in immediate focus on those issues that significantly impact MT quality. Data and statistics are tracked throughout the customization process and are always available for regression tests and overall project management. This environment is highly conducive to increased productivity and efficient QA in the MT customization effort.",0,0,0,,,,,,,,,,,,,
2011.jeptalnrecital-recital.1,Analyse de l’ambiguïté des requêtes utilisateurs par catégorisation thématique,2011-01-01,https://aclanthology.org/2011.jeptalnrecital-recital.1/,jeptalnrecital,,"Dans cet article, nous cherchons à identifier la nature de l’ambiguïté des requêtes utilisateurs issues d’un moteur de recherche dédié à l’actualité, 2424actu.fr, en utilisant une tâche de catégorisation. Dans un premier temps, nous verrons les différentes formes de l’ambiguïté des requêtes déjà décrites dans les travaux de TAL. Nous confrontons la vision lexicographique de l’ambiguïté à celle décrite par les techniques de classification appliquées à la recherche d’information. Dans un deuxième temps, nous appliquons une méthode de catégorisation thématique afin d’explorer l’ambiguïté des requêtes, celle-ci nous permet de conduire une analyse sémantique de ces requêtes, en intégrant la dimension temporelle propre au contexte des news. Nous proposons une typologie des phénomènes d’ambiguïté basée sur notre analyse sémantique. Enfin, nous comparons l’exploration par catégorisation à une ressource comme Wikipédia, montrant concrètement les divergences des deux approches.",0,0,0,,,,,,,,,,,,,
2020.sigmorphon-1.1,SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.1/,sigmorphon,,"A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systems’ ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.",1,1,1,1,1,0,b,d,1,1,i,f,"aka,ang,ast,aze,azg,bak,ben,bod,cat,cev,cly,cpa,cre,crh,ctp,czn,dak,dan,deu,dje,eng,est,evn,fas,fil,fin,frm,frr,fur,gaa,glg,gmh,gml,gsw,hil,hin,isl,izh,kan,kaz,kir,kjh,kon,kpv,krl,lin,liv,lld,lud,lug,mdf,mhr,mlg,mlt,mri,mwf,myv,nld,nno,nob,nya,olo,ood,orm,ote,otm,pei,pus,san,sme,sna,sot,swa,swe,syc,tel,tgk,tuk,udm,uig,urd,uzb,vec,vep,vot,vro,xno,xty,zpv,zul","aka,ang,azg,ceb,chm,cpa,cta,cta,ctp,dan,deu,eng,est,fil,fin,frr,gaa,gmh,hil,isl,izh,kon,krl,lin,liv,lug,mdf,mlg,mri,myv,nld,nob,nya,ote,otm,pei,sme,sot,swa,swe,vep,vot,xty,zpv,zul","aka,ang,ast,aze,azg,bak,ben,bod,cat,ceb,cly,cpa,cre,crh,ctp,czn,dak,dan,deu,dje,eng,est,evn,fas,fil,fin,frm,frr,fur,gaa,glg,gmh,gml,gsw,hil,hin,isl,izh,kan,kaz,kir,kjh,kon,kpv,krl,lin,liv,lld,lud,lug,mdf,mhr,mlg,mlt,mri,mwf,myv,nld,nno,nob,nya,olo,ood,orm,ote,otm,pei,pus,san,sme,sna,sot,swa,swe,syc,tel,tgk,tuk,udm,uig,urd,uzb,vec,vep,vot,vro,xno,xty,zpv,zul","aka,ang,ast,azj,azg,bak,ben,bod,cat,ceb,cly,cpa,cre,crh,ctp,czn,dak,dan,deu,dje,eng,ekk,evn,pes,tgl,fin,frm,frr,fur,gaa,glg,gmh,gml,gsw,hil,hin,isl,izh,kan,kaz,kir,kjh,kng,kpv,krl,lin,liv,lld,lud,lug,mdf,mhr,zsm,mlt,mri,mwf,myv,nld,nno,nor,nya,olo,ood,orm,ote,otm,pei,pst,san,sme,sna,sot,swh,swe,syc,tel,tgk,tuk,udm,uig,urd,uzb,vec,vep,vot,vro,xno,xty,zpv,zul"
2020.sigmorphon-1.6,The CMU-LTI submission to the SIGMORPHON 2020 Shared Task 0: Language-Specific Cross-Lingual Transfer,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.6/,sigmorphon,,"This paper describes the CMU-LTI submission to the SIGMORPHON 2020 Shared Task 0 on typologically diverse morphological inflection. The (unrestricted) submission uses the cross-lingual approach of our last year’s winning submission (Anastasopoulos and Neubig, 2019), but adapted to use specific transfer languages for each test language. Our system, with fixed non-tuned hyperparameters, achieved a macro-averaged accuracy of 80.65 ranking 20th among 31 systems, but it was still tied for best system in 25 of the 90 total languages.",0,0,0,,,,,,,,,,,,,
2020.sigmorphon-1.8,The NYU-CUBoulder Systems for SIGMORPHON 2020 Task 0 and Task 2,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.8/,sigmorphon,,"We describe the NYU-CUBoulder systems for the SIGMORPHON 2020 Task 0 on typologically diverse morphological inflection and Task 2 on unsupervised morphological paradigm completion. The former consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. The latter requires generating entire paradigms for a set of given lemmas from raw text alone. We model morphological inflection as a sequence-to-sequence problem, where the input is the sequence of the lemma’s characters with morphological tags, and the output is the sequence of the inflected form’s characters. First, we apply a transformer model to the task. Second, as inflected forms share most characters with the lemma, we further propose a pointer-generator transformer model to allow easy copying of input characters.",0,0,0,,,,,,,,,,,,,
2020.sigmorphon-1.14,Exploring Neural Architectures And Techniques For Typologically Diverse Morphological Inflection,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.14/,sigmorphon,,"Morphological inflection in low resource languages is critical to augment existing corpora in Low Resource Languages, which can help develop several applications in these languages with very good social impact. We describe our attention-based encoder-decoder approach that we implement using LSTMs and Transformers as the base units. We also describe the ancillary techniques that we experimented with, such as hallucination, language vector injection, sparsemax loss and adversarial language network alongside our approach to select the related language(s) for training. We present the results we generated on the constrained as well as unconstrained SIGMORPHON 2020 dataset (CITATION). One of the primary goals of our paper was to study the contribution varied components described above towards the performance of our system and perform an analysis on the same.",0,1,0,,,,p,,,,i,,"gsw,kan,liv,lug,sot,syc,xty,zpv,zul",,,
2020.sigmorphon-1.15,University of Illinois Submission to the SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.15/,sigmorphon,,"The objective of this shared task is to produce an inflected form of a word, given its lemma and a set of tags describing the attributes of the desired form. In this paper, we describe a transformer-based model that uses a bidirectional decoder to perform this task, and evaluate its performance on the 90 languages and 18 language families used in this task.",0,0,0,,,,,,,,,,,,,
2020.sigmorphon-1.20,The UniMelb Submission to the SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.20/,sigmorphon,,"The paper describes the University of Melbourne’s submission to the SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection. Our team submitted three systems in total, two neural and one non-neural. Our analysis of systems’ performance shows positive effects of newly introduced data hallucination technique that we employed in one of neural systems, especially in low-resource scenarios. A non-neural system based on observed inflection patterns shows optimistic results even in its simple implementation (>75% accuracy for 50% of languages). With possible improvement within the same modeling principle, accuracy might grow to values above 90%.",0,0,0,,,,,,,,,,,,,
2020.sigmorphon-1.22,Transliteration for Cross-Lingual Morphological Inflection,2020-01-01,https://aclanthology.org/2020.sigmorphon-1.22/,sigmorphon,,"Cross-lingual transfer between typologically related languages has been proven successful for the task of morphological inflection. However, if the languages do not share the same script, current methods yield more modest improvements. We explore the use of transliteration between related languages, as well as grapheme-to-phoneme conversion, as data preprocessing methods in order to alleviate this issue. We experimented with several diverse language pairs, finding that in most cases transliterating the transfer language data into the target one leads to accuracy improvements, even up to 9 percentage points. Converting both languages into a shared space like the International Phonetic Alphabet or the Latin alphabet is also beneficial, leading to improvements of up to 16 percentage points.",0,0,0,,,,,,,,,,,,,
P17-1137,Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling,2017-01-01,https://aclanthology.org/P17-1137/,acl,,"Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the “bursty” distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",1,1,1,0,0,0,p,p,0,0,l,n,"ces,deu,eng,fin,fra,rus,spa","ces,deu,eng,fin,fra,rus,spa","ces,deu,eng,fin,fra,rus,spa","ces,deu,eng,fin,fra,rus,spa"
2021.sigtyp-1.6,FrameNet and Typology,2021-01-01,https://aclanthology.org/2021.sigtyp-1.6/,sigtyp,,"FrameNet and the Multilingual FrameNet project have produced multilingual semantic annotations of parallel texts that yield extremely fine-grained typological insights. Moreover, frame semantic annotation of a wide cross-section of languages would provide information on the limits of Frame Semantics (Fillmore 1982, Fillmore1985). Multilingual semantic annotation offers critical input for research on linguistic diversity and recurrent patterns in computational typology. Drawing on results from FrameNet annotation of parallel texts, this paper proposes frame semantic annotation as a new component to complement the state of the art in computational semantic typology.",0,0,1,0,,0,,b,,1,,i,,"deu,eng,fra,jpn,por,spa,swe,zho",,
2022.mia-1.11,MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,2022-01-01,https://aclanthology.org/2022.mia-1.11/,mia,,"We present the results of the Workshop on Multilingual Information Access (MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question answering (QA) systems in 16 typologically diverse languages. In this task, we adapted two large-scale cross-lingual open-retrieval QA datasets in 14 typologically diverse languages, and newly annotated open-retrieval QA data in 2 underrepresented languages: Tagalog and Tamil. Four teams submitted their systems. The best constrained system uses entity-aware contextualized representations for document retrieval, thereby achieving an average F1 score of 31.6, which is 4.1 F1 absolute higher than the challenging baseline. The best system obtains particularly significant improvements in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores. The best unconstrained system achieves 32.2 F1, outperforming our baseline by 4.5 points.",1,1,1,1,1,0,b,b,0,0,i,n,"ara,ben,eng,fil,fin,jpn,khm,kor,mal,rus,spa,swe,tam,tel,zho","ara,ben,eng,fil,fin,jpn,khm,kor,msa,rus,spa,swe,tam,tel,zho","ara,ben,eng,fil,fin,jpn,khm,kor,mal,rus,spa,swe,tam,tel,zho","arb,ben,eng,tgl,fin,jpn,khm,kor,mal,rus,spa,swe,tam,tel,cmn"
2022.digitam-1.6,Towards a Unified ASR System for the Armenian Standards,2022-01-01,https://aclanthology.org/2022.digitam-1.6/,digitam,,"Armenian is a traditionally under-resourced language, which has seen a recent uptick in interest in the development of its tools and presence in the digital domain. Some of this recent interest has centred around the development of Automatic Speech Recognition (ASR) technologies. However, the language boasts two standard variants which diverge on multiple typological and structural levels. In this work, we examine some of the available bodies of data for ASR construction, present the challenges in the processing of these data and propose a methodology going forward.",0,0,0,,,,,,,,,,,,,
2020.dmr-1.4,Cross-lingual annotation: a road map for low- and no-resource languages,2020-01-01,https://aclanthology.org/2020.dmr-1.4/,dmr,,"This paper presents a “road map” for the annotation of semantic categories in typologically diverse languages, with potentially few linguistic resources, and often no existing computational resources. Past semantic annotation efforts have focused largely on high-resource languages, or relatively low-resource languages with a large number of native speakers. However, there are certain typological traits, namely the synthesis of multiple concepts into a single word, that are more common in languages with a smaller speech community. For example, what is expressed as a sentence in a more analytic language like English, may be expressed as a single word in a more synthetic language like Arapaho. This paper proposes solutions for annotating analytic and synthetic languages in a comparable way based on existing typological research, and introduces a road map for the annotation of languages with a dearth of resources.",0,0,1,,,,,d,,,,i,,,,
2021.naacl-main.283,X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering,2021-01-01,https://aclanthology.org/2021.naacl-main.283/,naacl,,"Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.",1,1,1,0,0,0,p,b,0,1,i,l,"ara,ben,fin,ind,rus,spa,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,spa,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,spa,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,spa,swh,tel,tha"
2021.naacl-main.350,How (Non-)Optimal is the Lexicon?,2021-01-01,https://aclanthology.org/2021.naacl-main.350/,naacl,,"The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf’s law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world’s languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon’s optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes—as measured by code length.",1,1,1,0,0,0,p,p,1,1,l,f,"eng,fin,heb,ind,tam,tur,yor","eng,fin,heb,ind,tam,tur,yor","eng,fin,heb,ind,tam,tur,yor","eng,fin,heb,ind,tam,tur,yor"
2020.repl4nlp-1.7,Improving Bilingual Lexicon Induction with Unsupervised Post-Processing of Monolingual Word Vector Spaces,2020-01-01,https://aclanthology.org/2020.repl4nlp-1.7/,repl4nlp,,"Work on projection-based induction of cross-lingual word embedding spaces (CLWEs) predominantly focuses on the improvement of the projection (i.e., mapping) mechanisms. In this work, in contrast, we show that a simple method for post-processing monolingual embedding spaces facilitates learning of the cross-lingual alignment and, in turn, substantially improves bilingual lexicon induction (BLI). The post-processing method we examine is grounded in the generalisation of first- and second-order monolingual similarities to the nth-order similarity. By post-processing monolingual spaces before the cross-lingual alignment, the method can be coupled with any projection-based method for inducing CLWE spaces. We demonstrate the effectiveness of this simple monolingual post-processing across a set of 15 typologically diverse languages (i.e., 15*14 BLI setups), and in combination with two different projection methods.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,cat,epo,est,eus,fin,heb,hun,ind,kat,kor,lit,nob,tha,tur","bul,cat,epo,est,eus,fin,heb,hun,ind,kat,kor,lit,nor,tha,tur","bul,cat,epo,est,eus,fin,heb,hun,ind,kat,kor,lit,nor,tha,tur","bul,cat,epo,ekk,eus,fin,heb,hun,ind,kat,kor,lit,nor,tha,tur"
C18-1245,Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level,2018-01-01,https://aclanthology.org/C18-1245/,coling,,"Emotion Representation Mapping (ERM) has the goal to convert existing emotion ratings from one representation format into another one, e.g., mapping Valence-Arousal-Dominance annotations for words or sentences into Ekman’s Basic Emotions and vice versa. ERM can thus not only be considered as an alternative to Word Emotion Induction (WEI) techniques for automatic emotion lexicon construction but may also help mitigate problems that come from the proliferation of emotion representation formats in recent years. We propose a new neural network approach to ERM that not only outperforms the previous state-of-the-art. Equally important, we present a refined evaluation methodology and gather strong evidence that our model yields results which are (almost) as reliable as human annotations, even in cross-lingual settings. Based on these results we generate new emotion ratings for 13 typologically diverse languages and claim that they have near-gold quality, at least.",1,1,1,0,1,0,d,b,0,0,n,n,"deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,zho","deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,zho","deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,zho","deu,ell,eng,fin,fra,ind,ita,nld,pol,por,spa,swe,cmn"
C18-1249,Clausal Modifiers in the Grammar Matrix,2018-01-01,https://aclanthology.org/C18-1249/,coling,,"We extend the coverage of an existing grammar customization system to clausal modifiers, also referred to as adverbial clauses. We present an analysis, taking a typologically-driven approach to account for this phenomenon across the world’s languages, which we implement in the Grammar Matrix customization system (Bender et al., 2002, 2010). Testing our analysis on testsuites from five genetically and geographically diverse languages that were not considered in development, we achieve 88.4% coverage and 1.5% overgeneration.",1,1,1,0,0,0,b,p,1,0,i,n,"cas,eus,lvk,mhi,ura","deu,dru,wmb,zho","cas,eus,lvk,mhi,ura","cas,eus,lvk,mhi,ura"
2022.emnlp-main.290,On the Calibration of Massively Multilingual Language Models,2022-01-01,https://aclanthology.org/2022.emnlp-main.290/,emnlp,,"Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",0,0,1,,,,,p,,,,n,,"ara,bul,deu,ell,eng,fra,hin,ind,ita,jpn,kor,rus,spa,swa,tam,tha,tur,urd,vie,zho",,
2022.emnlp-main.492,"Eeny, meeny, miny, moe. How to choose data for morphological inflection.",2022-01-01,https://aclanthology.org/2022.emnlp-main.492/,emnlp,,"Data scarcity is a widespread problem for numerous natural language processing (NLP) tasks within low-resource languages. Within morphology, the labour-intensive task of tagging/glossing data is a serious bottleneck for both NLP and fieldwork. Active learning (AL) aims to reduce the cost of data annotation by selecting data that is most informative for the model. In this paper, we explore four sampling strategies for the task of morphological inflection using a Transformer model: a pair of oracle experiments where data is chosen based on correct/incorrect predictions by the model, model confidence, entropy, and random selection. We investigate the robustness of each sampling strategy across 30 typologically diverse languages, as well as a 10-cycle iteration using Natügu as a case study. Our results show a clear benefit to selecting data based on model confidence. Unsurprisingly, the oracle experiment, which is presented as a proxy for linguist/language informer feedback, shows the most improvement. This is followed closely by low-confidence and high-entropy forms. We also show that despite the conventional wisdom of larger data sets yielding better accuracy, introducing more instances of high-confidence, low-entropy, or forms that the model can already inflect correctly, can reduce model performance.",1,1,1,0,0,0,p,p,1,0,l,i,"ady,amh,ara,arp,aym,cni,cpa,cre,ddo,deu,eus,evn,fas,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swa,tur,zul","ady,amh,ara,arp,aym,cni,cpa,cre,ddo,deu,eus,evn,fas,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swa,tur,zul","ady,amh,ara,arp,aym,cni,cpa,cre,ddo,deu,eus,evn,fas,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swa,tur,zul","ady,amh,arb,arp,ayr,cpx,cpa,cre,ddo,deu,eus,evn,pes,fin,gle,guj,hai,ind,khk,kor,mni,nav,ntu,que,rus,see,spa,swh,tur,zul"
2022.emnlp-main.503,Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages,2022-01-01,https://aclanthology.org/2022.emnlp-main.503/,emnlp,,"Pre-trained multilingual models, such as mBERT, XLM-R and mT5, are used to improve the performance on various tasks in low-resource languages via cross-lingual transfer. In this framework, English is usually seen as the most natural choice for a transfer language (for fine-tuning or continued training of a multilingual pre-trained model), but it has been revealed recently that this is often not the best choice. The success of cross-lingual transfer seems to depend on some properties of languages, which are currently hard to explain. Successful transfer often happens between unrelated languages and it often cannot be explained by data-dependent factors. In this study, we show that languages written in non-Latin and non-alphabetic scripts (mostly Asian languages) are the best choices for improving performance on the task of Masked Language Modelling (MLM) in a diverse set of 30 low-resource languages and that the success of the transfer is well predicted by our novel measure of Subword Evenness (SuE). Transferring language models over the languages that score low on our measure results in the lowest average perplexity over target low-resource languages. Our correlation coefficients obtained with three different pre-trained multilingual models are consistently higher than all the other predictors, including text-based measures (type-token ratio, entropy) and linguistically motivated choice (genealogical and typological proximity).",0,1,0,,,,p,,,,i,,"aey,amp,ape,apu,ara,arn,bsn,cha,dgz,fij,grn,gug,hae,hau,jac,kat,khk,kjy,las,mig,mlg,mya,naq,qvi,sag,swa,wic,xsu,yad,yaq,yor",,,
2022.emnlp-main.513,Discovering Language-neutral Sub-networks in Multilingual Language Models,2022-01-01,https://aclanthology.org/2022.emnlp-main.513/,emnlp,,"Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on cross-lingual transfer performance, remain open questions. In this work, we conceptualize language neutrality of multilingual models as a function of the overlap between language-encoding sub-networks of these models. We employ the lottery ticket hypothesis to discover sub-networks that are individually optimized for various languages and tasks. Our evaluation across three distinct tasks and eleven typologically-diverse languages demonstrates that sub-networks for different languages are topologically similar (i.e., language-neutral), making them effective initializations for cross-lingual transfer with limited performance degradation.",1,1,1,0,0,0,p,p,0,1,i,f,"ara,deu,eng,fas,fra,hin,rus,spa,swa,urd,zho","ara,deu,eng,fas,fra,hin,rus,spa,swa,urd,zho","ara,deu,eng,fas,fra,hin,rus,spa,swa,urd,zho","arb,deu,eng,pes,fra,hin,rus,spa,swh,urd,cmn"
2022.emnlp-main.532,DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages,2022-01-01,https://aclanthology.org/2022.emnlp-main.532/,emnlp,,"We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset including all collected behavioral data, to foster new research on the translation capabilities of NMT systems for typologically diverse languages.",1,1,1,1,1,1,b,b,1,1,i,l,"ara,ita,nld,tur,ukr,vie","ara,ita,nld,tur,ukr,vie","ara,ita,nld,tur,ukr,vie","arb,ita,nld,tur,ukr,vie"
2022.emnlp-main.552,Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?,2022-01-01,https://aclanthology.org/2022.emnlp-main.552/,emnlp,,"Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and whether it fairly reflects difference between languages. In this work, we investigate the distributions of grammatical relations induced from mBERT in the context of 24 typologically different languages. We demonstrate that the distance between the distributions of different languages is highly consistent with the syntactic difference in terms of linguistic formalisms. Such difference learnt via self-supervision plays a crucial role in the zero-shot transfer performance and can be predicted by variation in morphosyntactic properties between languages. These results suggest that mBERT properly encodes languages in a way consistent with linguistic diversity and provide insights into the mechanism of cross-lingual transfer.",1,1,1,0,0,0,p,p,0,1,f,f,"ara,bul,deu,ell,eng,est,fas,fin,fra,heb,hin,ita,jpn,kor,lat,nld,pol,por,ron,rus,spa,tur,vie,zho","ara,bul,deu,ell,eng,est,fas,fin,fra,heb,hin,ita,jpn,kor,lav,nld,pol,por,ron,rus,spa,tur,vie,zho","ara,bul,deu,ell,eng,est,fas,fin,fra,heb,hin,ita,jpn,kor,lav,nld,pol,por,ron,rus,spa,tur,vie,zho","arb,bul,deu,ell,eng,ekk,pes,fin,fra,heb,hin,ita,jpn,kor,lav,nld,pol,por,ron,rus,spa,tur,vie,cmn"
2022.emnlp-main.607,On Parsing as Tagging,2022-01-01,https://aclanthology.org/2022.emnlp-main.607/,emnlp,,"There are many proposals to reduce constituency parsing to tagging. To figure out what these approaches have in common, we offer a unifying pipeline, which consists of three steps: linearization, learning, and decoding. We prove that classic shift–reduce parsing can be reduced to tetratagging—the state-of-the-art constituency tagger—under two assumptions: right-corner transformation in the linearization step and factored scoring in the learning step. We ask what is the most critical factor that makes parsing-as-tagging methods accurate while being efficient. To answer this question, we empirically evaluate a taxonomy of tagging pipelines with different choices of linearizers, learners, and decoders. Based on the results in English as well as a set of 8 typologically diverse languages, we conclude that the linearization of the derivation tree and its alignment with the input sequence is the most critical factor in achieving accurate parsers as taggers.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eus,fra,heb,hun,kor,pol,swe","deu,eng,eus,fra,heb,hun,kor,pol,swe","deu,eus,fra,heb,hun,kor,pol,swe","deu,eus,fra,heb,hun,kor,pol,swe"
2022.emnlp-main.722,"An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy",2022-01-01,https://aclanthology.org/2022.emnlp-main.722/,emnlp,,"Polysemy is the phenomenon where a single word form possesses two or more related senses. It is an extremely ubiquitous part of natural language and analyzing it has sparked rich discussions in the linguistics, psychology and philosophy communities alike. With scarce attention paid to polysemy in computational linguistics, and even scarcer attention toward quantifying polysemy, in this paper, we propose a novel, unsupervised framework to compute and estimate polysemy scores for words in multiple languages. We infuse our proposed quantification with syntactic knowledge in the form of dependency structures. This informs the final polysemy scores of the lexicon motivated by recent linguistic findings that suggest there is an implicit relation between syntax and ambiguity/polysemy. We adopt a graph based approach by computing the discrete Ollivier Ricci curvature on a graph of the contextual nearest neighbors. We test our framework on curated datasets controlling for different sense distributions of words in 3 typologically diverse languages - English, French and Spanish. The effectiveness of our framework is demonstrated by significant correlations of our quantification with expert human annotated language resources like WordNet. We observe a 0.3 point increase in the correlation coefficient as compared to previous quantification studies in English. Our research leverages contextual language models and syntactic structures to empirically support the widely held theoretical linguistic notion that syntax is intricately linked to ambiguity/polysemy.",1,1,1,0,0,0,p,p,0,0,n,n,"eng,fra,spa","eng,fra,spa","eng,fra,spa","eng,fra,spa"
2022.emnlp-main.730,Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples,2022-01-01,https://aclanthology.org/2022.emnlp-main.730/,emnlp,,"Negative samples have not been efficiently explored in multilingual dense passage retrieval. In this paper, we propose a novel multilingual dense passage retrieval framework, mHFN, to recover and utilize hard and false negative samples. mHFN consists of three key components: 1) a multilingual hard negative sample augmentation module that allows knowledge of indistinguishable passages to be shared across multiple languages and synthesizes new hard negative samples by interpolating representations of queries and existing hard negative samples, 2) a multilingual negative sample cache queue that stores negative samples from previous batches in each language to increase the number of multilingual negative samples used in training beyond the batch size limit, and 3) a lightweight adaptive false negative sample filter that uses generated pseudo labels to separate unlabeled false negative samples and converts them into positive passages in training. We evaluate mHFN on Mr. TyDi, a high-quality multilingual dense passage retrieval dataset covering eleven typologically diverse languages, and experimental results show that mHFN outperforms strong sparse, dense and hybrid baselines and achieves new state-of-the-art performance on all languages. Our source code is available at https://github.com/Magnetic2014/mHFN.",1,1,1,0,0,0,p,p,0,0,i,n,"ara,ben,eng,fin,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,swh,tel,tha"
2022.nlppower-1.7,Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages,2022-01-01,https://aclanthology.org/2022.nlppower-1.7/,nlppower,,"Although recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape. We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages. We compare performance prediction with translating test data with a case study on four different multilingual datasets, and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.",0,0,0,,,,,,,,,,,,,
L12-1174,SMALLWorlds – Multilingual Content-Controlled Monologues,2012-01-01,https://aclanthology.org/L12-1174/,lrec,,"We present the speech corpus SMALLWorlds (Spoken Multi-lingual Accounts of Logically Limited Worlds), newly established and still growing. SMALLWorlds contains monologic descriptions of scenes or worlds which are simple enough to be formally describable. The descriptions are instances of content-controlled monologue: semantically """"""""pre-specified"""""""" but still bearing most hallmarks of spontaneous speech (hesitations and filled pauses, relaxed syntax, repetitions, self-corrections, incomplete constituents, irrelevant or redundant information, etc.) as well as idiosyncratic speaker traits. In the paper, we discuss the pros and cons of data so elicited. Following that, we present a typical SMALLWorlds task: the description of a simple drawing with differently coloured circles, squares, and triangles, with no hints given as to which description strategy or language style to use. We conclude with an example on how SMALLWorlds may be used: unsupervised lexical learning from phonetic transcription. At the time of writing, SMALLWorlds consists of more than 250 recordings in a wide range of typologically diverse languages from many parts of the world, some unwritten and endangered.",1,1,1,1,1,1,d,d,0,0,i,n,"ara,aze,ben,bos,bul,cat,ces,dan,deu,ell,eng,fas,fin,fra,fuc,gle,gsw,heb,hin,hrv,hun,ibo,isl,ita,jpn,khm,kjg,kor,lav,lit,ltz,new,nld,nob,npi,pan,pol,por,ron,rus,sci,spa,swe,tam,tcx,tha,tur,ukr,urd,vie,wuu,yue,zho","ara,dan,deu,eng,fas,fin,fra,hin,ita,jpn,kor,nld,rus,spa,swe,tam,tur,zho","ara,aze,ben,bos,bul,cat,ces,dan,deu,ell,eng,fas,fin,fra,fuc,gle,gsw,heb,hin,hrv,hun,ibo,isl,ita,jpn,khm,kjg,kor,lav,lit,ltz,new,nld,nob,npi,pan,pol,por,ron,rus,sci,spa,swe,tam,tcx,tha,tur,ukr,urd,vie,wuu,yue,zho","arb,azj,ben,bos,bul,cat,ces,dan,deu,ell,eng,pes,fin,fra,fuc,gle,gsw,heb,hin,hrv,hun,ibo,isl,ita,jpn,khm,kjg,kor,lav,lit,ltz,new,nld,nor,npi,pan,pol,por,ron,rus,sci,spa,swe,tam,tcx,tha,tur,ukr,urd,vie,wuu,yue,cmn"
2021.findings-emnlp.60,How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?,2021-01-01,https://aclanthology.org/2021.findings-emnlp.60/,findings,,"Data-driven subword segmentation has become the default strategy for open-vocabulary machine translation and other NLP tasks, but may not be sufficiently generic for optimal learning of non-concatenative morphology. We design a test suite to evaluate segmentation strategies on different types of morphological phenomena in a controlled, semi-synthetic setting. In our experiments, we compare how well machine translation models trained on subword- and character-level can translate these morphological phenomena. We find that learning to analyse and generate morphologically complex surface representations is still challenging, especially for non-concatenative morphological phenomena like reduplication or vowel harmony and for rare word stems. Based on our results, we recommend that novel text representation strategies be tested on a range of typologically diverse languages to minimise the risk of adopting a strategy that inadvertently disadvantages certain languages.",0,0,0,,,,,,,,,,,,,
2021.findings-emnlp.75,Discovering Representation Sprachbund For Multilingual Pre-Training,2021-01-01,https://aclanthology.org/2021.findings-emnlp.75/,findings,,"Multilingual pre-trained models have demonstrated their effectiveness in many multilingual NLP tasks and enabled zero-shot or few-shot transfer from high-resource languages to low-resource ones. However, due to significant typological differences and contradictions between some languages, such models usually perform poorly on many languages and cross-lingual settings, which shows the difficulty of learning a single model to handle massive diverse languages well at the same time. To alleviate this issue, we present a new multilingual pre-training pipeline. We propose to generate language representation from multilingual pre-trained model and conduct linguistic analysis to show that language representation similarity reflects linguistic similarity from multiple perspectives, including language family, geographical sprachbund, lexicostatistics, and syntax. Then we cluster all the target languages into multiple groups and name each group as a representation sprachbund. Thus, languages in the same representation sprachbund are supposed to boost each other in both pre-training and fine-tuning as they share rich linguistic similarity. We pre-train one multilingual model for each representation sprachbund. Experiments are conducted on cross-lingual benchmarks and significant improvements are achieved compared to strong baselines.",0,0,1,,,,,p,,1,,i,,"ara,bul,deu,ell,eng,fra,hin,rus,spa,swa,tha,tur,urd,vie,zho",,
2021.findings-emnlp.114,Multilingual Neural Machine Translation: Can Linguistic Hierarchies Help?,2021-01-01,https://aclanthology.org/2021.findings-emnlp.114/,findings,,"Multilingual Neural Machine Translation (MNMT) trains a single NMT model that supports translation between multiple languages, rather than training separate models for different languages. Learning a single model can enhance the low-resource translation by leveraging data from multiple languages. However, the performance of an MNMT model is highly dependent on the type of languages used in training, as transferring knowledge from a diverse set of languages degrades the translation performance due to negative transfer. In this paper, we propose a Hierarchical Knowledge Distillation (HKD) approach for MNMT which capitalises on language groups generated according to typological features and phylogeny of languages to overcome the issue of negative transfer. HKD generates a set of multilingual teacher-assistant models via a selective knowledge distillation mechanism based on the language groups, and then distills the ultimate multilingual model from those assistants in an adaptive way. Experimental results derived from the TED dataset with 53 languages demonstrate the effectiveness of our approach in avoiding the negative transfer effect in MNMT, leading to an improved translation performance (about 1 BLEU score in average) compared to strong baselines.",0,0,0,,,,,,,,,,,,,
2021.findings-emnlp.382,A multilabel approach to morphosyntactic probing,2021-01-01,https://aclanthology.org/2021.findings-emnlp.382/,findings,,"We propose using a multilabel probing task to assess the morphosyntactic representations of multilingual word embeddings. This tweak on canonical probing makes it easy to explore morphosyntactic representations, both holistically and at the level of individual features (e.g., gender, number, case), and leads more naturally to the study of how language models handle co-occurring features (e.g., agreement phenomena). We demonstrate this task with multilingual BERT (Devlin et al., 2018), training probes for seven typologically diverse languages: Afrikaans, Croatian, Finnish, Hebrew, Korean, Spanish, and Turkish. Through this simple but robust paradigm, we verify that multilingual BERT renders many morphosyntactic features simultaneously extractable. We further evaluate the probes on six held-out languages: Arabic, Chinese, Marathi, Slovenian, Tagalog, and Yoruba. This zero-shot style of probing has the added benefit of revealing which cross-linguistic properties a language model recognizes as being shared by multiple languages.",1,1,1,0,0,0,p,p,0,0,l,n,"afr,fin,heb,hrv,kor,spa,tur","afr,ara,fil,fin,heb,hrv,kor,mar,slv,spa,tur,yor,zho","afr,ara,fil,fin,heb,hrv,kor,mar,slv,spa,tur,yor,zho","afr,arb,tgl,fin,heb,hrv,kor,mar,slv,spa,tur,yor,cmn"
2020.tacl-1.30,TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,2020-01-01,https://aclanthology.org/2020.tacl-1.30/,tacl,,"Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA—a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology—the set of linguistic features each language expresses—such that we expect models performing well on this set to generalize across a large number of the world’s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don’t know the answer yet, and the data is collected directly in each language without the use of translation.",1,1,1,1,1,1,b,d,1,0,l,n,"ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,swh,tel,tha"
L06-1017,"Hierarchical Relationships “is-a”: Distinguishing Belonging, Inclusion and Part/of Relationships.",2006-01-01,https://aclanthology.org/L06-1017/,lrec,,"In thesauri, conceptual structures or semantic networks, relationships are too often vague. For instance, in terminology, the relationships between concepts are often reduced to the distinction established by standard (ISO 704, 1987) and (ISO 1087, 1990) between hierarchical relationships (genus-species relationships and part/whole relationships) and non-hierarchical relationships (time, space, causal relationships, etc.). The semantics of relationships are vague because the principal users of these relationships are industrial actors (translators of technical handbooks, terminologists, data-processing specialists, etc.). Nevertheless, the consistency of the models built must always be guaranteed... One possible approach to this problem consists in organizing the relationships in a typology based on logical properties. For instance, we typically use only the general relation Is-a. It is too vague. We assume that general relation Is-a is characterized by asymmetry. This asymmetry is specified in: (1) the belonging of one individualizable entity to a distributive class, (2) Inclusion among distributive classes and (3) relation part of (or composition).",0,0,0,,,,,,,,,,,,,
2023.findings-eacl.49,Cross-Lingual Transfer of Cognitive Processing Complexity,2023-01-01,https://aclanthology.org/2023.findings-eacl.49/,findings,,"When humans read a text, their eye movements are influenced by the structural complexity of the input sentences. This cognitive phenomenon holds across languages and recent studies indicate that multilingual language models utilize structural similarities between languages to facilitate cross-lingual transfer. We use sentence-level eye-tracking patterns as a cognitive indicator for structural complexity and show that the multilingual model XLM-RoBERTa can successfully predict varied patterns for 13 typologically diverse languages, despite being fine-tuned only on English data. We quantify the sensitivity of the model to structural complexity and distinguish a range of complexity characteristics. Our results indicate that the model develops a meaningful bias towards sentence length but also integrates cross-lingual differences. We conduct a control experiment with randomized word order and find that the model seems to additionally capture more complex structural information.",1,1,0,0,0,0,p,p,0,,n,,,,,
2023.findings-acl.885,Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages,2023-01-01,https://aclanthology.org/2023.findings-acl.885/,findings,,"While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets. In this work, we propose Cross-Lingual Knowledge Distillation (CLKD) from a strong English AS2 teacher as a method to train AS2 models for low-resource languages in the tasks without the need of labeled data for the target language. To evaluate our method, we introduce 1) Xtr-WikiQA, a translation-based WikiQA dataset for 9 additional languages, and 2) TyDi-AS2, a multilingual AS2 dataset with over 70K questions spanning 8 typologically diverse languages. We conduct extensive experiments on Xtr-WikiQA and TyDi-AS2 with multiple teachers, diverse monolingual and multilingual pretrained language models (PLMs) as students, and both monolingual and multilingual training. The results demonstrate that CLKD either outperforms or rivals even supervised fine-tuning with the same amount of labeled data and a combination of machine translation and the teacher model. Our method can potentially enable stronger AS2 models for low-resource languages, while TyDi-AS2 can serve as the largest multilingual AS2 dataset for further studies in the research community.",1,1,1,1,1,1,b,b,0,0,n,n,"ben,eng,fin,ind,jpn,kor,rus,swa","ben,eng,fin,ind,jpn,kor,rus,swa","ben,eng,fin,ind,jpn,kor,rus,swa","ben,eng,fin,ind,jpn,kor,rus,swh"
2023.findings-emnlp.52,Improving Cross-lingual Transfer through Subtree-aware Word Reordering,2023-01-01,https://aclanthology.org/2023.findings-emnlp.52/,findings,,"Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via source- or target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consistently outperforms strong baselines over different language pairs and model architectures. This performance advantage holds true in both zero-shot and few-shot scenarios.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,deu,fas,fra,gle,hin,kor,spa,tha","ara,deu,fas,fra,gle,hin,kor,spa,tha","ara,deu,fas,fra,gle,hin,kor,spa,tha","arb,deu,pes,fra,gle,hin,kor,spa,tha"
2023.findings-emnlp.664,JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing,2023-01-01,https://aclanthology.org/2023.findings-emnlp.664/,findings,,"Advancements in sign language processing have been hindered by a lack of sufficient data, impeding progress in recognition, translation, and production tasks. The absence of comprehensive sign language datasets across the world’s sign languages has widened the gap in this field, resulting in a few sign languages being studied more than others, making this research area extremely skewed mostly towards sign languages from high-income countries. In this work we introduce a new large and highly multilingual dataset for sign language translation: JWSign. The dataset consists of 2,530 hours of Bible translations in 98 sign languages, featuring more than 1,500 individual signers. On this dataset, we report neural machine translation experiments. Apart from bilingual baseline systems, we also train multilingual systems, including some that take into account the typological relatedness of signed or spoken languages. Our experiments highlight that multilingual systems are superior to bilingual baselines, and that in higher-resource scenarios, clustering language pairs that are related improves translation quality.",0,0,0,,,,,,,,,,,,,
2022.lrec-1.123,TeDDi Sample: Text Data Diversity Sample for Language Comparison and Multilingual NLP,2022-01-01,https://aclanthology.org/2022.lrec-1.123/,lrec,,"We present the TeDDi sample, a diversity sample of text data for language comparison and multilingual Natural Language Processing. The TeDDi sample currently features 89 languages based on the typological diversity sample in the World Atlas of Language Structures. It consists of more than 20k texts and is accompanied by open-source corpus processing tools. The aim of TeDDi is to facilitate text-based quantitative analysis of linguistic diversity. We describe in detail the TeDDi sample, how it was created, data availability, and its added value through for NLP and linguistic research.",0,1,0,,,,d,,,,i,,,,,
2022.lrec-1.299,Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of Lexical Gaps in Kinship,2022-01-01,https://aclanthology.org/2022.lrec-1.299/,lrec,,"This paper describes a method to enrich lexical resources with content relating to linguistic diversity, based on knowledge from the field of lexical typology. We capture the phenomenon of diversity through the notion of lexical gap and use a systematic method to infer gaps semi-automatically on a large scale, which we demonstrate on the kinship domain. The resulting free diversity-aware terminological resource consists of 198 concepts, 1,911 words, and 37,370 gaps in 699 languages. We see great potential in the use of resources such as ours for the improvement of a variety of cross-lingual NLP tasks, which we illustrate through an application in the evaluation of machine translation systems.",0,0,1,0,,0,,,,1,,,,,,
2022.lrec-1.309,Overlooked Data in Typological Databases: What Grambank Teaches Us About Gaps in Grammars,2022-01-01,https://aclanthology.org/2022.lrec-1.309/,lrec,,"Typological databases can contain a wealth of information beyond the collection of linguistic properties across languages. This paper shows how information often overlooked in typological databases can inform the research community about the state of description of the world’s languages. We illustrate this using Grambank, a morphosyntactic typological database covering 2,467 language varieties and based on 3,951 grammatical descriptions. We classify and quantify the comments that accompany coded values in Grambank. We then aggregate these comments and the coded values to derive a level of description for 17 grammatical domains that Grambank covers (negation, adnominal modification, participant marking, tense, aspect, etc.). We show that the description level of grammatical domains varies across space and time. Information about gaps and uncertainties in the descriptive knowledge of grammatical domains within and across languages is essential for a correct analysis of data in typological databases and for the study of grammatical diversity more generally. When collected in a database, such information feeds into disciplines that focus on primary data collection, such as grammaticography and language documentation.",0,0,0,,,,,,,,,,,,,
2022.lrec-1.355,Multilingual Pragmaticon: Database of Discourse Formulae,2022-01-01,https://aclanthology.org/2022.lrec-1.355/,lrec,,"The paper presents a multilingual database aimed to be used as a tool for typological analysis of response constructions called discourse formulae (DF), cf. English ‘No way¡ or French ‘Ça va¡ ( ‘all right’). The two primary qualities that make DF of theoretical interest for linguists are their idiomaticity and the special nature of their meanings (cf. consent, refusal, negation), determined by their dialogical function. The formal and semantic structures of these items are language-specific. Compiling a database with DF from various languages would help estimate the diversity of DF in both of these aspects, and, at the same time, establish some frequently occurring patterns. The DF in the database are accompanied with glosses and assigned with multiple tags, such as pragmatic function, additional semantics, the illocutionary type of the context, etc. As a starting point, Russian, Serbian and Slovene DF are included into the database. This data already shows substantial grammatical and lexical variability.",0,0,0,,,,,,,,,,,,,
2022.lrec-1.517,RRGparbank: A Parallel Role and Reference Grammar Treebank,2022-01-01,https://aclanthology.org/2022.lrec-1.517/,lrec,,"This paper describes the first release of RRGparbank, a multilingual parallel treebank for Role and Reference Grammar (RRG) containing annotations of George Orwell’s novel 1984 and its translations. The release comprises the entire novel for English and a constructionally diverse and highly parallel sample (“seed”) for German, French and Russian. The paper gives an overview of annotation decisions that have been taken and describes the adopted treebanking methodology. Finally, as a possible application, a multilingual parser is trained on the treebank data. RRGparbank is one of the first resources to apply RRG to large amounts of real-world data. Furthermore, it enables comparative and typological corpus studies in RRG. And, finally, it creates new possibilities of data-driven NLP applications based on RRG.",0,0,0,,,,,,,,,,,,,
2022.lrec-1.527,Evaluating Pre-training Objectives for Low-Resource Translation into Morphologically Rich Languages,2022-01-01,https://aclanthology.org/2022.lrec-1.527/,lrec,,"The scarcity of parallel data is a major limitation for Neural Machine Translation (NMT) systems, in particular for translation into morphologically rich languages (MRLs). An important way to overcome the lack of parallel data is to leverage target monolingual data, which is typically more abundant and easier to collect. We evaluate a number of techniques to achieve this, ranging from back-translation to random token masking, on the challenging task of translating English into four typologically diverse MRLs, under low-resource settings. Additionally, we introduce Inflection Pre-Training (or PT-Inflect), a novel pre-training objective whereby the NMT system is pre-trained on the task of re-inflecting lemmatized target sentences before being trained on standard source-to-target language translation. We conduct our evaluation on four typologically diverse target MRLs, and find that PT-Inflect surpasses NMT systems trained only on parallel data. While PT-Inflect is outperformed by back-translation overall, combining the two techniques leads to gains in some of the evaluated language pairs.",1,1,1,0,0,0,p,p,1,0,l,n,"deu,est,lit,tam,tur","deu,est,lit,tam,tur","deu,est,lit,tam,tur","deu,ekk,lit,tam,tur"
2022.lrec-1.612,MASALA: Modelling and Analysing the Semantics of Adpositions in Linguistic Annotation of Hindi,2022-01-01,https://aclanthology.org/2022.lrec-1.612/,lrec,,"We present a completed, publicly available corpus of annotated semantic relations of adpositions and case markers in Hindi. We used the multilingual SNACS annotation scheme, which has been applied to a variety of typologically diverse languages. Building on past work examining linguistic problems in SNACS annotation, we use language models to attempt automatic labelling of SNACS supersenses in Hindi and achieve results competitive with past work on English. We look towards upstream applications in semantic role labelling and extension to related languages such as Gujarati.",0,0,0,,,,,,,,,,,,,
2020.alw-1.15,Six Attributes of Unhealthy Conversations,2020-01-01,https://aclanthology.org/2020.alw-1.15/,alw,,"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either ‘healthy’ or ‘unhealthy’, in addition to binary labels for the presence of six potentially ‘unhealthy’ sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score. We argue that there is a need for datasets which enable research based on a broad notion of ‘unhealthy online conversation’. We build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation. For some of these attributes, this is the first publicly available dataset of this scale. We explore the quality of the dataset, present some summary statistics and initial models to illustrate the utility of this data, and highlight limitations and directions for further research.",0,0,0,,,,,,,,,,,,,
2022.nlpcss-1.10,Understanding Interpersonal Conflict Types and their Impact on Perception Classification,2022-01-01,https://aclanthology.org/2022.nlpcss-1.10/,nlpcss,,"Studies on interpersonal conflict have a long history and contain many suggestions for conflict typology. We use this as the basis of a novel annotation scheme and release a new dataset of situations and conflict aspect annotations. We then build a classifier to predict whether someone will perceive the actions of one individual as right or wrong in a given situation. Our analyses include conflict aspects, but also generated clusters, which are human validated, and show differences in conflict content based on the relationship of participants to the author. Our findings have important implications for understanding conflict and social norms.",0,0,0,,,,,,,,,,,,,
2020.emnlp-main.185,XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,2020-01-01,https://aclanthology.org/2020.emnlp-main.185/,emnlp,,"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",1,1,1,1,1,1,b,d,1,1,i,i,"eng,est,hat,ind,swa,tam,tha,tur,vie,zho","est,hat,ind,ita,que,swa,tam,tha,tur,vie,zho","eng,est,hat,ind,ita,que,swa,tam,tha,tur,vie,zho","eng,ekk,hat,ind,ita,que,swh,tam,tha,tur,vie,cmn"
2020.emnlp-main.186,The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures,2020-01-01,https://aclanthology.org/2020.emnlp-main.186/,emnlp,,"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI, parsing, POS tagging and MT. We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra. We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.",0,0,0,,,,,,,,,,,,,
2020.emnlp-main.257,Are All Good Word Vector Spaces Isomorphic?,2020-01-01,https://aclanthology.org/2020.emnlp-main.257/,emnlp,,"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. “under-training”).",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ben,eng,eus,glg,jpn,que,spa,tam,urd","ara,eng,jpn,spa","ara,ben,eng,eus,glg,jpn,que,spa,tam,urd","arb,ben,eng,eus,glg,jpn,que,spa,tam,urd"
2020.emnlp-main.328,Speakers Fill Lexical Semantic Gaps with Context,2020-01-01,https://aclanthology.org/2020.emnlp-main.328/,emnlp,,"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear—resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this—one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. 𝜌 = 0.40 in English). We then test our main hypothesis—that a word’s lexical ambiguity should negatively correlate with its contextual uncertainty—and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.",1,1,1,0,0,0,p,p,0,0,n,n,"afr,ara,ben,eng,est,fas,fil,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor","afr,ara,ben,eng,est,fas,fil,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor","afr,ara,ben,eng,est,fas,fil,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor","afr,arb,ben,eng,ekk,pes,tgl,fin,heb,ind,isl,kan,mal,mar,por,tat,tur,yor"
2020.emnlp-main.391,Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios,2020-01-01,https://aclanthology.org/2020.emnlp-main.391/,emnlp,,"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.",1,0,1,0,,0,,,,1,,,,"afr,bul,eus,fas,fin,hin,ind,lit,por,tel,tur","afr,bul,eus,fas,fin,hin,ind,lit,por,tel,tur","afr,bul,eus,pes,fin,hin,ind,lit,por,tel,tur"
2020.emnlp-main.479,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,2020-01-01,https://aclanthology.org/2020.emnlp-main.479/,emnlp,,"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as “Punta Cana is located in _.” However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.",1,1,1,1,1,1,b,b,0,0,i,n,"ben,ceb,ell,eng,fil,fra,heb,hun,ilo,jpn,kor,mal,mar,nld,pan,rus,spa,swa,tur,vie,war,yor,zho","ben,ceb,ell,eng,fil,fra,heb,hun,ilo,jpn,kor,mar,mlg,nld,pan,rus,spa,swa,tur,vie,war,yor,zho","ben,ceb,ell,eng,fil,fra,heb,hun,ilo,jpn,kor,mal,mar,nld,pan,rus,spa,swa,tur,vie,war,yor,zho","ben,ceb,ell,eng,tgl,fra,heb,hun,ilo,jpn,kor,mal,mar,nld,pan,rus,spa,swh,tur,vie,war,yor,cmn"
2020.emnlp-main.586,Probing Pretrained Language Models for Lexical Semantics,2020-01-01,https://aclanthology.org/2020.emnlp-main.586/,emnlp,,"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",1,1,1,0,0,0,p,p,0,0,i,n,"deu,eng,fin,rus,tur,zho","deu,eng,fin,ita,rus,tur,zho","deu,eng,fin,ita,rus,tur,zho","deu,eng,fin,ita,rus,tur,cmn"
2020.emnlp-main.617,MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer,2020-01-01,https://aclanthology.org/2020.emnlp-main.617/,emnlp,,"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.",1,1,1,0,0,0,p,p,0,0,i,n,"ara,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swa,tuk,xmf,zho","ara,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swa,tuk,xmf,zho","ara,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swa,tuk,xmf,zho","arb,cdo,eng,grn,ilo,isl,jav,jpn,mhr,mri,mya,que,swh,tuk,xmf,cmn"
2021.ranlp-1.40,Event Prominence Extraction Combining a Knowledge-Based Syntactic Parser and a BERT Classifier for Dutch,2021-01-01,https://aclanthology.org/2021.ranlp-1.40/,ranlp,,"A core task in information extraction is event detection that identifies event triggers in sentences that are typically classified into event types. In this study an event is considered as the unit to measure diversity and similarity in news articles in the framework of a news recommendation system. Current typology-based event detection approaches fail to handle the variety of events expressed in real-world situations. To overcome this, we aim to perform event salience classification and explore whether a transformer model is capable of classifying new information into less and more general prominence classes. After comparing a Support Vector Machine (SVM) baseline and our transformer-based classifier performances on several event span formats, we conceived multi-word event spans as syntactic clauses. Those are fed into our prominence classifier which is fine-tuned on pre-trained Dutch BERT word embeddings. On top of that we outperform a pipeline of a Conditional Random Field (CRF) approach to event-trigger word detection and the BERT-based classifier. To the best of our knowledge we present the first event extraction approach that combines an expert-based syntactic parser with a transformer-based classifier for Dutch.",0,0,0,,,,,,,,,,,,,
2021.ranlp-1.43,Tracing Source Language Interference in Translation with Graph-Isomorphism Measures,2021-01-01,https://aclanthology.org/2021.ranlp-1.43/,ranlp,,"Previous research has used linguistic features to show that translations exhibit traces of source language interference and that phylogenetic trees between languages can be reconstructed from the results of translations into the same language. Recent research has shown that instances of translationese (source language interference) can even be detected in embedding spaces, comparing embeddings spaces of original language data with embedding spaces resulting from translations into the same language, using a simple Eigenvector-based divergence from isomorphism measure. To date, it remains an open question whether alternative graph-isomorphism measures can produce better results. In this paper, we (i) explore Gromov-Hausdorff distance, (ii) present a novel spectral version of the Eigenvector-based method, and (iii) evaluate all approaches against a broad linguistic typological database (URIEL). We show that language distances resulting from our spectral isomorphism approaches can reproduce genetic trees on a par with previous work without requiring any explicit linguistic information and that the results can be extended to non-Indo-European languages. Finally, we show that the methods are robust under a variety of modeling conditions.",0,0,0,,,,,,,,,,,,,
2021.ranlp-1.48,Improving Character-Aware Neural Language Model by Warming up Character Encoder under Skip-gram Architecture,2021-01-01,https://aclanthology.org/2021.ranlp-1.48/,ranlp,,"Character-aware neural language models can capture the relationship between words by exploiting character-level information and are particularly effective for languages with rich morphology. However, these models are usually biased towards information from surface forms. To alleviate this problem, we propose a simple and effective method to improve a character-aware neural language model by forcing a character encoder to produce word-based embeddings under Skip-gram architecture in a warm-up step without extra training data. We empirically show that the resulting character-aware neural language model achieves obvious improvements of perplexity scores on typologically diverse languages, that contain many low-frequency or unseen words.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ces,deu,eng,est,fin,heb,jpn,kan,kor,por,rus,spa,zho","ara,ces,deu,eng,est,fin,heb,jpn,kan,kor,por,rus,spa,zho","ara,ces,deu,eng,est,fin,heb,jpn,kan,kor,por,rus,spa,zho","arb,ces,deu,eng,ekk,fin,heb,jpn,kan,kor,por,rus,spa,cmn"
2020.cl-2.4,LINSPECTOR: Multilingual Probing Tasks for Word Representations,2020-01-01,https://aclanthology.org/2020.cl-2.4/,cl,,"Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.",1,1,1,0,0,0,p,p,0,1,i,f,"deu,fin,rus,spa,tur",,"deu,fin,rus,spa,tur","deu,fin,rus,spa,tur"
2020.cl-4.5,Multi-SimLex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity,2020-01-01,https://aclanthology.org/2020.cl-4.5/,cl,,"We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex–style resources for additional languages. We make these contributions—the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.",1,1,1,1,1,1,b,d,1,0,i,n,"cmn,cym,eng,est,fin,fra,heb,pol,rus,spa,swa,yue","ara,cym,eng,est,fin,fra,heb,pol,rus,spa,swa,yue,zho","cym,eng,est,fin,fra,heb,pol,rus,spa,swa,yue,zho","cym,eng,ekk,fin,fra,heb,pol,rus,spa,swh,yue,cmn"
2021.rocling-1.1,Universal Recurrent Neural Network Grammar,2021-01-01,https://aclanthology.org/2021.rocling-1.1/,rocling,,"Modern approaches to Constituency Parsing are mono-lingual supervised approaches which require large amount of labelled data to be trained on, thus limiting their utility to only a handful of high-resource languages. To address this issue of data-sparsity for low-resource languages we propose Universal Recurrent Neural Network Grammars (UniRNNG) which is a multi-lingual variant of the popular Recurrent Neural Network Grammars (RNNG) model for constituency parsing. UniRNNG involves Cross-lingual Transfer Learning for Constituency Parsing task. The architecture of UniRNNG is inspired by Principle and Parameter theory proposed by Noam Chomsky. UniRNNG utilises the linguistic typology knowledge available as feature-values within WALS database, to generalize over multiple languages. Once trained on sufficiently diverse polyglot corpus UniRNNG can be applied to any natural language thus making it Language-agnostic constituency parser. Experiments reveal that our proposed UniRNNG outperform state-of-the-art baseline approaches for most of the target languages, for which these are tested.",0,0,0,,,,,,,,,,,,,
2020.sigtyp-1.3,NEMO: Frequentist Inference Approach to Constrained Linguistic Typology Feature Prediction in SIGTYP 2020 Shared Task,2020-01-01,https://aclanthology.org/2020.sigtyp-1.3/,sigtyp,,"This paper describes the NEMO submission to SIGTYP 2020 shared task (Bjerva et al., 2020) which deals with prediction of linguistic typological features for multiple languages using the data derived from World Atlas of Language Structures (WALS). We employ frequentist inference to represent correlations between typological features and use this representation to train simple multi-class estimators that predict individual features. We describe two submitted ridge regression-based configurations which ranked second and third overall in the constrained task. Our best configuration achieved the microaveraged accuracy score of 0.66 on 149 test languages.",0,0,0,,,,,,,,,,,,,
2021.eacl-main.189,"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT",2021-01-01,https://aclanthology.org/2021.eacl-main.189/,eacl,,"Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model’s internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,zho","ara,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,zho","ara,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,zho","arb,ces,deu,eng,fin,fra,hin,ind,ita,jpn,pol,por,rus,slv,spa,tur,cmn"
2021.eacl-main.194,Subword Pooling Makes a Difference,2021-01-01,https://aclanthology.org/2021.eacl-main.194/,eacl,,"Contextual word-representations became a standard in modern natural language processing systems. These models use subword tokenization to handle large vocabularies and unknown words. Word-level usage of such systems requires a way of pooling multiple subwords that correspond to a single word. In this paper we investigate how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages. We compare these in two massively multilingual models, mBERT and XLM-RoBERTa. For morphological tasks, the widely used ‘choose the first subword’ is the worst strategy and the best results are obtained by using attention over the subwords. For POS tagging both of these strategies perform poorly and the best choice is to use a small LSTM over the subwords. The same strategy works best for NER and we show that mBERT is better than XLM-RoBERTa in all 9 languages. We publicly release all code, data and the full result tables at https://github.com/juditacs/subword-choice .",1,1,1,0,0,0,p,p,0,0,f,n,"ara,ces,deu,eng,fin,fra,jpn,kor,zho","ara,ces,deu,eng,fin,fra,jpn,kor,zho","ara,ces,deu,eng,fin,fra,jpn,kor,zho","arb,ces,deu,eng,fin,fra,jpn,kor,cmn"
2021.eacl-main.302,From characters to words: the turning point of BPE merges,2021-01-01,https://aclanthology.org/2021.eacl-main.302/,eacl,,"The distributions of orthographic word types are very different across languages due to typological characteristics, different writing traditions and potentially other factors. The wide range of cross-linguistic diversity is still a major challenge for NLP and the study of language. We use BPE and information-theoretic measures to investigate if distributions become similar under specific levels of subword tokenization. We perform a cross-linguistic comparison, following incremental merges of BPE (we go from characters to words) for 47 diverse languages. We show that text entropy values (a feature of probability distributions) tend to converge at specific subword levels: relatively few BPE merges (around 350) lead to the most similar distributions across languages. Additionally, we analyze the interaction between subword and word-level distributions and show that our findings can be interpreted in light of the ongoing discussion regarding different types of morphological complexity.",1,1,1,0,0,0,p,p,,1,i,l,"aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,fil,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swa,tha,tur,vie,xsu,yad,yaq,yor","aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,fil,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,kew,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swh,tha,tur,vie,xsu,yad,yaq,yor","aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,fil,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,kew,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swh,tha,tur,vie,xsu,yad,yaq,yor","aey,amp,ape,apu,arn,arz,bsn,cha,deu,dgz,ell,eng,eus,fij,tgl,fin,fra,gug,hae,hau,hin,ind,jac,kal,kat,kew,khk,kor,laj,mig,mya,mzh,naq,pes,plt,qvi,rus,sag,spa,swh,tha,tur,vie,xsu,yad,yaq,yor"
2023.eacl-main.85,K-hop neighbourhood regularization for few-shot learning on graphs: A case study of text classification,2023-01-01,https://aclanthology.org/2023.eacl-main.85/,eacl,,"We present FewShotTextGCN, a novel method designed to effectively utilize the properties of word-document graphs for improved learning in low-resource settings. We introduce K-hop Neighbourhood Regularization, a regularizer for heterogeneous graphs, and show that it stabilizes and improves learning when only a few training samples are available. We furthermore propose a simplification in the graph-construction method, which results in a graph that is ∼7 times less dense and yields better performance in little-resource settings while performing on par with the state of the art in high-resource settings. Finally, we introduce a new variant of Adaptive Pseudo-Labeling tailored for word-document graphs. When using as little as 20 samples for training, we outperform a strong TextGCN baseline with 17% in absolute accuracy on average over eight languages. We demonstrate that our method can be applied to document classification without any language model pretraining on a wide range of typologically diverse languages while performing on par with large pretrained language models.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eng,fra,ita,jpn,rus,spa,zho","deu,eng,fra,ita,jpn,rus,spa,zho","deu,eng,fra,ita,jpn,rus,spa,zho","deu,eng,fra,ita,jpn,rus,spa,cmn"
2023.eacl-srw.10,Template-guided Grammatical Error Feedback Comment Generation,2023-01-01,https://aclanthology.org/2023.eacl-srw.10/,eacl,,"Writing is an important element of language learning, and an increasing amount of learner writing is taking place in online environments. Teachers can provide valuable feedback by commenting on learner text. However, providing relevant feedback for every issue for every student can be time-consuming. To address this, we turn to the NLP subfield of feedback comment generation, the task of automatically generating explanatory notes for learner text with the goal of enhancing learning outcomes. However, freely-generated comments may mix multiple topics seen in the training data or even give misleading advice. In this thesis proposal, we seek to address these issues by categorizing comments and constraining the outputs of noisy classes. We describe an annotation scheme for feedback comment corpora using comment topics with a broader scope than existing typologies focused on error correction. We outline plans for experiments in grouping and clustering, replacing particularly diverse categories with modular templates, and comparing the generation results of using different linguistic features and model architectures with the original dataset versus the newly annotated one. This paper presents the first two years (the master’s component) of a research project for a five-year combined master’s and Ph.D program.",0,0,0,,,,,,,,,,,,,
2021.acl-long.243,How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models,2021-01-01,https://aclanthology.org/2021.acl-long.243/,"acl, ijcnlp",,"In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.",1,1,1,0,0,0,p,p,1,1,i,i,"ara,eng,fin,ind,jpn,kor,rus,tur,zho","ara,eng,fin,ind,jpn,kor,rus,tur,zho","ara,eng,fin,ind,jpn,kor,rus,tur,zho","arb,eng,fin,ind,jpn,kor,rus,tur,cmn"
2021.acl-short.46,Modeling Task-Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation,2021-01-01,https://aclanthology.org/2021.acl-short.46/,"acl, ijcnlp",,"Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast.",0,0,0,,,,,,,,,,,,,
2021.acl-short.69,When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation,2021-01-01,https://aclanthology.org/2021.acl-short.69/,"acl, ijcnlp",,"Subword segmentation algorithms have been a de facto choice when building neural machine translation systems. However, most of them need to learn a segmentation model based on some heuristics, which may produce sub-optimal segmentation. This can be problematic in some scenarios when the target language has rich morphological changes or there is not enough data for learning compact composition rules. Translating at fully character level has the potential to alleviate the issue, but empirical performances of character-based models has not been fully explored. In this paper, we present an in-depth comparison between character-based and subword-based NMT systems under three settings: translating to typologically diverse languages, training with low resource, and adapting to unseen domains. Experiment results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains.",1,1,1,0,0,0,p,p,1,1,l,l,"ara,fin,fra,heb,ron,tur,vie,zsm","ara,fin,fra,heb,mri,ron,tur,vie","ara,fin,fra,heb,ron,tur,vie,zsm","arb,fin,fra,heb,ron,tur,vie,zsm"
2021.acl-short.72,Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking,2021-01-01,https://aclanthology.org/2021.acl-short.72/,"acl, ijcnlp",,"Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data.",1,1,1,0,0,0,b,p,,0,i,n,"ces,deu,ell,eng,est,fin,fra,hrv,ita,jpn,kor,lav,nld,nob,pol,por,rus,spa,swe,tha,tur,zho","deu,eng,fin,jpn,kor,rus,spa,tha,tur,zho","ces,deu,ell,eng,est,fin,fra,hrv,ita,jpn,kor,lav,nld,nob,pol,por,rus,spa,swe,tha,tur,zho","ces,deu,ell,eng,ekk,fin,fra,hrv,ita,jpn,kor,lav,nld,nor,pol,por,rus,spa,swe,tha,tur,cmn"
2021.starsem-1.22,Inducing Language-Agnostic Multilingual Representations,2021-01-01,https://aclanthology.org/2021.starsem-1.22/,starsem,,"Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this: (i) re-aligning the vector spaces of target languages (all together) to a pivot source language; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approaches—unlike vector normalization, vector space re-alignment and text normalization do not achieve consistent gains across encoders and languages. Due to the approaches’ additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however.",1,1,1,0,0,0,p,p,,1,n,i,"afr,ben,deu,eng,est,fil,fin,fra,hin,hun,ind,ita,jav,mar,nld,por,spa,urd,zsm","afr,ben,deu,eng,est,fil,fin,fra,hin,hun,ind,ita,jav,mar,msa,nld,por,spa,urd","afr,ben,deu,eng,est,fil,fin,fra,hin,hun,ind,ita,jav,mar,nld,por,spa,urd,zsm","afr,ben,deu,eng,ekk,tgl,fin,fra,hin,hun,ind,ita,jav,mar,nld,por,spa,urd,zsm"
2020.jeptalnrecital-taln.19,L’expression des émotions dans les textes pour enfants : constitution d’un corpus annoté (Expressing emotions in texts for children: constitution of an annotated corpus),2020-01-01,https://aclanthology.org/2020.jeptalnrecital-taln.19/,jeptalnrecital,,"Cet article présente une typologie de divers modes d’expression linguistique des émotions, le schéma d’annotation sous Glozz qui implémente cette typologie et un corpus de textes journalistiques pour enfants annoté à l’aide de ce schéma. Ces travaux préliminaires s’insèrent dans le contexte d’une étude relative au développement des capacités langagières des enfants, en particulier de leur capacité à comprendre un texte selon des critères émotionnels.",0,0,0,,,,,,,,,,,,,
P19-1341,A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction,2019-01-01,https://aclanthology.org/P19-1341/,acl,,"We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world’s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show – across typologically diverse languages in PBC+ – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.",1,1,1,0,1,0,p,p,0,0,i,n,"bul,deu,hrv,hun,pol,por,rus,slk,slv,spa,sqi,srp,swe",,"bul,deu,hrv,hun,pol,por,rus,slk,slv,spa,sqi,srp,swe","bul,deu,hrv,hun,pol,por,rus,slk,slv,spa,sqi,srp,swe"
P19-1383,Is Word Segmentation Child’s Play in All Languages?,2019-01-01,https://aclanthology.org/P19-1383/,acl,,"When learning language, infants need to break down the flow of input speech into minimal word-like units, a process best described as unsupervised bottom-up segmentation. Proposed strategies include several segmentation algorithms, but only cross-linguistically robust algorithms could be plausible candidates for human word learning, since infants have no initial knowledge of the ambient language. We report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages. The results consist evidence that some segmentation algorithms are cross-linguistically valid, thus could be considered as potential strategies employed by all infants.",1,1,1,0,0,0,p,p,1,1,i,i,"ind,inu,jpn,rus,ses,tur,yuc,zho","iku,ind,jpn,rus,sot,tur,yua,zho","iku,ind,jpn,rus,sot,tur,yua,zho","iku,ind,jpn,rus,sot,tur,yua,cmn"
W16-3806,Universal Dependencies: A Cross-Linguistic Perspective on Grammar and Lexicon,2016-01-01,https://aclanthology.org/W16-3806/,gramlex,,"Universal Dependencies is an initiative to develop cross-linguistically consistent grammatical annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning and parsing research from a language typology perspective. It assumes a dependency-based approach to syntax and a lexicalist approach to morphology, which together entail that the fundamental units of grammatical annotation are words. Words have properties captured by morphological annotation and enter into relations captured by syntactic annotation. Moreover, priority is given to relations between lexical content words, as opposed to grammatical function words. In this position paper, I discuss how this approach allows us to capture similarities and differences across typologically diverse languages.",0,0,0,,,,,,,,,,,,,
2021.tacl-1.1,Reducing Confusion in Active Learning for Part-Of-Speech Tagging,2021-01-01,https://aclanthology.org/2021.tacl-1.1/,tacl,,"Active learning (AL) uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech (POS) taggers. Existing AL heuristics are generally designed on the principle of selecting uncertain yet representative training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages (German, Swedish, Galician, North Sami, Persian, and Ukrainian), we found the surprising result that even in an oracle scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances that maximally reduce the confusion between particular pairs of output tags. Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin. We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The code is publicly released here.1",1,1,1,0,0,0,p,p,1,0,i,n,"deu,fas,glg,sme,swe,ukr","deu,fas,glg,sme,swe,ukr","deu,fas,glg,sme,swe,ukr","deu,pes,glg,sme,swe,ukr"
2021.tacl-1.25,Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages,2021-01-01,https://aclanthology.org/2021.tacl-1.25/,tacl,,"Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task–language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task–language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods. Our code is available at github.com/cambridgeltl/parameter-factorization.",1,1,1,0,0,0,p,p,0,1,i,i,"aii,amh,ara,bam,cym,est,eus,fao,fil,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,sme,tam,tel,tha,tur,uig,vie,wol,yor,yue","amh,ara,bam,cym,est,eus,fao,fil,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,tam,tel,tha,tur,uig,vie,wol,yor,yue","aii,amh,ara,bam,cym,est,eus,fao,fil,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,sme,tam,tel,tha,tur,uig,vie,wol,yor,yue","aii,amh,arb,bam,cym,ekk,eus,fao,tgl,fin,glg,gun,heb,hsb,hun,hye,ind,kaz,kmr,kor,kpv,mlt,myv,sme,tam,tel,tha,tur,uig,vie,wol,yor,yue"
2021.tacl-1.82,MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering,2021-01-01,https://aclanthology.org/2021.tacl-1.82/,tacl,,"Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1",1,1,1,1,1,1,b,d,0,1,i,f,"ara,cmn,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,nld,nob,pol,por,rus,spa,swe,tha,tur,vie,yue,zsm","ara,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,mal,nld,nor,pol,por,rus,spa,swe,tha,tur,vie,zho","ara,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,nld,nor,pol,por,rus,spa,swe,tha,tur,vie,yue,zho,zsm","arb,dan,deu,eng,fin,fra,heb,hun,ita,jpn,khm,kor,nld,nor,pol,por,rus,spa,swe,tha,tur,vie,yue,cmn,zsm"
2020.udw-1.4,Corpus evidence for word order freezing in Russian and German,2020-01-01,https://aclanthology.org/2020.udw-1.4/,udw,,"We use Universal Dependencies treebanks to test whether a well-known typological trade-off between word order freedom and richness of morphological marking of core arguments holds within individual languages. Using Russian and German treebank data, we show that the following phenomenon (sometimes dubbed word order freezing) does occur: those sentences where core arguments cannot be distinguished by morphological means (due to case syncretism or other kinds of ambiguity) have more rigid order of subject, verb and object than those where unambiguous morphological marking is present. In ambiguous clauses, word order is more often equal to the one which is default or dominant (most frequent) in the language. While Russian and German differ with respect to how exactly they mark core arguments, the effect of morphological ambiguity is significant in both languages. It is, however, small, suggesting that languages do adapt to the evolutionary pressure on communicative efficiency and avoidance of redundancy, but that the pressure is weak in this particular respect.",0,0,0,,,,,,,,,,,,,
2020.lrec-1.20,The ACQDIV Corpus Database and Aggregation Pipeline,2020-01-01,https://aclanthology.org/2020.lrec-1.20/,lrec,,"We present the ACQDIV corpus database and aggregation pipeline, a tool developed as part of the European Research Council (ERC) funded project ACQDIV, which aims to identify the universal cognitive processes that allow children to acquire any language. The corpus database represents 15 corpora from 14 typologically maximally diverse languages. Here we give an overview of the project, database, and our extensible software package for adding more corpora to the current language sample. Lastly, we discuss how we use the corpus database to mine for universal patterns in child language acquisition corpora and we describe avenues for future research.",1,1,1,1,1,1,d,d,1,1,i,i,"byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw","byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw","byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw","byx,cre,ctn,eng,ike,ind,jpn,mux,roh,rus,sot,tur,yua,yuw"
2020.lrec-1.324,Building a Time-Aligned Cross-Linguistic Reference Corpus from Language Documentation Data (DoReCo),2020-01-01,https://aclanthology.org/2020.lrec-1.324/,lrec,,"Natural speech data on many languages have been collected by language documentation projects aiming to preserve lingustic and cultural traditions in audivisual records. These data hold great potential for large-scale cross-linguistic research into phonetics and language processing. Major obstacles to utilizing such data for typological studies include the non-homogenous nature of file formats and annotation conventions found both across and within archived collections. Moreover, time-aligned audio transcriptions are typically only available at the level of broad (multi-word) phrases but not at the word and segment levels. We report on solutions developed for these issues within the DoReCo (DOcumentation REference COrpus) project. DoReCo aims at providing time-aligned transcriptions for at least 50 collections of under-resourced languages. This paper gives a preliminary overview of the current state of the project and details our workflow, in particular standardization of formats and conventions, the addition of segmental alignments with WebMAUS, and DoReCo’s applicability for subsequent research programs. By making the data accessible to the scientific community, DoReCo is designed to bridge the gap between language documentation and linguistic inquiry.",0,0,0,,,,,,,,,,,,,
2020.lrec-1.458,An Analysis of Massively Multilingual Neural Machine Translation for Low-Resource Languages,2020-01-01,https://aclanthology.org/2020.lrec-1.458/,lrec,,"In this work, we explore massively multilingual low-resource neural machine translation. Using translations of the Bible (which have parallel structure across languages), we train models with up to 1,107 source languages. We create various multilingual corpora, varying the number and relatedness of source languages. Using these, we investigate the best ways to use this many-way aligned resource for multilingual machine translation. Our experiments employ a grammatically and phylogenetically diverse set of source languages during testing for more representative evaluations. We find that best practices in this domain are highly language-specific: adding more languages to a training set is often better, but too many harms performance—the best number depends on the source language. Furthermore, training on related languages can improve or degrade performance, depending on the language. As there is no one-size-fits-most answer, we find that it is critical to tailor one’s approach to the source language and its typology.",0,0,0,,,,,,,,,,,,,
2020.lrec-1.472,An Enhanced Mapping Scheme of the Universal Part-Of-Speech for Korean,2020-01-01,https://aclanthology.org/2020.lrec-1.472/,lrec,,"When mapping a language specific Part-Of-Speech (POS) tag set to the Universal POS tag set (UPOS), it is critical to consider the individual language’s linguistic features and the UPOS definitions. In this paper, we present an enhanced Sejong POS mapping to the UPOS in accordance with the Korean linguistic typology and the substantive definitions of the UPOS categories. This work updated one third of the Sejong POS mapping to the UPOS. We also introduced a new mapping for the KAIST POS tag set, another widely used Korean POS tag set, to the UPOS.",0,0,0,,,,,,,,,,,,,
2020.lrec-1.493,Morphological Segmentation for Low Resource Languages,2020-01-01,https://aclanthology.org/2020.lrec-1.493/,lrec,,"This paper describes a new morphology resource created by Linguistic Data Consortium and the University of Pennsylvania for the DARPA LORELEI Program. The data consists of approximately 2000 tokens annotated for morphological segmentation in each of 9 low resource languages, along with root information for 7 of the languages. The languages annotated show a broad diversity of typological features. A minimal annotation scheme for segmentation was developed such that it could capture the patterns of a wide range of languages and also be performed reliably by non-linguist annotators. The basic annotation guidelines were designed to be language-independent, but included language-specific morphological paradigms and other specifications. The resulting annotated corpus is designed to support and stimulate the development of unsupervised morphological segmenters and analyzers by providing a gold standard for their evaluation on a more typologically diverse set of languages than has previously been available. By providing root annotation, this corpus is also a step toward supporting research in identifying richer morphological structures than simple morpheme boundaries.",1,1,1,1,1,1,b,d,1,1,i,i,"aka,fil,hin,hun,ind,rus,spa,swa,tam","aka,fil,hin,hun,ind,rus,spa,swa,tam","aka,fil,hin,hun,ind,rus,spa,swa,tam","aka,tgl,hin,hun,ind,rus,spa,swh,tam"
2020.lrec-1.503,SpiCE: A New Open-Access Corpus of Conversational Bilingual Speech in Cantonese and English,2020-01-01,https://aclanthology.org/2020.lrec-1.503/,lrec,,"This paper describes the design, collection, orthographic transcription, and phonetic annotation of SpiCE, a new corpus of conversational Cantonese-English bilingual speech recorded in Vancouver, Canada. The corpus includes high-quality recordings of 34 early bilinguals in both English and Cantonese—to date, 27 have been recorded for a total of 19 hours of participant speech. Participants completed a sentence reading task, storyboard narration, and conversational interview in each language. Transcription and annotation for the corpus are currently underway. Transcripts produced with Google Cloud Speech-to-Text are available for all participants, and will be included in the initial SpiCE corpus release. Hand-corrected orthographic transcripts and force-aligned phonetic transcripts will be released periodically, and upon completion for all recordings, comprise the second release of the corpus. As an open-access language resource, SpiCE will promote bilingualism research for a typologically distinct pair of languages, of which Cantonese remains understudied despite there being millions of speakers around the world. The SpiCE corpus is especially well-suited for phonetic research on conversational speech, and enables researchers to study cross-language within-speaker phenomena for a diverse group of early Cantonese-English bilinguals. These are areas with few existing high-quality resources.",0,0,1,,,,,d,,0,,n,,"eng,yue",,
2020.lrec-1.709,"Decomposing and Comparing Meaning Relations: Paraphrasing, Textual Entailment, Contradiction, and Specificity",2020-01-01,https://aclanthology.org/2020.lrec-1.709/,lrec,,"In this paper, we present a methodology for decomposing and comparing multiple meaning relations (paraphrasing, textual entailment, contradiction, and specificity). The methodology includes SHARel - a new typology that consists of 26 linguistic and 8 reason-based categories. We use the typology to annotate a corpus of 520 sentence pairs in English and we demonstrate that unlike previous typologies, SHARel can be applied to all relations of interest with a high inter-annotator agreement. We analyze and compare the frequency and distribution of the linguistic and reason-based phenomena involved in paraphrasing, textual entailment, contradiction, and specificity. This comparison allows for a much more in-depth analysis of the workings of the individual relations and the way they interact and compare with each other. We release all resources (typology, annotation guidelines, and annotated corpus) to the community.",0,0,0,,,,,,,,,,,,,
2020.lrec-1.879,"MorphAGram, Evaluation and Framework for Unsupervised Morphological Segmentation",2020-01-01,https://aclanthology.org/2020.lrec-1.879/,lrec,,"Computational morphological segmentation has been an active research topic for decades as it is beneficial for many natural language processing tasks. With the high cost of manually labeling data for morphology and the increasing interest in low-resource languages, unsupervised morphological segmentation has become essential for processing a typologically diverse set of languages, whether high-resource or low-resource. In this paper, we present and release MorphAGram, a publicly available framework for unsupervised morphological segmentation that uses Adaptor Grammars (AG) and is based on the work presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource).",1,1,1,1,1,1,d,b,1,1,i,l,"ara,azd,deu,eng,est,fin,hch,kat,mfy,nah,tur,zul","ara,azd,deu,eng,est,fin,hch,kat,mfy,tur,zul","ara,azd,deu,eng,est,fin,hch,kat,mfy,ngu,tur,zul","arb,azd,deu,eng,ekk,fin,hch,kat,mfy,ngu,tur,zul"
W18-5209,"Evidence Types, Credibility Factors, and Patterns or Soft Rules for Weighing Conflicting Evidence: Argument Mining in the Context of Legal Rules Governing Evidence Assessment",2018-01-01,https://aclanthology.org/W18-5209/,argmining,,"This paper reports on the results of an empirical study of adjudicatory decisions about veterans’ claims for disability benefits in the United States. It develops a typology of kinds of relevant evidence (argument premises) employed in cases, and it identifies factors that the tribunal considers when assessing the credibility or trustworthiness of individual items of evidence. It also reports on patterns or “soft rules” that the tribunal uses to comparatively weigh the probative value of conflicting evidence. These evidence types, credibility factors, and comparison patterns are developed to be inter-operable with legal rules governing the evidence assessment process in the U.S. This approach should be transferable to other legal and non-legal domains.",0,0,0,,,,,,,,,,,,,
2023.vardial-1.20,Dialect Representation Learning with Neural Dialect-to-Standard Normalization,2023-01-01,https://aclanthology.org/2023.vardial-1.20/,vardial,,"Language label tokens are often used in multilingual neural language modeling and sequence-to-sequence learning to enhance the performance of such models. An additional product of the technique is that the models learn representations of the language tokens, which in turn reflect the relationships between the languages. In this paper, we study the learned representations of dialects produced by neural dialect-to-standard normalization models. We use two large datasets of typologically different languages, namely Finnish and Norwegian, and evaluate the learned representations against traditional dialect divisions of both languages. We find that the inferred dialect embeddings correlate well with the traditional dialects. The methodology could be further used in noisier settings to find new insights into language variation.",0,0,0,,,,,,,,,,,,,
2023.emnlp-main.19,Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection,2023-01-01,https://aclanthology.org/2023.emnlp-main.19/,emnlp,,"Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation strategy StemCorrupt, a method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples. Our analysis uncovers that StemCorrupt brings about fundamental changes in the underlying data distribution, revealing inherent compositional concatenative structure. To complement our theoretical analysis, we investigate the data-efficiency of StemCorrupt. Through evaluation across a diverse set of seven typologically distinct languages, we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of compared to competitive baselines. Furthermore, we explore the impact of typological features on the choice of augmentation strategy and find that languages incorporating non-concatenativity, such as morphonological alternations, derive less benefit from synthetic examples with high predictive uncertainty. We attribute this effect to phonotactic violations induced by StemCorrupt, emphasizing the need for further research to ensure optimal performance across the entire spectrum of natural language morphology.",1,1,1,1,1,0,p,b,0,1,l,l,"ara,ben,fin,kat,nav,spa,tur","ara,ben,fin,kat,nav,spa,tur","ara,ben,fin,kat,nav,spa,tur","arb,ben,fin,kat,nav,spa,tur"
2023.emnlp-main.258,MEGA: Multilingual Evaluation of Generative AI,2023-01-01,https://aclanthology.org/2023.emnlp-main.258/,emnlp,,"Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",1,1,1,0,0,1,b,b,0,0,n,n,,,,
2023.emnlp-main.350,Controllable Contrastive Generation for Multilingual Biomedical Entity Linking,2023-01-01,https://aclanthology.org/2023.emnlp-main.350/,emnlp,,"Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive generation framework for MBEL, which summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template. Instead of tackling the MBEL problem with a discriminative classifier, we formulate it as a sequence-to-sequence generation task, which better exploits the shared dependencies between source mentions and target entities. Moreover, Con2GEN matches against UMLS concepts in as many languages and types as possible, hence facilitating cross-information disambiguation. Extensive experiments show that our model achieves promising performance improvements compared with several state-of-the-art techniques on the XL-BEL and the Mantra GSC datasets spanning 12 typologically diverse languages.",1,1,1,0,0,0,p,p,0,0,n,n,"deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,zho","deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,zho","deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,zho","deu,eng,fin,fra,jpn,kor,nld,rus,spa,tha,tur,cmn"
2023.emnlp-main.595,On Bilingual Lexicon Induction with Large Language Models,2023-01-01,https://aclanthology.org/2023.emnlp-main.595/,emnlp,,"Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,cat,deu,eng,fra,hun,ita,rus","bul,cat,deu,eng,fin,fra,hrv,hun,ita,rus,tur","bul,cat,deu,eng,fin,fra,hrv,hun,ita,rus,tur","bul,cat,deu,eng,fin,fra,hrv,hun,ita,rus,tur"
2023.emnlp-main.614,Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models,2023-01-01,https://aclanthology.org/2023.emnlp-main.614/,emnlp,,"Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of “tokens” processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API’s pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI’s language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable, to begin with. Through these analyses, we aim to increase transparency around language model APIs’ pricing policies and encourage the vendors to make them more equitable.",1,1,1,0,0,0,p,p,0,0,n,n,,"afr,asm,bel,ben,bos,bul,cat,ces,cym,dan,deu,ell,eng,epo,est,eus,fil,fin,fra,gla,gle,glg,guj,heb,hin,hrv,hun,hye,ind,isl,ita,jav,jpn,kan,kat,kaz,khm,kir,kor,lit,mal,mar,mkd,nld,pan,pol,por,ron,rus,san,sin,slk,slv,som,spa,srp,sun,swe,tam,tel,tha,tur,uig,ukr,urd,vie,zho","afr,asm,bel,ben,bos,bul,cat,ces,cym,dan,deu,ell,eng,epo,est,eus,fil,fin,fra,gla,gle,glg,guj,heb,hin,hrv,hun,hye,ind,isl,ita,jav,jpn,kan,kat,kaz,khm,kir,kor,lit,mal,mar,mkd,nld,pan,pol,por,ron,rus,san,sin,slk,slv,som,spa,srp,sun,swe,tam,tel,tha,tur,uig,ukr,urd,vie,zho","afr,asm,bel,ben,bos,bul,cat,ces,cym,dan,deu,ell,eng,epo,ekk,eus,tgl,fin,fra,gla,gle,glg,guj,heb,hin,hrv,hun,hye,ind,isl,ita,jav,jpn,kan,kat,kaz,khm,kir,kor,lit,mal,mar,mkd,nld,pan,pol,por,ron,rus,san,sin,slk,slv,som,spa,srp,sun,swe,tam,tel,tha,tur,uig,ukr,urd,vie,cmn"
D19-1102,A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages,2019-01-01,https://aclanthology.org/D19-1102/,"emnlp, ijcnlp",,"Parsers are available for only a handful of the world’s languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languages—North Sámi, Galician, and Kazah—We find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.",1,1,1,0,0,0,p,p,0,1,n,l,"glg,kaz,sme","glg,kaz,sme","glg,kaz,sme","glg,kaz,sme"
D19-1176,Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts,2019-01-01,https://aclanthology.org/D19-1176/,"emnlp, ijcnlp",,"Microaggressions are subtle, often veiled, manifestations of human biases. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of microaggressions in communication has made it difficult for researchers to analyze their exact nature, and to quantify and extract microaggressions automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of microaggressions based on a small subset of data that we have. We then create two datasets: one with examples of diverse types of microaggressions recollected by their targets, and another with gender-based microaggressions in public conversations on social media. We introduce a new, more objective, criterion for annotation and an active-learning based procedure that increases the likelihood of surfacing posts containing microaggressions. Finally, we analyze the trends that emerge from these new datasets.",0,0,0,,,,,,,,,,,,,
D19-1288,Towards Zero-shot Language Modeling,2019-01-01,https://aclanthology.org/D19-1288/,"emnlp, ijcnlp",,"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace’s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world’s languages, we hope that these insights will broaden the scope of applications for language technology.",1,1,1,0,0,0,p,p,0,0,i,n,"acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cjp,cni,dan,deu,dik,dje,djk,dop,eng,epo,est,eus,ewe,fil,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,que,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul","acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cip,cni,dan,deu,dik,dje,djk,dop,eng,epo,est,eus,ewe,fil,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,quw,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul","acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cjp,cni,dan,deu,dik,dje,djk,dop,eng,epo,est,eus,ewe,fil,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,quw,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul","acu,afr,agr,ake,amu,bsn,cak,ceb,ces,cha,chq,cjp,cpx,dan,deu,dik,dje,djk,dop,eng,epo,ekk,eus,ewe,tgl,fin,fra,gbi,gla,glv,hat,hrv,hun,ind,isl,ita,jak,jiv,kab,kbh,kek,lat,lav,lit,mam,mri,nhg,nld,nor,pck,plt,pol,por,pot,ppk,quc,quw,rom,ron,shi,slk,slv,sna,som,spa,sqi,srp,ssw,swe,tmh,tur,usp,vie,wal,wol,xho,zul"
D19-1574,Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers,2019-01-01,https://aclanthology.org/D19-1574/,"emnlp, ijcnlp",,"This paper explores the task of leveraging typology in the context of cross-lingual dependency parsing. While this linguistic information has shown great promise in pre-neural parsing, results for neural architectures have been mixed. The aim of our investigation is to better understand this state-of-the-art. Our main findings are as follows: 1) The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological dimensions in a compositional manner; 2) Typology consistent with the actual corpus statistics yields better transfer performance; 3) Typological similarity is only a rough proxy of cross-lingual transferability with respect to parsing.",0,0,0,,,,,,,,,,,,,
2010.jeptalnrecital-long.16,Anatomie des structures énumératives,2010-01-01,https://aclanthology.org/2010.jeptalnrecital-long.16/,jeptalnrecital,,"Cet article présente les premiers résultats d’une campagne d’annotation de corpus à grande échelle réalisée dans le cadre du projet ANNODIS. Ces résultats concernent la partie descendante du dispositif d’annotation, et plus spécifiquement les structures énumératives. Nous nous intéressons à la structuration énumérative en tant que stratégie de base de mise en texte, apparaissant à différents niveaux de granularité, associée à différentes fonctions discursives, et signalée par des indices divers. Avant l’annotation manuelle, une étape de pré-traitement a permis d’obtenir le marquage systématique de traits associés à la signalisation de l’organisation du discours. Nous décrivons cette étape de marquage automatique, ainsi que la procédure d’annotation. Nous proposons ensuite une première typologie des structures énumératives basée sur la description quantitative des données annotées manuellement, prenant en compte la couverture textuelle, la composition et les types d’indices.",0,0,0,,,,,,,,,,,,,
2021.cl-2.11,Universal Dependencies,2021-01-01,https://aclanthology.org/2021.cl-2.11/,cl,,"Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate–argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.",0,0,1,,,,,d,,,,,,,,
1963.earlymt-1.28,Types of language hierarchy,1963-01-01,https://aclanthology.org/1963.earlymt-1.28/,earlymt,,"Various relations lead to hierarchical systems of linguistic description. This paper considers briefly a typology of descriptive metalanguages based on such relations and sketches possible consequences for computational linguistics. Its scope is accordingly limited to metalanguages having operational interpretations which specify individual linguistic processes and structural interpretations which specify language data of individual languages. Immediate-constituent, context-free metalanguages are used to illustrate hierarchical types.",0,0,0,,,,,,,,,,,,,
2022.naacl-main.93,Sort by Structure: Language Model Ranking as Dependency Probing,2022-01-01,https://aclanthology.org/2022.naacl-main.93/,naacl,,"Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored. The field of Computer Vision has begun to tackle encoder ranking, with promising forays into Natural Language Processing, however they lack coverage of linguistic tasks such as structured prediction. We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM’s contextualized embeddings. Across 46 typologically and architecturally diverse LM-language pairs, our probing approach predicts the best LM choice 79% of the time using orders of magnitude less compute than training a full parser. Within this study, we identify and analyze one recently proposed decoupled LM—RemBERT—and find it strikingly contains less inherent dependency information, but often yields the best parser after full fine-tuning. Without this outlier our approach identifies the best LM in 89% of cases.",0,1,0,,,,p,,,,n,,"ara,eng,fin,grc,heb,kor,rus,swe,zho",,,
2022.naacl-main.94,Quantifying Synthesis and Fusion and their Impact on Machine Translation,2022-01-01,https://aclanthology.org/2022.naacl-main.94/,naacl,,"Theoretical work in morphological typology offers the possibility of measuring morphological diversity on a continuous scale. However, literature in Natural Language Processing (NLP) typically labels a whole language with a strict type of morphology, e.g. fusional or agglutinative. In this work, we propose to reduce the rigidity of such claims, by quantifying morphological typology at the word and segment level. We consider Payne (2017)’s approach to classify morphology using two indices: synthesis (e.g. analytic to polysynthetic) and fusion (agglutinative to fusional). For computing synthesis, we test unsupervised and supervised morphological segmentation methods for English, German and Turkish, whereas for fusion, we propose a semi-automatic method using Spanish as a case study. Then, we analyse the relationship between machine translation quality and the degree of synthesis and fusion at word (nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment level (previous language pairs plus English-German in both directions). We complement the word-level analysis with human evaluation, and overall, we observe a consistent impact of both indexes on machine translation quality.",0,0,0,,,,,,,,,,,,,
2022.naacl-main.176,Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations,2022-01-01,https://aclanthology.org/2022.naacl-main.176/,naacl,,"In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,cat,dan,deu,est,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,zho","bul,cat,dan,deu,est,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,zho","bul,cat,dan,deu,est,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,zho","bul,cat,dan,deu,ekk,eus,fra,glg,hrv,hun,ita,jpn,kor,nld,slv,spa,cmn"
2022.naacl-main.270,Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog,2022-01-01,https://aclanthology.org/2022.naacl-main.270/,naacl,,"Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MultiWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts, Multi2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot cross-lingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks.",1,1,1,1,1,1,b,b,0,0,i,n,"ara,deu,rus,zho","ara,deu,eng,rus,zho","ara,deu,rus,zho","arb,deu,rus,cmn"
2022.naacl-main.298,Unsupervised Stem-based Cross-lingual Part-of-Speech Tagging for Morphologically Rich Low-Resource Languages,2022-01-01,https://aclanthology.org/2022.naacl-main.298/,naacl,,"Unsupervised cross-lingual projection for part-of-speech (POS) tagging relies on the use of parallel data to project POS tags from a source language for which a POS tagger is available onto a target language across word-level alignments. The projected tags then form the basis for learning a POS model for the target language. However, languages with rich morphology often yield sparse word alignments because words corresponding to the same citation form do not align well. We hypothesize that for morphologically complex languages, it is more efficient to use the stem rather than the word as the core unit of abstraction. Our contributions are: 1) we propose an unsupervised stem-based cross-lingual approach for POS tagging for low-resource languages of rich morphology; 2) we further investigate morpheme-level alignment and projection; and 3) we examine whether the use of linguistic priors for morphological segmentation improves POS tagging. We conduct experiments using six source languages and eight morphologically complex target languages of diverse typologies. Our results show that the stem-based approach improves the POS models for all the target languages, with an average relative error reduction of 10.3% in accuracy per target language, and outperforms the word-based approach that operates on three-times more data for about two thirds of the language pairs we consider. Moreover, we show that morpheme-level alignment and projection and the use of linguistic priors for morphological segmentation further improve POS tagging.",1,1,1,0,0,0,,,1,1,,,,"amh,ara,deu,eng,eus,fin,fra,ind,kat,kaz,rus,spa,tel,tur","amh,ara,deu,eng,eus,fin,fra,ind,kat,kaz,rus,spa,tel,tur","amh,arb,deu,eng,eus,fin,fra,ind,kat,kaz,rus,spa,tel,tur"
2022.naacl-main.386,Syn2Vec: Synset Colexification Graphs for Lexical Semantic Similarity,2022-01-01,https://aclanthology.org/2022.naacl-main.386/,naacl,,"In this paper we focus on patterns of colexification (co-expressions of form-meaning mapping in the lexicon) as an aspect of lexical-semantic organization, and use them to build large scale synset graphs across BabelNet’s typologically diverse set of 499 world languages. We introduce and compare several approaches: monolingual and cross-lingual colexification graphs, popular distributional models, and fusion approaches. The models are evaluated against human judgments on a semantic similarity task for nine languages. Our strong empirical findings also point to the importance of universality of our graph synset embedding representations with no need for any language-specific adaptation when evaluated on the lexical similarity task. The insights of our exploratory investigation of large-scale colexification graphs could inspire significant advances in NLP across languages, especially for tasks involving languages which lack dedicated lexical resources, and can benefit from language transfer from large shared cross-lingual semantic spaces.",1,1,1,0,0,1,p,b,0,0,n,n,"ara,eng,fin,heb,pol,rus,spa,zho",,"ara,eng,fin,heb,pol,rus,spa,zho","arb,eng,fin,heb,pol,rus,spa,cmn"
N19-1097,A Systematic Study of Leveraging Subword Information for Learning Word Representations,2019-01-01,https://aclanthology.org/N19-1097/,naacl,,"The use of subword-level information (e.g., characters, character n-grams, morphemes) has become ubiquitous in modern word representation learning. Its importance is attested especially for morphologically rich languages which generate a large number of rare words. Despite a steadily increasing interest in such subword-informed word representations, their systematic comparative analysis across typologically diverse languages and different tasks is still missing. In this work, we deliver such a study focusing on the variation of two crucial components required for subword-level integration into word representation models: 1) segmentation of words into subword units, and 2) subword composition functions to obtain final word representations. We propose a general framework for learning subword-informed word representations that allows for easy experimentation with different segmentation and composition components, also including more advanced techniques based on position embeddings and self-attention. Using the unified framework, we run experiments over a large number of subword-informed word representation configurations (60 in total) on 3 tasks (general and rare word similarity, dependency parsing, fine-grained entity typing) for 5 languages representing 3 language types. Our main results clearly indicate that there is no “one-size-fits-all” configuration, as performance is both language- and task-dependent. We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation.",1,1,1,0,0,0,p,p,1,1,n,l,"deu,eng,fin,heb,tur","deu,eng,fin,heb,tur","deu,eng,fin,heb,tur","deu,eng,fin,heb,tur"
N19-1153,Improving Lemmatization of Non-Standard Languages with Joint Learning,2019-01-01,https://aclanthology.org/N19-1153/,naacl,,"Lemmatization of standard languages is concerned with (i) abstracting over morphological differences and (ii) resolving token-lemma ambiguities of inflected words in order to map them to a dictionary headword. In the present paper we aim to improve lemmatization performance on a set of non-standard historical languages in which the difficulty is increased by an additional aspect (iii): spelling variation due to lacking orthographic standards. We approach lemmatization as a string-transduction task with an Encoder-Decoder architecture which we enrich with sentence information using a hierarchical sentence encoder. We show significant improvements over the state-of-the-art by fine-tuning the sentence encodings to jointly optimize a bidirectional language model loss. Crucially, our architecture does not require POS or morphological annotations, which are not always available for historical corpora. Additionally, we also test the proposed model on a set of typologically diverse standard languages showing results on par or better than a model without fine-tuned sentence representations and previous state-of-the-art systems. Finally, to encourage future work on processing of non-standard varieties, we release the dataset of non-standard languages underlying the present study, which is based on openly accessible sources.",1,1,1,0,0,0,p,p,0,0,i,n,"ara,bul,ces,cga,cgl,cgr,crm,deu,eng,est,eus,fas,fin,fra,fro,gml,goo,heb,hun,ita,lat,lav,nob,rus,slv,spa,tur,urd","ara,bul,ces,deu,eng,est,eus,fas,fin,fra,fra,heb,hun,ita,lat,lav,nds,nld,nor,rus,slv,slv,spa,tur,urd","ara,bul,ces,cga,crm,deu,dum,eng,est,eus,fas,fin,fra,fro,gml,goo,heb,hun,ita,lat,lav,nob,rus,slv,spa,tur,urd","arb,bul,ces,cga,crm,deu,dum,eng,ekk,eus,pes,fin,fra,fro,gml,slv,heb,hun,ita,lat,lav,nor,rus,slv,spa,tur,urd"
N19-1203,Contextualization of Morphological Inflection,2019-01-01,https://aclanthology.org/N19-1203/,naacl,,"Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide “gold” tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.",1,1,1,0,0,0,p,p,0,0,n,n,"bul,eng,eus,fin,gla,hin,ita,lat,pol,swe","bul,eng,eus,fin,gle,hin,ita,lat,pol,swe","bul,eng,eus,fin,gla,hin,ita,lat,pol,swe","bul,eng,eus,fin,gla,hin,ita,lat,pol,swe"
2022.acl-long.265,Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go,2022-01-01,https://aclanthology.org/2022.acl-long.265/,acl,,"Aligning with ACL 2022 special Theme on “Language Diversity: from Low Resource to Endangered Languages”, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages. Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed. To facilitate future research, we also highlight current efforts, communities, venues, datasets, and tools. Our main objective is to motivate and advocate for an Afrocentric approach to technology development. With this in mind, we recommend what technologies to build and how to build, evaluate, and deploy them based on the needs of local African communities.",0,0,0,,,,,,,,,,,,,
2022.acl-long.582,Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing,2022-01-01,https://aclanthology.org/2022.acl-long.582/,acl,,"Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks. We apply model-agnostic meta-learning (MAML) to the task of cross-lingual dependency parsing. We train our model on a diverse set of languages to learn a parameter initialization that can adapt quickly to new languages. We find that meta-learning with pre-training can significantly improve upon the performance of language transfer and standard supervised learning baselines for a variety of unseen, typologically diverse, and low-resource languages, in a few-shot learning setup.",1,1,1,0,0,0,p,p,1,0,i,n,"bre,bua,deu,fao,fas,fin,fra,hsb,hun,hye,jpn,kaz,swe,tam,urd,vie","ara,bre,bua,bul,ces,deu,eng,fao,fas,fin,fra,hin,hsb,hun,hye,ita,jpn,kaz,kor,nor,rus,swe,tam,tel,urd,vie","ara,bre,bua,bul,ces,deu,eng,fao,fas,fin,fra,hin,hsb,hun,hye,ita,jpn,kaz,kor,nor,rus,swe,tam,tel,urd,vie","arb,bre,bxm,bul,ces,deu,eng,fao,pes,fin,fra,hin,hsb,hun,hye,ita,jpn,kaz,kor,nor,rus,swe,tam,tel,urd,vie"
2016.jeptalnrecital-jep.75,"Stress, charge cognitive et signal de parole : étude exploratoire auprès de pilotes de chasse. (Stress, cognitive load and speech signal : an exploratory study among fighter pilots)",2016-01-01,https://aclanthology.org/2016.jeptalnrecital-jep.75/,jeptalnrecital,,"Cet article traite des effets de la charge cognitive sur la fréquence fondamentale de pilotes de F-16 placés dans un scénario de vol de nuit. La charge cognitive a été estimée à l’aide de paramètres liés à la tâche (hétéro-évaluation), à l’individu (anxiété, auto-évaluation du stress ressenti) et à la situation (simulation contrôlée). Nos résultats montrent que l’écart mélodique est un bon candidat pour évaluer le niveau de la charge cognitive, même si la relation entre eux présente des profils individuels spécifiques. La création d’une typologie des situations de communication, l’adjonction d’autres indices acoustiques et le croisement avec des données physiologiques constituent les perspectives de cette étude.",0,0,0,,,,,,,,,,,,,
2022.sigmorphon-1.19,SIGMORPHON–UniMorph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection,2022-01-01,https://aclanthology.org/2022.sigmorphon-1.19/,sigmorphon,,"The 2022 SIGMORPHON–UniMorph shared task on large scale morphological inflection generation included a wide range of typologically diverse languages: 33 languages from 11 top-level language families: Arabic (Modern Standard), Assamese, Braj, Chukchi, Eastern Armenian, Evenki, Georgian, Gothic, Gujarati, Hebrew, Hungarian, Itelmen, Karelian, Kazakh, Ket, Khalkha Mongolian, Kholosi, Korean, Lamahalot, Low German, Ludic, Magahi, Middle Low German, Old English, Old High German, Old Norse, Polish, Pomak, Slovak, Turkish, Upper Sorbian, Veps, and Xibe. We emphasize generalization along different dimensions this year by evaluating test items with unseen lemmas and unseen features separately under small and large training conditions. Across the five submitted systems and two baselines, the prediction of inflections with unseen features proved challenging, with average performance decreased substantially from last year. This was true even for languages for which the forms were in principle predictable, which suggests that further work is needed in designing systems that capture the various types of generalization required for the world’s languages.",1,1,1,1,1,1,b,d,1,0,i,n,"ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,arb,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep"
2022.sigmorphon-1.20,SIGMORPHON 2022 Task 0 Submission Description: Modelling Morphological Inflection with Data-Driven and Rule-Based Approaches,2022-01-01,https://aclanthology.org/2022.sigmorphon-1.20/,sigmorphon,,"This paper describes our participation in the 2022 SIGMORPHON-UniMorph Shared Task on Typologically Diverse and AcquisitionInspired Morphological Inflection Generation. We present two approaches: one being a modification of the neural baseline encoderdecoder model, the other being hand-coded morphological analyzers using finite-state tools (FST) and outside linguistic knowledge. While our proposed modification of the baseline encoder-decoder model underperforms the baseline for almost all languages, the FST methods outperform other systems in the respective languages by a large margin. This confirms that purely data-driven approaches have not yet reached the maturity to replace trained linguists for documentation and analysis especially considering low-resource and endangered languages.",0,0,0,,,,,,,,,,,"ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep",,
2022.sigmorphon-1.23,Generalizing Morphological Inflection Systems to Unseen Lemmas,2022-01-01,https://aclanthology.org/2022.sigmorphon-1.23/,sigmorphon,,"This paper presents experiments on morphological inflection using data from the SIGMORPHON-UniMorph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection. We present a transformer inflection system, which enriches the standard transformer architecture with reverse positional encoding and type embeddings. We further apply data hallucination and lemma copying to augment training data. We train models using a two-stage procedure: (1) We first train on the augmented training data using standard backpropagation and teacher forcing. (2) We then continue training with a variant of the scheduled sampling algorithm dubbed student forcing. Our system delivers competitive performance under the small and large data conditions on the shared task datasets.",0,0,0,,,,,,,,,,,"ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep",,
2022.sigmorphon-1.25,Morphology is not just a naive Bayes – UniMelb Submission to SIGMORPHON 2022 ST on Morphological Inflection,2022-01-01,https://aclanthology.org/2022.sigmorphon-1.25/,sigmorphon,,"The paper describes the Flexica team’s submission to the SIGMORPHON 2022 Shared Task 1 Part 1: Typologically Diverse Morphological Inflection. Our team submitted a nonneural system that extracted transformation patterns from alignments between a lemma and inflected forms. For each inflection category, we chose a pattern based on its abstractness score. The system outperformed the non-neural baseline, the extracted patterns covered a substantial part of possible inflections. However, we discovered that such score that does not account for all possible combinations of string segments as well as morphosyntactic features is not sufficient for a certain proportion of inflection cases.",1,1,0,0,0,0,p,p,0,0,n,n,"ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,ara,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep","ang,arb,asm,bra,ckt,evn,gml,goh,got,guj,heb,hsb,hsb,hsi,hun,hye,itl,kat,kaz,ket,khk,kor,krl,lud,mag,nds,non,pol,poma,slk,slp,tur,vep"
D18-1029,On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling,2018-01-01,https://aclanthology.org/D18-1029/,emnlp,,"A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world’s languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.",1,1,1,0,0,0,p,p,1,1,i,l,,"amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,msa,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho","amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,msa,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho","amh,arb,bul,cat,ces,dan,deu,ell,eng,ekk,eus,pes,tgl,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,zsm,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,cmn"
Q18-1032,Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction,2018-01-01,https://aclanthology.org/Q18-1032/,tacl,,"Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available.",1,1,1,0,0,0,p,p,1,1,i,l,"amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mng,mya,nan,nld,nob,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho,zsm","amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mon,msa,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho","amh,ara,bul,cat,ces,dan,deu,ell,eng,est,eus,fas,fil,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mng,mya,nan,nld,nob,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,zho,zsm","amh,arb,bul,cat,ces,dan,deu,ell,eng,ekk,eus,pes,tgl,fin,fra,heb,hin,hrv,hun,ind,ita,jav,jpn,kan,kat,khm,kor,lav,lit,mng,mya,nan,nld,nor,pol,por,ron,rus,slk,slv,spa,srp,swe,tam,tha,tur,ukr,vie,cmn,zsm"
2021.icon-main.41,Multi-Source Cross-Lingual Constituency Parsing,2021-01-01,https://aclanthology.org/2021.icon-main.41/,icon,,"Pretrained multilingual language models have become a key part of cross-lingual transfer for many natural language processing tasks, even those without bilingual information. This work further investigates the cross-lingual transfer ability of these models for constituency parsing and focuses on multi-source transfer. Addressing structure and label set diversity problems, we propose the integration of typological features into the parsing model and treebank normalization. We trained the model on eight languages with diverse structures and use transfer parsing for an additional six low-resource languages. The experimental results show that the treebank normalization is essential for cross-lingual transfer performance and the typological features introduce further improvement. As a result, our approach improves the baseline F1 of multi-source transfer by 5 on average.",0,0,0,,,,,,,,,,,,,
2021.wassa-1.7,Universal Joy A Data Set and Results for Classifying Emotions Across Languages,2021-01-01,https://aclanthology.org/2021.wassa-1.7/,wassa,,"While emotions are universal aspects of human psychology, they are expressed differently across different languages and cultures. We introduce a new data set of over 530k anonymized public Facebook posts across 18 languages, labeled with five different emotions. Using multilingual BERT embeddings, we show that emotions can be reliably inferred both within and across languages. Zero-shot learning produces promising results for low-resource languages. Following established theories of basic emotions, we provide a detailed analysis of the possibilities and limits of cross-lingual emotion classification. We find that structural and typological similarity between languages facilitates cross-lingual learning, as well as linguistic diversity of training data. Our results suggest that there are commonalities underlying the expression of emotion in different languages. We publicly release the anonymized data for future research.",0,0,0,,,,,,,,,,,,,
2021.latechclfl-1.12,Translationese in Russian Literary Texts,2021-01-01,https://aclanthology.org/2021.latechclfl-1.12/,latechclfl,,"The paper reports the results of a translationese study of literary texts based on translated and non-translated Russian. We aim to find out if translations deviate from non-translated literary texts, and if the established differences can be attributed to typological relations between source and target languages. We expect that literary translations from typologically distant languages should exhibit more translationese, and the fingerprints of individual source languages (and their families) are traceable in translations. We explore linguistic properties that distinguish non-translated Russian literature from translations into Russian. Our results show that non-translated fiction is different from translations to the degree that these two language varieties can be automatically classified. As expected, language typology is reflected in translations of literary texts. We identified features that point to linguistic specificity of Russian non-translated literature and to shining-through effects. Some of translationese features cut across all language pairs, while others are characteristic of literary translations from languages belonging to specific language families.",0,0,0,,,,,,,,,,,,,
2020.acl-main.112,Learning and Evaluating Emotion Lexicons for 91 Languages,2020-01-01,https://aclanthology.org/2020.acl-main.112/,acl,,"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.",1,1,1,1,1,0,p,b,0,0,n,n,"deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,zho","deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,zho","deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,zho","deu,ell,eng,hrv,ind,ita,nld,pol,por,spa,tur,cmn"
2020.acl-main.420,Information-Theoretic Probing for Linguistic Structure,2020-01-01,https://aclanthology.org/2020.acl-main.420/,acl,,"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research—plus English—totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.",1,1,1,0,0,0,p,p,0,0,n,n,"ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd","ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd","ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd","ces,eng,eus,fin,ind,kor,mar,tam,tel,tur,urd"
2020.acl-main.596,Modeling Morphological Typology for Unsupervised Learning of Language Morphology,2020-01-01,https://aclanthology.org/2020.acl-main.596/,acl,,"This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology. By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology. The system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems. We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type.",1,1,1,0,0,0,p,p,1,1,f,l,"aka,hin,hun,ind,rus,spa,swa,tag,tam","aka,hin,hun,ind,rus,spa,swa,tag,tam","aka,hin,hun,ind,rus,spa,swa,tag,tam","aka,hin,hun,ind,rus,spa,swh,tag,tam"
2020.acl-main.598,Unsupervised Morphological Paradigm Completion,2020-01-01,https://aclanthology.org/2020.acl-main.598/,acl,,"We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.",1,1,1,0,0,0,p,p,0,1,f,f,"bul,deu,eng,eus,fas,fin,kan,mlt,nav,por,rus,spa,swe,tur","bul,deu,eng,eus,fas,fin,kan,mlt,nav,por,rus,spa,swe,tur","bul,deu,eng,eus,fas,fin,kan,mlt,nav,por,rus,spa,swe,tur","bul,deu,eng,eus,pes,fin,kan,mlt,nav,por,rus,spa,swe,tur"
2020.acl-main.733,Frugal Paradigm Completion,2020-01-01,https://aclanthology.org/2020.acl-main.733/,acl,,"Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive. We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible. It induces typological information during training which it uses to determine the best sources at test time. We evaluate our language-agnostic approach on 7 diverse languages. Compared to popular alternative approaches, ours reduces manual labor by 16-63% and is the most robust to typological variation.",0,0,0,,,,,,,,,,,,,
C16-1298,Universal Reordering via Linguistic Typology,2016-01-01,https://aclanthology.org/C16-1298/,coling,,"In this paper we explore the novel idea of building a single universal reordering model from English to a large number of target languages. To build this model we exploit typological features of word order for a large number of target languages together with source (English) syntactic features and we train this model on a single combined parallel corpus representing all (22) involved language pairs. We contribute experimental evidence for the usefulness of linguistically defined typological features for building such a model. When the universal reordering model is used for preordering followed by monotone translation (no reordering inside the decoder), our experiments show that this pipeline gives comparable or improved translation performance with a phrase-based baseline for a large number of language pairs (12 out of 22) from diverse language families.",0,0,0,,,,,,,,,,,,,
2022.fieldmatters-1.8,"How to encode arbitrarily complex morphology in word embeddings, no corpus needed",2022-01-01,https://aclanthology.org/2022.fieldmatters-1.8/,fieldmatters,,"In this paper, we present a straightforward technique for constructing interpretable word embeddings from morphologically analyzed examples (such as interlinear glosses) for all of the world’s languages. Currently, fewer than 300-400 languages out of approximately 7000 have have more than a trivial amount of digitized texts; of those, between 100-200 languages (most in the Indo-European language family) have enough text data for BERT embeddings of reasonable quality to be trained. The word embeddings in this paper are explicitly designed to be both linguistically interpretable and fully capable of handling the broad variety found in the world’s diverse set of 7000 languages, regardless of corpus size or morphological characteristics. We demonstrate the applicability of our representation through examples drawn from a typologically diverse set of languages whose morphology includes prefixes, suffixes, infixes, circumfixes, templatic morphemes, derivational morphemes, inflectional morphemes, and reduplication.",1,1,1,0,0,0,p,p,1,1,l,l,"cat,ckt,eng,gug,mlt,ypk,zsm","cat,ckt,eng,grn,mlt,ypk,zsm","cat,ckt,eng,gug,mlt,ypk,zsm","cat,ckt,eng,gug,mlt,ypk,zsm"
2021.sigmorphon-1.3,The Match-Extend serialization algorithm in Multiprecedence,2021-01-01,https://aclanthology.org/2021.sigmorphon-1.3/,sigmorphon,,"Raimy (1999; 2000a; 2000b) proposed a graphical formalism for modeling reduplication, originallymostly focused on phonological overapplication in a derivational framework. This framework is now known as Precedence-based phonology or Multiprecedence phonology. Raimy’s idea is that the segments at the input to the phonology are not totally ordered by precedence. This paper tackles a challenge that arose with Raimy’s work, the development of a deterministic serialization algorithm as part of the derivation of surface forms. The Match-Extend algorithm introduced here requires fewer assumptions and sticks tighter to the attested typology. The algorithm also contains no parameter or constraint specific to individual graphs or topologies, unlike previous proposals. Match-Extend requires nothing except knowing the last added set of links.",0,0,0,,,,,,,,,,,,,
2021.sigmorphon-1.25,SIGMORPHON 2021 Shared Task on Morphological Reinflection: Generalization Across Languages,2021-01-01,https://aclanthology.org/2021.sigmorphon-1.25/,sigmorphon,,"This year’s iteration of the SIGMORPHON Shared Task on morphological reinflection focuses on typological diversity and cross-lingual variation of morphosyntactic features. In terms of the task, we enrich UniMorph with new data for 32 languages from 13 language families, with most of them being under-resourced: Kunwinjku, Classical Syriac, Arabic (Modern Standard, Egyptian, Gulf), Hebrew, Amharic, Aymara, Magahi, Braj, Kurdish (Central, Northern, Southern), Polish, Karelian, Livvi, Ludic, Veps, Võro, Evenki, Xibe, Tuvan, Sakha, Turkish, Indonesian, Kodi, Seneca, Asháninka, Yanesha, Chukchi, Itelmen, Eibela. We evaluate six systems on the new data and conduct an extensive error analysis of the systems’ predictions. Transformer-based models generally demonstrate superior performance on the majority of languages, achieving >90% accuracy on 65% of them. The languages on which systems yielded low accuracy are mainly under-resourced, with a limited amount of data. Most errors made by the systems are due to allomorphy, honorificity, and form variation. In addition, we observe that systems especially struggle to inflect multiword lemmas. The systems also produce misspelled forms or end up in repetitive loops (e.g., RNN-based models). Finally, we report a large drop in systems’ performance on previously unseen lemmas.",1,1,1,0,0,0,p,p,1,1,l,l,,"ail,ame,amh,ara,arz,atb,aym,bra,bul,ces,ckb,ckt,cni,deu,evn,gup,heb,ind,itl,kmr,kod,krl,lud,mag,nld,olo,pol,por,rus,sah,sdh,see,sjo,spa,syc,tur,tyv,vep,vro","ail,ame,amh,ara,arz,atb,aym,bra,bul,ces,ckb,ckt,cni,deu,evn,gup,heb,ind,itl,kmr,kod,krl,lud,mag,nld,olo,pol,por,rus,sah,sdh,see,sjo,spa,syc,tur,tyv,vep,vro","ail,ame,amh,arb,arz,atb,ayr,bra,bul,ces,ckb,ckt,cpx,deu,evn,gup,heb,ind,itl,kmr,kod,krl,lud,mag,nld,olo,pol,por,rus,sah,sdh,see,sjo,spa,syc,tur,tyv,vep,vro"
2021.sigmorphon-1.27,BME Submission for SIGMORPHON 2021 Shared Task 0. A Three Step Training Approach with Data Augmentation for Morphological Inflection,2021-01-01,https://aclanthology.org/2021.sigmorphon-1.27/,sigmorphon,,"We present the BME submission for the SIGMORPHON 2021 Task 0 Part 1, Generalization Across Typologically Diverse Languages shared task. We use an LSTM encoder-decoder model with three step training that is first trained on all languages, then fine-tuned on each language family and finally fine-tuned on individual languages. We use a different type of data augmentation technique in the first two steps. Our system outperformed the only other submission. Although it remains worse than the Transformer baseline released by the organizers, our model is simpler and our data augmentation techniques are easily applicable to new languages. We perform ablation studies and show that the augmentation techniques and the three training steps often help but sometimes have a negative effect. Our code is publicly available.",0,0,0,,,,,,,,,,,,,
2020.coling-main.423,Manual Clustering and Spatial Arrangement of Verbs for Multilingual Evaluation and Typology Analysis,2020-01-01,https://aclanthology.org/2020.coling-main.423/,coling,,"We present the first evaluation of the applicability of a spatial arrangement method (SpAM) to a typologically diverse language sample, and its potential to produce semantic evaluation resources to support multilingual NLP, with a focus on verb semantics. We demonstrate SpAM’s utility in allowing for quick bottom-up creation of large-scale evaluation datasets that balance cross-lingual alignment with language specificity. Starting from a shared sample of 825 English verbs, translated into Chinese, Japanese, Finnish, Polish, and Italian, we apply a two-phase annotation process which produces (i) semantic verb classes and (ii) fine-grained similarity scores for nearly 130 thousand verb pairs. We use the two types of verb data to (a) examine cross-lingual similarities and variation, and (b) evaluate the capacity of static and contextualised representation models to accurately reflect verb semantics, contrasting the performance of large language specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics.",1,1,1,1,1,0,b,p,0,1,f,f,"cmn,fin,ita,jpn,pol","eng,fin,ita,jpn,pol,zho","cmn,fin,ita,jpn,pol","cmn,fin,ita,jpn,pol"
2020.coling-main.559,XHate-999: Analyzing and Detecting Abusive Language Across Domains and Languages,2020-01-01,https://aclanthology.org/2020.coling-main.559/,coling,,"We present XHate-999, a multi-domain and multilingual evaluation data set for abusive language detection. By aligning test instances across six typologically diverse languages, XHate-999 for the first time allows for disentanglement of the domain transfer and language transfer effects in abusive language detection. We conduct a series of domain- and language-transfer experiments with state-of-the-art monolingual and multilingual transformer models, setting strong baseline results and profiling XHate-999 as a comprehensive evaluation resource for abusive language detection. Finally, we show that domain- and language-adaption, via intermediate masked language modeling on abusive corpora in the target language, can lead to substantially improved abusive language detection in the target language in the zero-shot transfer setups.",1,1,1,1,1,1,b,b,,0,i,n,"deu,hrv,rus,sqi,tur","deu,eng,hrv,rus,sqi,tur","deu,eng,hrv,rus,sqi,tur","deu,eng,hrv,rus,sqi,tur"
2021.nodalida-main.39,It’s Basically the Same Language Anyway: the Case for a Nordic Language Model,2021-01-01,https://aclanthology.org/2021.nodalida-main.39/,nodalida,,"When is it beneficial for a research community to organize a broader collaborative effort on a topic, and when should we instead promote individual efforts? In this opinion piece, we argue that we are at a stage in the development of large-scale language models where a collaborative effort is desirable, despite the fact that the preconditions for making individual contributions have never been better. We consider a number of arguments for collaboratively developing a large-scale Nordic language model, include environmental considerations, cost, data availability, language typology, cultural similarity, and transparency. Our primary goal is to raise awareness and foster a discussion about our potential impact and responsibility as NLP community.",0,0,0,,,,,,,,,,,,,
2023.bsnlp-1.15,Can BERT eat RuCoLA? Topological Data Analysis to Explain,2023-01-01,https://aclanthology.org/2023.bsnlp-1.15/,bsnlp,,"This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach is based on best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them and feed them to linear classifiers. We introduce two novel features, chordality and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, CoLA and RuCoLA, in English and Russian, which are typologically different languages. On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LM’s during fine-tuning, defining the LM’s prediction confidences, and associating individual heads with fine-grained grammar phenomena. Our results contribute to understanding the behaviour of monolingual LMs in the acceptability classification task, provide insights into the functional roles of attention heads, and highlight the advantages of TDA-based approaches for analyzing LMs.We release the code and the experimental results for further uptake.",0,0,0,,,,,,,,,,,,,
S17-1003,Decoding Sentiment from Distributed Representations of Sentences,2017-01-01,https://aclanthology.org/S17-1003/,starsem,,"Distributed representations of sentences have been developed recently to represent their meaning as real-valued vectors. However, it is not clear how much information such representations retain about the polarity of sentences. To study this question, we decode sentiment from unsupervised sentence representations learned with different architectures (sensitive to the order of words, the order of sentences, or none) in 9 typologically diverse languages. Sentiment results from the (recursive) composition of lexical items and grammatical strategies such as negation and concession. The results are manifold: we show that there is no ‘one-size-fits-all’ representation architecture outperforming the others across the board. Rather, the top-ranking architectures depend on the language at hand. Moreover, we find that in several cases the additive composition model based on skip-gram word vectors may surpass supervised state-of-art architectures such as bi-directional LSTMs. Finally, we provide a possible explanation of the observed variation based on the type of negative constructions in each language.",1,1,1,0,0,0,p,p,0,0,n,n,"ara,eng,fra,ita,nld,rus,spa,tur,zho","ara,eng,fra,ita,nld,rus,spa,zho","ara,eng,fra,ita,nld,rus,spa,tur,zho","arb,eng,fra,ita,nld,rus,spa,tur,cmn"
2021.emnlp-main.28,How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?,2021-01-01,https://aclanthology.org/2021.emnlp-main.28/,emnlp,,"As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn to distinguish between core features and data artifacts. While models trained on this type of data have shown promising out-of-domain generalizability, it is still unclear what the sources of such improvements are. We investigate the benefits of CAD for social NLP models by focusing on three social computing constructs — sentiment, sexism, and hate speech. Assessing the performance of models trained with and without CAD across different types of datasets, we find that while models trained on CAD show lower in-domain performance, they generalize better out-of-domain. We unpack this apparent discrepancy using machine explanations and find that CAD reduces model reliance on spurious features. Leveraging a novel typology of CAD to analyze their relationship with model performance, we find that CAD which acts on the construct directly or a diverse set of CAD leads to higher performance.",0,0,0,,,,,,,,,,,,,
2021.emnlp-main.793,An Information-Theoretic Characterization of Morphological Fusion,2021-01-01,https://aclanthology.org/2021.emnlp-main.793/,emnlp,,"Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that languages have characteristic levels of fusion; rather, the degree of fusion varies across part-of-speech within languages.",0,0,0,,,,,,,,,,,,,
2021.emnlp-main.802,XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation,2021-01-01,https://aclanthology.org/2021.emnlp-main.802/,emnlp,,"Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.",1,1,1,1,1,1,b,d,0,1,i,f,"afr,ara,aze,ben,bul,deu,ell,eng,est,eus,fas,fil,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,msa,mya,nld,pan,pol,por,que,rus,spa,swa,tam,tel,tha,tur,ukr,urd,vie,wol,yor,zho","afr,ara,aze,ben,bul,deu,ell,eng,est,eus,fas,fil,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,msa,mya,nld,pan,pol,por,que,ron,rus,spa,swa,tam,tel,tha,tur,ukr,urd,vie,wol,yor,zho","afr,ara,aze,ben,bul,deu,ell,eng,est,eus,fas,fil,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,msa,mya,nld,pan,pol,por,que,ron,rus,spa,swa,tam,tel,tha,tur,ukr,urd,vie,wol,yor,zho","afr,arb,azj,ben,bul,deu,ell,eng,ekk,eus,pes,tgl,fin,fra,guj,hat,heb,hin,hun,ind,ita,jav,jpn,kat,kaz,kor,lit,mal,mar,zsm,mya,nld,pan,pol,por,que,ron,rus,spa,swh,tam,tel,tha,tur,ukr,urd,vie,wol,yor,cmn"
2021.emnlp-main.818,Visually Grounded Reasoning across Languages and Cultures,2021-01-01,https://aclanthology.org/2021.emnlp-main.818/,emnlp,,"The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.",1,1,1,1,1,1,b,d,0,0,i,n,"cmn,ind,swa,tam,tur","ind,swa,tam,tur,zho","ind,swa,tam,tur,zho","ind,swh,tam,tur,cmn"
2021.mrl-1.12,Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval,2021-01-01,https://aclanthology.org/2021.mrl-1.12/,mrl,,"We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual retrieval in eleven typologically diverse languages, designed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for representation learning perform poorly when applied to out-of-distribution data. As a starting point, we provide zero-shot baselines for this new dataset based on a multi-lingual adaptation of DPR that we call “mDPR”. Experiments show that although the effectiveness of mDPR is much lower than BM25, dense representations nevertheless appear to provide valuable relevance signals, improving BM25 results in sparse–dense hybrids. In addition to analyses of our results, we also discuss future challenges and present a research agenda in multi-lingual dense retrieval. Mr. TyDi can be downloaded at https://github.com/castorini/mr.tydi.",1,1,1,1,1,1,b,d,0,0,n,n,"ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","ara,ben,eng,fin,ind,jpn,kor,rus,swa,tel,tha","arb,ben,eng,fin,ind,jpn,kor,rus,swh,tel,tha"
2021.mrl-1.18,Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling,2021-01-01,https://aclanthology.org/2021.mrl-1.18/,mrl,,"Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding (NLU). Since annotated datasets are only available for a handful of languages, our work focuses particularly on a zero-shot scenario where the target language is unseen during training. In the context of zero-shot learning, this task is typically approached using representations from pre-trained multilingual language models such as mBERT or by fine-tuning on data automatically translated into the target language. We propose a novel method which augments monolingual source data using multilingual code-switching via random translations, to enhance generalizability of large multilingual language models when fine-tuning them for downstream tasks. Experiments on the MultiATIS++ benchmark show that our method leads to an average improvement of +4.2% in accuracy for the intent task and +1.8% in F1 for the slot-filling task over the state-of-the-art across 8 typologically diverse languages. We also study the impact of code-switching into different families of languages on downstream performance. Furthermore, we present an application of our method for crisis informatics using a new human-annotated tweet dataset of slot filling in English and Haitian Creole, collected during the Haiti earthquake.",1,1,1,0,0,0,p,p,0,1,n,f,"deu,fra,hin,jpn,por,spa,tur,zho","deu,eng,fra,hat,hin,jpn,por,spa,tur,zho","deu,fra,hin,jpn,por,spa,tur,zho","deu,fra,hin,jpn,por,spa,tur,cmn"
2021.mrl-1.23,Well-Defined Morphology is Sentence-Level Morphology,2021-01-01,https://aclanthology.org/2021.mrl-1.23/,mrl,,"Morphological tasks have gained decent popularity within the NLP community in the recent years, with large multi-lingual datasets providing morphological analysis of words, either in or out of context. However, the lack of a clear linguistic definition for words destines the annotative work to be incomplete and mired in inconsistencies, especially cross-linguistically. In this work we expand morphological inflection of words to inflection of sentences to provide true universality disconnected from orthographic traditions of white-space usage. To allow annotation for sentence-inflection we define a morphological annotation scheme by a fixed set of inflectional features. We present a small cross-linguistic dataset including semi-manually generated simple sentences in 4 typologically diverse languages annotated according to our suggested scheme, and show that the task of reinflection gets substantially more difficult but that the change of scope from words to well-defined sentences allows interface with contextualized language models.",0,1,0,,,,p,,,,n,,"deu,eng,heb,tur",,,
2023.tacl-1.59,A Cross-Linguistic Pressure for Uniform Information Density in Word Order,2023-01-01,https://aclanthology.org/2023.tacl-1.59/,tacl,,"While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1",1,1,1,0,0,0,p,p,1,1,i,l,"deu,eng,fas,fra,hin,hun,ind,rus,tur,vie","deu,eng,fas,fra,hin,hun,ind,rus,tur,vie","deu,eng,fas,fra,hin,hun,ind,rus,tur,vie","deu,eng,pes,fra,hin,hun,ind,rus,tur,vie"
2023.tacl-1.63,MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,2023-01-01,https://aclanthology.org/2023.tacl-1.63/,tacl,,"MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http://miracl.ai/.",1,1,1,1,1,1,b,d,0,1,i,f,"ara,ben,deu,eng,fas,fin,fra,hin,ind,jpn,kor,rus,spa,swa,tel,tha,yor,zho","ara,ben,deu,eng,fas,fin,fra,ind,jpn,kor,rus,spa,swa,tel,tha,yor,zho","ara,ben,deu,eng,fas,fin,fra,hin,ind,jpn,kor,rus,spa,swa,tel,tha,yor,zho","arb,ben,deu,eng,pes,fin,fra,hin,ind,jpn,kor,rus,spa,swh,tel,tha,yor,cmn"
2010.amta-papers.6,Coupling Statistical Machine Translation with Rule-based Transfer and Generation,2010-01-01,https://aclanthology.org/2010.amta-papers.6/,amta,,"In this paper, we present the insights gained from a detailed study of coupling a highly modular English-Hindi RBMT system with a standard phrase-based SMT system. Coupling the RBMT and SMT systems at various stages in the RBMT pipeline, we observe the effects of the source transformations at each stage on the performance of the coupled MT system. We propose an architecture that systematically exploits the structural transfer and robust generation capabilities of the RBMT system. Working with the English-Hindi language pair, we show that the coupling configurations explored in our experiments help address different aspects of the typological divergence between these languages. In spite of working with very small datasets, we report significant improvements both in terms of BLEU (7.14 and 0.87 over the RBMT and the SMT baselines respectively) and subjective evaluation (relative decrease of 17% in SSER).",0,0,0,,,,,,,,,,,,,
2023.sigtyp-1.2,Multilingual End-to-end Dependency Parsing with Linguistic Typology knowledge,2023-01-01,https://aclanthology.org/2023.sigtyp-1.2/,sigtyp,,"We evaluate a Multilingual End-to-end BERT based Dependency Parser which parses an input sentence by directly predicting the relative head-position for each word within it. Our model is a Cross-lingual dependency parser which is trained on a diverse polyglot corpus of high-resource source languages, and is applied on a low-resource target language. To make model more robust to typological variations between source and target languages, and to facilitate the cross-lingual transferring, we utilized the Linguistic typology knowledge, available in typological databases WALS and URIEL. We induce such typology knowledge within our model through an auxiliary task within Multi-task Learning framework.",1,0,1,0,0,0,,p,0,1,,f,,"deu,est,hin,hrv,ita,vie,zho","deu,est,hin,hrv,ita,vie,zho","deu,ekk,hin,hrv,ita,vie,cmn"
2023.sigtyp-1.17,Grambank’s Typological Advances Support Computational Research on Diverse Languages,2023-01-01,https://aclanthology.org/2023.sigtyp-1.17/,sigtyp,,"Of approximately 7,000 languages around the world, only a handful have abundant computational resources. Extending the reach of language technologies to diverse, less-resourced languages is important for tackling the challenges of digital equity and inclusion. Here we introduce the Grambank typological database as a resource to support such efforts. To date, work that uses typological data to extend computational research to less-resourced languages has relied on cross-linguistic morphosyntax datasets that are sparsely populated, use categorical coding that can be difficult to interpret, and introduce redundant information across features. Grambank presents similar information (e.g. word order, grammatical relation marking, constructions like interrogatives and negation), but is designed to avoid several disadvantages of legacy typological resources. Grambank’s 195 features encode basic information about morphology and syntax for 2,467 languages. 83% of these languages are annotated for at least 100 features. By implementing binary coding for most features and curating the dataset to avoid logical dependencies, Grambank presents information in a user-friendly format for computational applications. The scale, completeness, reliability, format, and documentation of Grambank make it a useful resource for linguistically-informed models, cross-lingual NLP, and research targeting less-resourced languages.",0,0,1,,,,,d,,1,,i,,,,
2023.sigtyp-1.19,Gradual Language Model Adaptation Using Fine-Grained Typology,2023-01-01,https://aclanthology.org/2023.sigtyp-1.19/,sigtyp,,"Transformer-based language models (LMs) offer superior performance in a wide range of NLP tasks compared to previous paradigms. However, the vast majority of the world’s languages do not have adequate training data available for monolingual LMs (Joshi et al., 2020). While the use of multilingual LMs might address this data imbalance, there is evidence that multilingual LMs struggle when it comes to model adaptation to to resource-poor languages (Wu and Dredze, 2020), or to languages which have typological characteristics unseen by the LM (Üstün et al., 2022). Other approaches aim to adapt monolingual LMs to resource-poor languages that are related to the model language. However, there are conflicting findings regarding whether language relatedness correlates with successful adaptation (de Vries et al., 2021), or not (Ács et al., 2021). With gradual LM adaptation, our approach presented in this extended abstract, we add to the research direction of monolingual LM adaptation. Instead of direct adaptation to a target language, we propose adaptation in stages, first adapting to one or more intermediate languages before the final adaptation step. Inspired by principles of curriculum learning (Bengio et al., 2009), we search for an ideal ordering of languages that can result in improved LM performance on the target language. We follow evidence that typological similarity might correlate with the success of cross-lingual transfer (Pires et al., 2019; Üstün et al., 2022; de Vries et al., 2021) as we believe the success of this transfer is essential for successful model adaptation. Thus we order languages based on their relative typological similarity between them. In our approach, we quantify typological similarity using structural vectors as derived from counts of dependency links (Bjerva et al., 2019), as such fine-grained measures can give a more accurate picture of the typological characteristics of languages (Ponti et al., 2019). We believe that gradual LM adaptation may lead to improved LM performance on a range of resource-poor languages and typologically diverse languages. Additionally, it enables future research to evaluate the correlation between the success of cross-lingual transfer and various typological similarity measures.",0,0,0,,,,,,,,,,,,,
L08-1324,User-Centred Design of Error Correction Tools,2008-01-01,https://aclanthology.org/L08-1324/,lrec,,"This paper presents a methodology for the design and implementation of user-centred language checking applications. The methodology is based on the separation of three critical aspects in this kind of application: functional purpose (educational or corrective goal), types of warning messages, and linguistic resources and computational techniques used. We argue that to assure a user-centred design there must be a clear-cut division between the error typology underlying the system and the software architecture. The methodology described has been used to implement two different user-driven spell, grammar and style checkers for Catalan. We discuss that this is an issue often neglected in commercial applications, and remark the benefits of such a methodology in the scalability of language checking applications. We evaluate our application in terms of recall, precision and noise, and compare it to the only other existing grammar checker for Catalan, to our knowledge.",0,0,0,,,,,,,,,,,,,
L08-1354,Annotation of Information Structure: an Evaluation across different Types of Texts,2008-01-01,https://aclanthology.org/L08-1354/,lrec,,"We report on the evaluation of information structural annotation according to the Linguistic Information Structure Annotation Guidelines (LISA, (Dipper et al., 2007)). The annotation scheme differentiates between the categories of information status, topic, and focus. It aims at being language-independent and has been applied to highly heterogeneous data: written and spoken evidence from typologically diverse languages. For the evaluation presented here, we focused on German texts of different types, both written texts and transcriptions of spoken language, and analyzed the annotation quantitatively and qualitatively.",0,0,0,,,,,,,,,,,,,
L02-1035,Co-reference annotation and resources: A multilingual corpus of typologically diverse languages,2002-01-01,https://aclanthology.org/L02-1035/,lrec,,,1,1,1,1,1,1,d,d,1,0,l,n,"jpn,kij","jpn,kij","jpn,kij","jpn,kij"
